{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFNN-based_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will construct a Deep feedforward neural network(DFNN)-based predictor for predicting the acticity of CES2 inhibitor. Herein, DFNN model with were carried out by pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "os.chdir('./')\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Molecular characterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./release/') \n",
    "from Data_preprocess import load_data,calcMCFP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Dataset_CES2_inhibitors_pIC50.csv\"\n",
    "\n",
    "dataset,canonical_smi,canonical_mols = load_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate MCFP descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = calcMCFP(mols = canonical_mols, dataset = dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./release/') \n",
    "from DFNN_predictor import RegressionDataset,data_process,Net,fit_NN_model,NN_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# 查看是否可用GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_test_split from sklearn was employed for training ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test,train_dataset,test_dataset = data_process(data=pred_data, \n",
    "                                                                        test_size=0.3, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters and training NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02722907066345215,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921d15c7e4fc47b79703c04ec9fa3aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 37.76263 | Test Loss: 37.33814\n",
      "Epoch 002: | Train Loss: 37.37641 | Test Loss: 36.64640\n",
      "Epoch 003: | Train Loss: 36.21207 | Test Loss: 35.54752\n",
      "Epoch 004: | Train Loss: 36.03541 | Test Loss: 33.60041\n",
      "Epoch 005: | Train Loss: 32.10538 | Test Loss: 30.04410\n",
      "Epoch 006: | Train Loss: 27.86246 | Test Loss: 23.71211\n",
      "Epoch 007: | Train Loss: 20.65418 | Test Loss: 14.10335\n",
      "Epoch 008: | Train Loss: 9.97358 | Test Loss: 5.08433\n",
      "Epoch 009: | Train Loss: 4.17402 | Test Loss: 5.04789\n",
      "Epoch 010: | Train Loss: 4.72249 | Test Loss: 4.62037\n",
      "Epoch 011: | Train Loss: 4.57718 | Test Loss: 3.46403\n",
      "Epoch 012: | Train Loss: 3.22293 | Test Loss: 3.28951\n",
      "Epoch 013: | Train Loss: 3.00133 | Test Loss: 3.21170\n",
      "Epoch 014: | Train Loss: 2.67975 | Test Loss: 2.98486\n",
      "Epoch 015: | Train Loss: 2.65039 | Test Loss: 2.82591\n",
      "Epoch 016: | Train Loss: 2.41614 | Test Loss: 2.68407\n",
      "Epoch 017: | Train Loss: 2.42474 | Test Loss: 2.51492\n",
      "Epoch 018: | Train Loss: 2.59470 | Test Loss: 2.38005\n",
      "Epoch 019: | Train Loss: 2.02624 | Test Loss: 2.26275\n",
      "Epoch 020: | Train Loss: 1.87622 | Test Loss: 2.02249\n",
      "Epoch 021: | Train Loss: 2.70612 | Test Loss: 1.89331\n",
      "Epoch 022: | Train Loss: 1.67343 | Test Loss: 1.83634\n",
      "Epoch 023: | Train Loss: 1.56708 | Test Loss: 1.62459\n",
      "Epoch 024: | Train Loss: 1.48069 | Test Loss: 1.51190\n",
      "Epoch 025: | Train Loss: 2.00517 | Test Loss: 1.42414\n",
      "Epoch 026: | Train Loss: 1.55898 | Test Loss: 1.42830\n",
      "Epoch 027: | Train Loss: 1.28722 | Test Loss: 1.25726\n",
      "Epoch 028: | Train Loss: 1.13267 | Test Loss: 1.17282\n",
      "Epoch 029: | Train Loss: 1.34667 | Test Loss: 1.10635\n",
      "Epoch 030: | Train Loss: 1.17918 | Test Loss: 1.08295\n",
      "Epoch 031: | Train Loss: 1.01405 | Test Loss: 1.09905\n",
      "Epoch 032: | Train Loss: 1.23489 | Test Loss: 1.01970\n",
      "Epoch 033: | Train Loss: 0.95555 | Test Loss: 0.97283\n",
      "Epoch 034: | Train Loss: 0.91775 | Test Loss: 0.92974\n",
      "Epoch 035: | Train Loss: 0.92800 | Test Loss: 0.90622\n",
      "Epoch 036: | Train Loss: 0.86291 | Test Loss: 0.89492\n",
      "Epoch 037: | Train Loss: 0.90479 | Test Loss: 0.87497\n",
      "Epoch 038: | Train Loss: 0.79778 | Test Loss: 0.87016\n",
      "Epoch 039: | Train Loss: 0.84894 | Test Loss: 0.82926\n",
      "Epoch 040: | Train Loss: 0.81059 | Test Loss: 0.82183\n",
      "Epoch 041: | Train Loss: 0.75673 | Test Loss: 0.81159\n",
      "Epoch 042: | Train Loss: 0.73619 | Test Loss: 0.83464\n",
      "Epoch 043: | Train Loss: 0.73427 | Test Loss: 0.80108\n",
      "Epoch 044: | Train Loss: 0.96408 | Test Loss: 0.82386\n",
      "Epoch 045: | Train Loss: 0.78489 | Test Loss: 0.78116\n",
      "Epoch 046: | Train Loss: 0.78994 | Test Loss: 0.76508\n",
      "Epoch 047: | Train Loss: 1.02462 | Test Loss: 0.78258\n",
      "Epoch 048: | Train Loss: 0.77144 | Test Loss: 0.75915\n",
      "Epoch 049: | Train Loss: 0.83246 | Test Loss: 0.79106\n",
      "Epoch 050: | Train Loss: 0.72431 | Test Loss: 0.77272\n",
      "Epoch 051: | Train Loss: 0.64904 | Test Loss: 0.84194\n",
      "Epoch 052: | Train Loss: 0.63715 | Test Loss: 0.77172\n",
      "Epoch 053: | Train Loss: 0.76368 | Test Loss: 0.78136\n",
      "Epoch 054: | Train Loss: 0.70817 | Test Loss: 0.75853\n",
      "Epoch 055: | Train Loss: 0.62107 | Test Loss: 0.75226\n",
      "Epoch 056: | Train Loss: 0.57981 | Test Loss: 0.79115\n",
      "Epoch 057: | Train Loss: 0.59965 | Test Loss: 0.74462\n",
      "Epoch 058: | Train Loss: 0.56114 | Test Loss: 0.74603\n",
      "Epoch 059: | Train Loss: 0.61735 | Test Loss: 0.73947\n",
      "Epoch 060: | Train Loss: 0.58702 | Test Loss: 0.74884\n",
      "Epoch 061: | Train Loss: 0.53598 | Test Loss: 0.76929\n",
      "Epoch 062: | Train Loss: 0.53429 | Test Loss: 0.74859\n",
      "Epoch 063: | Train Loss: 0.56291 | Test Loss: 0.74121\n",
      "Epoch 064: | Train Loss: 0.62297 | Test Loss: 0.74418\n",
      "Epoch 065: | Train Loss: 0.54085 | Test Loss: 0.79786\n",
      "Epoch 066: | Train Loss: 0.52759 | Test Loss: 0.73699\n",
      "Epoch 067: | Train Loss: 0.75534 | Test Loss: 0.79265\n",
      "Epoch 068: | Train Loss: 0.63146 | Test Loss: 0.81321\n",
      "Epoch 069: | Train Loss: 0.49870 | Test Loss: 0.76415\n",
      "Epoch 070: | Train Loss: 0.77410 | Test Loss: 0.76429\n",
      "Epoch 071: | Train Loss: 0.56050 | Test Loss: 0.77092\n",
      "Epoch 072: | Train Loss: 0.53033 | Test Loss: 0.82505\n",
      "Epoch 073: | Train Loss: 0.64456 | Test Loss: 0.77027\n",
      "Epoch 074: | Train Loss: 0.50307 | Test Loss: 0.85285\n",
      "Epoch 075: | Train Loss: 0.66904 | Test Loss: 0.77180\n",
      "Epoch 076: | Train Loss: 0.63073 | Test Loss: 0.80504\n",
      "Epoch 077: | Train Loss: 0.53018 | Test Loss: 0.78474\n",
      "Epoch 078: | Train Loss: 0.58594 | Test Loss: 0.87743\n",
      "Epoch 079: | Train Loss: 0.68549 | Test Loss: 0.77768\n",
      "Epoch 080: | Train Loss: 0.47296 | Test Loss: 0.75296\n",
      "Epoch 081: | Train Loss: 0.47527 | Test Loss: 0.74294\n",
      "Epoch 082: | Train Loss: 0.45844 | Test Loss: 0.80776\n",
      "Epoch 083: | Train Loss: 0.49402 | Test Loss: 0.74663\n",
      "Epoch 084: | Train Loss: 0.48317 | Test Loss: 0.75638\n",
      "Epoch 085: | Train Loss: 0.47648 | Test Loss: 0.79808\n",
      "Epoch 086: | Train Loss: 0.50177 | Test Loss: 0.76421\n",
      "Epoch 087: | Train Loss: 0.57082 | Test Loss: 0.80099\n",
      "Epoch 088: | Train Loss: 0.84340 | Test Loss: 0.75091\n",
      "Epoch 089: | Train Loss: 0.53311 | Test Loss: 0.78421\n",
      "Epoch 090: | Train Loss: 0.49899 | Test Loss: 0.89225\n",
      "Epoch 091: | Train Loss: 0.50710 | Test Loss: 0.73821\n",
      "Epoch 092: | Train Loss: 0.53082 | Test Loss: 0.74750\n",
      "Epoch 093: | Train Loss: 0.54139 | Test Loss: 0.79363\n",
      "Epoch 094: | Train Loss: 0.56780 | Test Loss: 0.74247\n",
      "Epoch 095: | Train Loss: 0.41964 | Test Loss: 0.82682\n",
      "Epoch 096: | Train Loss: 0.41147 | Test Loss: 0.74393\n",
      "Epoch 097: | Train Loss: 0.53412 | Test Loss: 0.75243\n",
      "Epoch 098: | Train Loss: 0.46376 | Test Loss: 0.75275\n",
      "Epoch 099: | Train Loss: 0.98671 | Test Loss: 0.78558\n",
      "Epoch 100: | Train Loss: 0.65270 | Test Loss: 0.76854\n",
      "Epoch 101: | Train Loss: 0.81913 | Test Loss: 0.82799\n",
      "Epoch 102: | Train Loss: 0.46925 | Test Loss: 0.87502\n",
      "Epoch 103: | Train Loss: 0.47312 | Test Loss: 0.90146\n",
      "Epoch 104: | Train Loss: 0.46959 | Test Loss: 0.81959\n",
      "Epoch 105: | Train Loss: 0.45162 | Test Loss: 0.80509\n",
      "Epoch 106: | Train Loss: 0.50237 | Test Loss: 0.87949\n",
      "Epoch 107: | Train Loss: 0.53244 | Test Loss: 0.77964\n",
      "Epoch 108: | Train Loss: 0.82733 | Test Loss: 0.76352\n",
      "Epoch 109: | Train Loss: 1.14781 | Test Loss: 0.80405\n",
      "Epoch 110: | Train Loss: 0.54911 | Test Loss: 0.87698\n",
      "Epoch 111: | Train Loss: 0.44039 | Test Loss: 0.85307\n",
      "Epoch 112: | Train Loss: 0.40864 | Test Loss: 0.74740\n",
      "Epoch 113: | Train Loss: 0.40982 | Test Loss: 0.74293\n",
      "Epoch 114: | Train Loss: 0.45211 | Test Loss: 0.77019\n",
      "Epoch 115: | Train Loss: 0.39710 | Test Loss: 0.77603\n",
      "Epoch 116: | Train Loss: 0.36723 | Test Loss: 0.74847\n",
      "Epoch 117: | Train Loss: 0.36429 | Test Loss: 0.76650\n",
      "Epoch 118: | Train Loss: 0.35055 | Test Loss: 0.75168\n",
      "Epoch 119: | Train Loss: 0.47878 | Test Loss: 0.75285\n",
      "Epoch 120: | Train Loss: 0.36803 | Test Loss: 0.75791\n",
      "Epoch 121: | Train Loss: 0.35416 | Test Loss: 0.77872\n",
      "Epoch 122: | Train Loss: 0.35572 | Test Loss: 0.76784\n",
      "Epoch 123: | Train Loss: 0.34710 | Test Loss: 0.76183\n",
      "Epoch 124: | Train Loss: 0.34923 | Test Loss: 0.77039\n",
      "Epoch 125: | Train Loss: 0.34102 | Test Loss: 0.76837\n",
      "Epoch 126: | Train Loss: 0.43149 | Test Loss: 0.78624\n",
      "Epoch 127: | Train Loss: 0.42323 | Test Loss: 0.78047\n",
      "Epoch 128: | Train Loss: 0.49392 | Test Loss: 0.75130\n",
      "Epoch 129: | Train Loss: 0.44218 | Test Loss: 0.77258\n",
      "Epoch 130: | Train Loss: 0.36401 | Test Loss: 0.75278\n",
      "Epoch 131: | Train Loss: 0.38803 | Test Loss: 0.78392\n",
      "Epoch 132: | Train Loss: 0.38081 | Test Loss: 0.76637\n",
      "Epoch 133: | Train Loss: 0.37982 | Test Loss: 0.74842\n",
      "Epoch 134: | Train Loss: 0.34774 | Test Loss: 0.75759\n",
      "Epoch 135: | Train Loss: 0.33682 | Test Loss: 0.73991\n",
      "Epoch 136: | Train Loss: 0.34073 | Test Loss: 0.75637\n",
      "Epoch 137: | Train Loss: 0.42357 | Test Loss: 0.75044\n",
      "Epoch 138: | Train Loss: 0.41339 | Test Loss: 0.76642\n",
      "Epoch 139: | Train Loss: 0.36975 | Test Loss: 0.74406\n",
      "Epoch 140: | Train Loss: 0.32601 | Test Loss: 0.77616\n",
      "Epoch 141: | Train Loss: 0.32311 | Test Loss: 0.73589\n",
      "Epoch 142: | Train Loss: 0.66748 | Test Loss: 0.73663\n",
      "Epoch 143: | Train Loss: 0.37078 | Test Loss: 0.76280\n",
      "Epoch 144: | Train Loss: 0.36322 | Test Loss: 0.80020\n",
      "Epoch 145: | Train Loss: 0.38996 | Test Loss: 0.74993\n",
      "Epoch 146: | Train Loss: 0.35103 | Test Loss: 0.78519\n",
      "Epoch 147: | Train Loss: 0.32559 | Test Loss: 0.73392\n",
      "Epoch 148: | Train Loss: 0.37731 | Test Loss: 0.73922\n",
      "Epoch 149: | Train Loss: 0.32412 | Test Loss: 0.76315\n",
      "Epoch 150: | Train Loss: 0.41176 | Test Loss: 0.79247\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 150\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_FEATURES = x_train.shape[1] #查看行数 len(X_train) 或 X_train.shape[0]；查看列数 X_train.shape[1]\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1)\n",
    "\n",
    "# training model\n",
    "loss_status,model=fit_NN_model(train_loader,test_loader,EPOCHS,BATCH_SIZE,LEARNING_RATE,NUM_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for training set: 0.7666204904101118\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = model(torch.from_numpy(x_train).float()).tolist()\n",
    "y_pred_test = model(torch.from_numpy(x_test).float()).tolist()\n",
    "\n",
    "print(\"R2 for training set:\",r2_score(y_train, y_pred_train))\n",
    "print(\"R2 for training set:\",r2_score(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./model/DFNN.pka\"\n",
    "\n",
    "#torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./release/') \n",
    "from DFNN_predictor import NN_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Demo_pre_act.csv\"\n",
    "\n",
    "dataset,canonical_smi,canonical_mols = load_data(path)\n",
    "vali_data = calcMCFP(mols = canonical_mols, dataset = dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vali_data  # validation set\n",
    "path = \"./model/DFNN.pka\"\n",
    "\n",
    "pre_act = NN_predict(data = vali_data, path =path, num_features = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
