{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFNN-based_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will construct a Deep feedforward neural network(DFNN)-based predictor for predicting the acticity of CES2 inhibitor. Herein, DFNN model with were carried out by pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "os.chdir('./')\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Molecular characterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./release/') \n",
    "from Data_preprocess import load_data,calcMCFP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"pIC50数据-删除模糊数据后补充新增数据-734.csv\"\n",
    "\n",
    "dataset,canonical_smi,canonical_mols = load_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate MCFP descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = calcMCFP(mols = canonical_mols, dataset = dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./release/') \n",
    "from DFNN_predictor import RegressionDataset,data_process,Net,fit_NN_model,NN_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# 查看是否可用GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_test_split from sklearn was employed for training ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test,train_dataset,test_dataset = data_process(data=pred_data, \n",
    "                                                                        test_size=0.3, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters and training NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_FEATURES = x_train.shape[1] #查看行数 len(X_train) 或 X_train.shape[0]；查看列数 X_train.shape[1]\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1)\n",
    "\n",
    "# training model\n",
    "loss_status,model=fit_NN_model(train_loader,test_loader,EPOCHS,BATCH_SIZE,LEARNING_RATE,NUM_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 十次重复实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04932522773742676,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a27ae98c1a4da0a9a75ee94496dd7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 38.20195 | Test Loss: 39.07507\n",
      "Epoch 002: | Train Loss: 37.68136 | Test Loss: 38.12298\n",
      "Epoch 003: | Train Loss: 37.21672 | Test Loss: 36.76679\n",
      "Epoch 004: | Train Loss: 35.11477 | Test Loss: 33.59139\n",
      "Epoch 005: | Train Loss: 30.42025 | Test Loss: 26.53878\n",
      "Epoch 006: | Train Loss: 24.32776 | Test Loss: 14.38379\n",
      "Epoch 007: | Train Loss: 8.92491 | Test Loss: 4.06230\n",
      "Epoch 008: | Train Loss: 4.36284 | Test Loss: 6.12646\n",
      "Epoch 009: | Train Loss: 4.72588 | Test Loss: 3.86982\n",
      "Epoch 010: | Train Loss: 3.97870 | Test Loss: 3.41397\n",
      "Epoch 011: | Train Loss: 2.93907 | Test Loss: 3.24121\n",
      "Epoch 012: | Train Loss: 2.76504 | Test Loss: 3.15359\n",
      "Epoch 013: | Train Loss: 2.67974 | Test Loss: 2.98160\n",
      "Epoch 014: | Train Loss: 2.54245 | Test Loss: 2.83238\n",
      "Epoch 015: | Train Loss: 2.51154 | Test Loss: 2.71979\n",
      "Epoch 016: | Train Loss: 3.36058 | Test Loss: 2.61539\n",
      "Epoch 017: | Train Loss: 2.37475 | Test Loss: 2.56897\n",
      "Epoch 018: | Train Loss: 2.14767 | Test Loss: 2.35838\n",
      "Epoch 019: | Train Loss: 2.24762 | Test Loss: 2.26168\n",
      "Epoch 020: | Train Loss: 2.01957 | Test Loss: 2.17409\n",
      "Epoch 021: | Train Loss: 2.36949 | Test Loss: 2.16593\n",
      "Epoch 022: | Train Loss: 1.97367 | Test Loss: 2.11848\n",
      "Epoch 023: | Train Loss: 1.86894 | Test Loss: 1.91832\n",
      "Epoch 024: | Train Loss: 1.98130 | Test Loss: 1.84973\n",
      "Epoch 025: | Train Loss: 1.63850 | Test Loss: 1.78611\n",
      "Epoch 026: | Train Loss: 1.60047 | Test Loss: 1.73924\n",
      "Epoch 027: | Train Loss: 1.53898 | Test Loss: 1.68206\n",
      "Epoch 028: | Train Loss: 2.24125 | Test Loss: 1.64710\n",
      "Epoch 029: | Train Loss: 1.55164 | Test Loss: 1.64295\n",
      "Epoch 030: | Train Loss: 2.05935 | Test Loss: 1.47918\n",
      "Epoch 031: | Train Loss: 1.63026 | Test Loss: 1.58056\n",
      "Epoch 032: | Train Loss: 1.36616 | Test Loss: 1.50419\n",
      "Epoch 033: | Train Loss: 1.38778 | Test Loss: 1.43496\n",
      "Epoch 034: | Train Loss: 1.27333 | Test Loss: 1.37451\n",
      "Epoch 035: | Train Loss: 1.24165 | Test Loss: 1.32992\n",
      "Epoch 036: | Train Loss: 1.19686 | Test Loss: 1.29593\n",
      "Epoch 037: | Train Loss: 1.42268 | Test Loss: 1.26636\n",
      "Epoch 038: | Train Loss: 1.17720 | Test Loss: 1.24365\n",
      "Epoch 039: | Train Loss: 1.45675 | Test Loss: 1.23275\n",
      "Epoch 040: | Train Loss: 1.20570 | Test Loss: 1.22378\n",
      "Epoch 041: | Train Loss: 1.19314 | Test Loss: 1.24832\n",
      "Epoch 042: | Train Loss: 1.19207 | Test Loss: 1.17634\n",
      "Epoch 043: | Train Loss: 1.15455 | Test Loss: 1.17443\n",
      "Epoch 044: | Train Loss: 1.05428 | Test Loss: 1.16485\n",
      "Epoch 045: | Train Loss: 1.03345 | Test Loss: 1.15429\n",
      "Epoch 046: | Train Loss: 1.02479 | Test Loss: 1.13247\n",
      "Epoch 047: | Train Loss: 1.02101 | Test Loss: 1.11285\n",
      "Epoch 048: | Train Loss: 0.99355 | Test Loss: 1.10950\n",
      "Epoch 049: | Train Loss: 0.96932 | Test Loss: 1.07103\n",
      "Epoch 050: | Train Loss: 0.97821 | Test Loss: 1.06610\n",
      "Epoch 051: | Train Loss: 1.04528 | Test Loss: 1.04823\n",
      "Epoch 052: | Train Loss: 1.21510 | Test Loss: 1.09957\n",
      "Epoch 053: | Train Loss: 0.96758 | Test Loss: 1.03844\n",
      "Epoch 054: | Train Loss: 0.91131 | Test Loss: 1.01085\n",
      "Epoch 055: | Train Loss: 0.90278 | Test Loss: 1.02816\n",
      "Epoch 056: | Train Loss: 0.90371 | Test Loss: 1.00121\n",
      "Epoch 057: | Train Loss: 0.93830 | Test Loss: 0.98693\n",
      "Epoch 058: | Train Loss: 1.23661 | Test Loss: 0.99070\n",
      "Epoch 059: | Train Loss: 0.96344 | Test Loss: 0.98646\n",
      "Epoch 060: | Train Loss: 1.05695 | Test Loss: 1.11513\n",
      "Epoch 061: | Train Loss: 1.19066 | Test Loss: 1.00848\n",
      "Epoch 062: | Train Loss: 1.60458 | Test Loss: 0.96997\n",
      "Epoch 063: | Train Loss: 1.18089 | Test Loss: 1.01966\n",
      "Epoch 064: | Train Loss: 0.88865 | Test Loss: 1.19179\n",
      "Epoch 065: | Train Loss: 0.96297 | Test Loss: 0.97269\n",
      "Epoch 066: | Train Loss: 0.82912 | Test Loss: 0.99101\n",
      "Epoch 067: | Train Loss: 0.92354 | Test Loss: 1.00490\n",
      "Epoch 068: | Train Loss: 0.94687 | Test Loss: 1.02000\n",
      "Epoch 069: | Train Loss: 0.94803 | Test Loss: 0.96873\n",
      "Epoch 070: | Train Loss: 1.06538 | Test Loss: 0.95845\n",
      "Epoch 071: | Train Loss: 1.37114 | Test Loss: 1.03140\n",
      "Epoch 072: | Train Loss: 1.10770 | Test Loss: 1.01384\n",
      "Epoch 073: | Train Loss: 0.93383 | Test Loss: 1.05539\n",
      "Epoch 074: | Train Loss: 0.80340 | Test Loss: 0.96265\n",
      "Epoch 075: | Train Loss: 0.88634 | Test Loss: 0.96956\n",
      "Epoch 076: | Train Loss: 0.98128 | Test Loss: 0.99814\n",
      "Epoch 077: | Train Loss: 0.80443 | Test Loss: 0.96122\n",
      "Epoch 078: | Train Loss: 0.77308 | Test Loss: 1.01237\n",
      "Epoch 079: | Train Loss: 0.94692 | Test Loss: 0.96055\n",
      "Epoch 080: | Train Loss: 0.98689 | Test Loss: 0.94513\n",
      "Epoch 081: | Train Loss: 1.05602 | Test Loss: 1.03432\n",
      "Epoch 082: | Train Loss: 0.84324 | Test Loss: 0.97218\n",
      "Epoch 083: | Train Loss: 0.81674 | Test Loss: 1.06141\n",
      "Epoch 084: | Train Loss: 0.84826 | Test Loss: 0.96497\n",
      "Epoch 085: | Train Loss: 1.17639 | Test Loss: 0.94346\n",
      "Epoch 086: | Train Loss: 0.82589 | Test Loss: 1.14474\n",
      "Epoch 087: | Train Loss: 1.00044 | Test Loss: 0.93436\n",
      "Epoch 088: | Train Loss: 0.73438 | Test Loss: 0.97648\n",
      "Epoch 089: | Train Loss: 1.16359 | Test Loss: 0.96829\n",
      "Epoch 090: | Train Loss: 0.91953 | Test Loss: 1.09522\n",
      "Epoch 091: | Train Loss: 0.77544 | Test Loss: 0.94452\n",
      "Epoch 092: | Train Loss: 0.79858 | Test Loss: 0.92558\n",
      "Epoch 093: | Train Loss: 0.79807 | Test Loss: 0.92965\n",
      "Epoch 094: | Train Loss: 0.76108 | Test Loss: 1.05672\n",
      "Epoch 095: | Train Loss: 0.74556 | Test Loss: 0.93744\n",
      "Epoch 096: | Train Loss: 0.70536 | Test Loss: 0.93818\n",
      "Epoch 097: | Train Loss: 0.70164 | Test Loss: 0.96311\n",
      "Epoch 098: | Train Loss: 0.72807 | Test Loss: 0.92877\n",
      "Epoch 099: | Train Loss: 1.12886 | Test Loss: 0.95056\n",
      "Epoch 100: | Train Loss: 1.55493 | Test Loss: 0.93642\n",
      "Epoch 101: | Train Loss: 1.22205 | Test Loss: 0.98773\n",
      "Epoch 102: | Train Loss: 0.83837 | Test Loss: 1.24339\n",
      "Epoch 103: | Train Loss: 0.81729 | Test Loss: 0.90879\n",
      "Epoch 104: | Train Loss: 0.71202 | Test Loss: 0.94719\n",
      "Epoch 105: | Train Loss: 0.73601 | Test Loss: 0.93212\n",
      "Epoch 106: | Train Loss: 0.70465 | Test Loss: 0.92555\n",
      "Epoch 107: | Train Loss: 0.67532 | Test Loss: 0.96526\n",
      "Epoch 108: | Train Loss: 0.70771 | Test Loss: 0.92936\n",
      "Epoch 109: | Train Loss: 0.72734 | Test Loss: 0.93730\n",
      "Epoch 110: | Train Loss: 0.72298 | Test Loss: 1.00551\n",
      "Epoch 111: | Train Loss: 0.70495 | Test Loss: 0.93626\n",
      "Epoch 112: | Train Loss: 0.69666 | Test Loss: 0.91941\n",
      "Epoch 113: | Train Loss: 0.66989 | Test Loss: 0.92532\n",
      "Epoch 114: | Train Loss: 0.68251 | Test Loss: 0.94905\n",
      "Epoch 115: | Train Loss: 0.72041 | Test Loss: 0.94195\n",
      "Epoch 116: | Train Loss: 0.70245 | Test Loss: 0.95478\n",
      "Epoch 117: | Train Loss: 0.84170 | Test Loss: 0.93952\n",
      "Epoch 118: | Train Loss: 0.73337 | Test Loss: 0.97145\n",
      "Epoch 119: | Train Loss: 0.74247 | Test Loss: 0.91301\n",
      "Epoch 120: | Train Loss: 0.67354 | Test Loss: 0.95867\n",
      "Epoch 121: | Train Loss: 0.65640 | Test Loss: 0.91905\n",
      "Epoch 122: | Train Loss: 0.85846 | Test Loss: 0.90150\n",
      "Epoch 123: | Train Loss: 0.83743 | Test Loss: 0.90097\n",
      "Epoch 124: | Train Loss: 0.69137 | Test Loss: 0.90975\n",
      "Epoch 125: | Train Loss: 0.86895 | Test Loss: 0.92804\n",
      "Epoch 126: | Train Loss: 0.67973 | Test Loss: 0.91493\n",
      "Epoch 127: | Train Loss: 0.70562 | Test Loss: 0.99454\n",
      "Epoch 128: | Train Loss: 0.66765 | Test Loss: 0.94327\n",
      "Epoch 129: | Train Loss: 0.62952 | Test Loss: 0.90656\n",
      "Epoch 130: | Train Loss: 0.70440 | Test Loss: 0.93940\n",
      "Epoch 131: | Train Loss: 0.70530 | Test Loss: 0.92731\n",
      "Epoch 132: | Train Loss: 0.88432 | Test Loss: 0.90310\n",
      "Epoch 133: | Train Loss: 0.72714 | Test Loss: 1.03360\n",
      "Epoch 134: | Train Loss: 0.68790 | Test Loss: 0.89920\n",
      "Epoch 135: | Train Loss: 0.71714 | Test Loss: 0.91305\n",
      "Epoch 136: | Train Loss: 0.66489 | Test Loss: 0.99488\n",
      "Epoch 137: | Train Loss: 0.64972 | Test Loss: 0.89604\n",
      "Epoch 138: | Train Loss: 0.63272 | Test Loss: 0.91123\n",
      "Epoch 139: | Train Loss: 0.65440 | Test Loss: 0.92184\n",
      "Epoch 140: | Train Loss: 0.64999 | Test Loss: 0.91137\n",
      "Epoch 141: | Train Loss: 0.65024 | Test Loss: 0.88343\n",
      "Epoch 142: | Train Loss: 0.62447 | Test Loss: 0.91496\n",
      "Epoch 143: | Train Loss: 0.61869 | Test Loss: 0.89775\n",
      "Epoch 144: | Train Loss: 0.75750 | Test Loss: 0.87447\n",
      "Epoch 145: | Train Loss: 0.86203 | Test Loss: 0.94579\n",
      "Epoch 146: | Train Loss: 0.75334 | Test Loss: 0.94478\n",
      "Epoch 147: | Train Loss: 0.67621 | Test Loss: 0.86317\n",
      "Epoch 148: | Train Loss: 0.61748 | Test Loss: 0.92815\n",
      "Epoch 149: | Train Loss: 1.13388 | Test Loss: 0.85663\n",
      "Epoch 150: | Train Loss: 0.83491 | Test Loss: 0.86692\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014868021011352539,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc21b83dfe341d68d500b6862d952dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 39.71754 | Test Loss: 40.03689\n",
      "Epoch 002: | Train Loss: 38.35868 | Test Loss: 39.45195\n",
      "Epoch 003: | Train Loss: 42.22313 | Test Loss: 38.78136\n",
      "Epoch 004: | Train Loss: 42.31578 | Test Loss: 37.80727\n",
      "Epoch 005: | Train Loss: 36.30861 | Test Loss: 35.92226\n",
      "Epoch 006: | Train Loss: 35.07088 | Test Loss: 32.05824\n",
      "Epoch 007: | Train Loss: 29.20720 | Test Loss: 24.90144\n",
      "Epoch 008: | Train Loss: 22.34539 | Test Loss: 14.33786\n",
      "Epoch 009: | Train Loss: 10.86532 | Test Loss: 4.43519\n",
      "Epoch 010: | Train Loss: 3.59577 | Test Loss: 5.74402\n",
      "Epoch 011: | Train Loss: 4.94833 | Test Loss: 3.84396\n",
      "Epoch 012: | Train Loss: 4.19300 | Test Loss: 3.69101\n",
      "Epoch 013: | Train Loss: 3.32780 | Test Loss: 3.29621\n",
      "Epoch 014: | Train Loss: 3.08457 | Test Loss: 3.35071\n",
      "Epoch 015: | Train Loss: 2.78423 | Test Loss: 3.04093\n",
      "Epoch 016: | Train Loss: 2.61630 | Test Loss: 2.80810\n",
      "Epoch 017: | Train Loss: 2.41835 | Test Loss: 2.65722\n",
      "Epoch 018: | Train Loss: 2.32590 | Test Loss: 2.49308\n",
      "Epoch 019: | Train Loss: 2.12755 | Test Loss: 2.32308\n",
      "Epoch 020: | Train Loss: 2.09716 | Test Loss: 2.18335\n",
      "Epoch 021: | Train Loss: 2.05633 | Test Loss: 2.04615\n",
      "Epoch 022: | Train Loss: 1.79619 | Test Loss: 1.98467\n",
      "Epoch 023: | Train Loss: 2.30874 | Test Loss: 1.84823\n",
      "Epoch 024: | Train Loss: 1.86677 | Test Loss: 1.75073\n",
      "Epoch 025: | Train Loss: 1.59026 | Test Loss: 1.65759\n",
      "Epoch 026: | Train Loss: 1.62018 | Test Loss: 1.54155\n",
      "Epoch 027: | Train Loss: 1.37186 | Test Loss: 1.52868\n",
      "Epoch 028: | Train Loss: 1.33704 | Test Loss: 1.43508\n",
      "Epoch 029: | Train Loss: 1.30017 | Test Loss: 1.38196\n",
      "Epoch 030: | Train Loss: 1.33439 | Test Loss: 1.36810\n",
      "Epoch 031: | Train Loss: 1.24861 | Test Loss: 1.29729\n",
      "Epoch 032: | Train Loss: 1.16352 | Test Loss: 1.23970\n",
      "Epoch 033: | Train Loss: 1.13403 | Test Loss: 1.20991\n",
      "Epoch 034: | Train Loss: 1.10391 | Test Loss: 1.17694\n",
      "Epoch 035: | Train Loss: 1.12584 | Test Loss: 1.13069\n",
      "Epoch 036: | Train Loss: 1.07404 | Test Loss: 1.10690\n",
      "Epoch 037: | Train Loss: 1.03326 | Test Loss: 1.10360\n",
      "Epoch 038: | Train Loss: 1.77869 | Test Loss: 1.06298\n",
      "Epoch 039: | Train Loss: 1.09246 | Test Loss: 1.14946\n",
      "Epoch 040: | Train Loss: 1.05378 | Test Loss: 1.06906\n",
      "Epoch 041: | Train Loss: 1.04843 | Test Loss: 1.02553\n",
      "Epoch 042: | Train Loss: 0.98530 | Test Loss: 1.01049\n",
      "Epoch 043: | Train Loss: 0.94674 | Test Loss: 1.03195\n",
      "Epoch 044: | Train Loss: 0.94016 | Test Loss: 0.98501\n",
      "Epoch 045: | Train Loss: 1.21356 | Test Loss: 0.97851\n",
      "Epoch 046: | Train Loss: 1.19246 | Test Loss: 0.97474\n",
      "Epoch 047: | Train Loss: 0.94050 | Test Loss: 0.97936\n",
      "Epoch 048: | Train Loss: 0.90249 | Test Loss: 1.00110\n",
      "Epoch 049: | Train Loss: 0.87053 | Test Loss: 0.95618\n",
      "Epoch 050: | Train Loss: 0.88497 | Test Loss: 0.95123\n",
      "Epoch 051: | Train Loss: 1.03823 | Test Loss: 0.94719\n",
      "Epoch 052: | Train Loss: 0.93235 | Test Loss: 0.93744\n",
      "Epoch 053: | Train Loss: 0.85702 | Test Loss: 1.01734\n",
      "Epoch 054: | Train Loss: 0.85048 | Test Loss: 0.92850\n",
      "Epoch 055: | Train Loss: 0.82888 | Test Loss: 0.91979\n",
      "Epoch 056: | Train Loss: 0.83324 | Test Loss: 0.93278\n",
      "Epoch 057: | Train Loss: 0.80398 | Test Loss: 0.91975\n",
      "Epoch 058: | Train Loss: 0.90758 | Test Loss: 0.91378\n",
      "Epoch 059: | Train Loss: 0.80481 | Test Loss: 0.93663\n",
      "Epoch 060: | Train Loss: 0.79527 | Test Loss: 0.90653\n",
      "Epoch 061: | Train Loss: 0.82403 | Test Loss: 0.91628\n",
      "Epoch 062: | Train Loss: 0.77763 | Test Loss: 0.91478\n",
      "Epoch 063: | Train Loss: 0.76714 | Test Loss: 0.87520\n",
      "Epoch 064: | Train Loss: 0.78784 | Test Loss: 0.87686\n",
      "Epoch 065: | Train Loss: 0.97522 | Test Loss: 0.87148\n",
      "Epoch 066: | Train Loss: 0.82258 | Test Loss: 0.91691\n",
      "Epoch 067: | Train Loss: 0.80777 | Test Loss: 0.85905\n",
      "Epoch 068: | Train Loss: 1.05755 | Test Loss: 0.96405\n",
      "Epoch 069: | Train Loss: 0.86012 | Test Loss: 0.89144\n",
      "Epoch 070: | Train Loss: 0.72882 | Test Loss: 0.83234\n",
      "Epoch 071: | Train Loss: 0.69873 | Test Loss: 0.85439\n",
      "Epoch 072: | Train Loss: 0.69532 | Test Loss: 0.82815\n",
      "Epoch 073: | Train Loss: 0.68684 | Test Loss: 0.82288\n",
      "Epoch 074: | Train Loss: 0.72455 | Test Loss: 0.82348\n",
      "Epoch 075: | Train Loss: 0.67665 | Test Loss: 0.83236\n",
      "Epoch 076: | Train Loss: 0.76828 | Test Loss: 0.85190\n",
      "Epoch 077: | Train Loss: 0.67345 | Test Loss: 0.82975\n",
      "Epoch 078: | Train Loss: 0.70993 | Test Loss: 0.84168\n",
      "Epoch 079: | Train Loss: 0.73500 | Test Loss: 0.84671\n",
      "Epoch 080: | Train Loss: 0.68746 | Test Loss: 0.83311\n",
      "Epoch 081: | Train Loss: 0.68665 | Test Loss: 0.89002\n",
      "Epoch 082: | Train Loss: 0.74903 | Test Loss: 0.81212\n",
      "Epoch 083: | Train Loss: 0.68886 | Test Loss: 0.80996\n",
      "Epoch 084: | Train Loss: 0.64378 | Test Loss: 0.86071\n",
      "Epoch 085: | Train Loss: 0.61847 | Test Loss: 0.79561\n",
      "Epoch 086: | Train Loss: 0.59073 | Test Loss: 0.80167\n",
      "Epoch 087: | Train Loss: 0.65877 | Test Loss: 0.79145\n",
      "Epoch 088: | Train Loss: 0.62488 | Test Loss: 0.81185\n",
      "Epoch 089: | Train Loss: 0.55972 | Test Loss: 0.77394\n",
      "Epoch 090: | Train Loss: 0.55167 | Test Loss: 0.77516\n",
      "Epoch 091: | Train Loss: 0.97100 | Test Loss: 0.76674\n",
      "Epoch 092: | Train Loss: 0.64894 | Test Loss: 0.78304\n",
      "Epoch 093: | Train Loss: 2.26809 | Test Loss: 0.89511\n",
      "Epoch 094: | Train Loss: 0.62543 | Test Loss: 0.75746\n",
      "Epoch 095: | Train Loss: 0.55328 | Test Loss: 0.75109\n",
      "Epoch 096: | Train Loss: 0.51552 | Test Loss: 0.75869\n",
      "Epoch 097: | Train Loss: 0.64275 | Test Loss: 0.75179\n",
      "Epoch 098: | Train Loss: 0.61909 | Test Loss: 0.73708\n",
      "Epoch 099: | Train Loss: 0.73207 | Test Loss: 0.73112\n",
      "Epoch 100: | Train Loss: 0.60970 | Test Loss: 0.81056\n",
      "Epoch 101: | Train Loss: 0.56746 | Test Loss: 0.75758\n",
      "Epoch 102: | Train Loss: 0.50102 | Test Loss: 0.73033\n",
      "Epoch 103: | Train Loss: 0.48055 | Test Loss: 0.72207\n",
      "Epoch 104: | Train Loss: 0.54805 | Test Loss: 0.72245\n",
      "Epoch 105: | Train Loss: 0.47893 | Test Loss: 0.74104\n",
      "Epoch 106: | Train Loss: 0.58895 | Test Loss: 0.70755\n",
      "Epoch 107: | Train Loss: 0.47596 | Test Loss: 0.71522\n",
      "Epoch 108: | Train Loss: 0.44608 | Test Loss: 0.70303\n",
      "Epoch 109: | Train Loss: 0.73019 | Test Loss: 0.71018\n",
      "Epoch 110: | Train Loss: 0.71305 | Test Loss: 0.74765\n",
      "Epoch 111: | Train Loss: 0.50960 | Test Loss: 0.79781\n",
      "Epoch 112: | Train Loss: 0.47589 | Test Loss: 0.81869\n",
      "Epoch 113: | Train Loss: 0.48628 | Test Loss: 0.77767\n",
      "Epoch 114: | Train Loss: 0.49167 | Test Loss: 0.77009\n",
      "Epoch 115: | Train Loss: 0.41133 | Test Loss: 0.74592\n",
      "Epoch 116: | Train Loss: 0.40570 | Test Loss: 0.75161\n",
      "Epoch 117: | Train Loss: 0.60736 | Test Loss: 0.74218\n",
      "Epoch 118: | Train Loss: 0.42116 | Test Loss: 0.75177\n",
      "Epoch 119: | Train Loss: 0.38092 | Test Loss: 0.72389\n",
      "Epoch 120: | Train Loss: 0.42535 | Test Loss: 0.73193\n",
      "Epoch 121: | Train Loss: 0.38210 | Test Loss: 0.73170\n",
      "Epoch 122: | Train Loss: 0.36760 | Test Loss: 0.72415\n",
      "Epoch 123: | Train Loss: 0.38043 | Test Loss: 0.74699\n",
      "Epoch 124: | Train Loss: 0.38458 | Test Loss: 0.73678\n",
      "Epoch 125: | Train Loss: 0.37030 | Test Loss: 0.73308\n",
      "Epoch 126: | Train Loss: 0.35587 | Test Loss: 0.74437\n",
      "Epoch 127: | Train Loss: 0.39552 | Test Loss: 0.73853\n",
      "Epoch 128: | Train Loss: 0.36649 | Test Loss: 0.72746\n",
      "Epoch 129: | Train Loss: 0.44960 | Test Loss: 0.72934\n",
      "Epoch 130: | Train Loss: 0.42256 | Test Loss: 0.77995\n",
      "Epoch 131: | Train Loss: 0.41581 | Test Loss: 0.73806\n",
      "Epoch 132: | Train Loss: 0.41365 | Test Loss: 0.74472\n",
      "Epoch 133: | Train Loss: 0.39111 | Test Loss: 0.74840\n",
      "Epoch 134: | Train Loss: 0.34085 | Test Loss: 0.75687\n",
      "Epoch 135: | Train Loss: 0.34441 | Test Loss: 0.75111\n",
      "Epoch 136: | Train Loss: 0.37445 | Test Loss: 0.75959\n",
      "Epoch 137: | Train Loss: 0.34351 | Test Loss: 0.74376\n",
      "Epoch 138: | Train Loss: 0.33279 | Test Loss: 0.73848\n",
      "Epoch 139: | Train Loss: 0.32703 | Test Loss: 0.75652\n",
      "Epoch 140: | Train Loss: 0.33561 | Test Loss: 0.73896\n",
      "Epoch 141: | Train Loss: 0.32569 | Test Loss: 0.74857\n",
      "Epoch 142: | Train Loss: 0.31730 | Test Loss: 0.75025\n",
      "Epoch 143: | Train Loss: 0.42845 | Test Loss: 0.76236\n",
      "Epoch 144: | Train Loss: 0.34442 | Test Loss: 0.75089\n",
      "Epoch 145: | Train Loss: 0.34489 | Test Loss: 0.75113\n",
      "Epoch 146: | Train Loss: 0.34351 | Test Loss: 0.77370\n",
      "Epoch 147: | Train Loss: 0.32139 | Test Loss: 0.75568\n",
      "Epoch 148: | Train Loss: 0.31867 | Test Loss: 0.76815\n",
      "Epoch 149: | Train Loss: 0.38078 | Test Loss: 0.76435\n",
      "Epoch 150: | Train Loss: 0.38386 | Test Loss: 0.77095\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018073081970214844,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e77db6048864f3c898bc72069e8a4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 38.76634 | Test Loss: 38.23876\n",
      "Epoch 002: | Train Loss: 40.10675 | Test Loss: 36.74614\n",
      "Epoch 003: | Train Loss: 36.89179 | Test Loss: 35.20099\n",
      "Epoch 004: | Train Loss: 37.83776 | Test Loss: 32.51256\n",
      "Epoch 005: | Train Loss: 33.23140 | Test Loss: 26.89156\n",
      "Epoch 006: | Train Loss: 23.46235 | Test Loss: 17.29491\n",
      "Epoch 007: | Train Loss: 12.23886 | Test Loss: 6.15365\n",
      "Epoch 008: | Train Loss: 4.32569 | Test Loss: 5.06185\n",
      "Epoch 009: | Train Loss: 5.22525 | Test Loss: 4.60380\n",
      "Epoch 010: | Train Loss: 3.75705 | Test Loss: 3.47878\n",
      "Epoch 011: | Train Loss: 3.44228 | Test Loss: 3.56497\n",
      "Epoch 012: | Train Loss: 3.09108 | Test Loss: 3.05447\n",
      "Epoch 013: | Train Loss: 2.70867 | Test Loss: 3.09280\n",
      "Epoch 014: | Train Loss: 2.60797 | Test Loss: 2.81577\n",
      "Epoch 015: | Train Loss: 2.43961 | Test Loss: 2.60912\n",
      "Epoch 016: | Train Loss: 2.78880 | Test Loss: 2.44386\n",
      "Epoch 017: | Train Loss: 2.58472 | Test Loss: 2.31642\n",
      "Epoch 018: | Train Loss: 2.13118 | Test Loss: 2.18388\n",
      "Epoch 019: | Train Loss: 1.91592 | Test Loss: 2.02327\n",
      "Epoch 020: | Train Loss: 1.80659 | Test Loss: 1.89649\n",
      "Epoch 021: | Train Loss: 2.75041 | Test Loss: 1.75350\n",
      "Epoch 022: | Train Loss: 1.73610 | Test Loss: 1.81712\n",
      "Epoch 023: | Train Loss: 4.54500 | Test Loss: 1.64327\n",
      "Epoch 024: | Train Loss: 1.69073 | Test Loss: 1.64165\n",
      "Epoch 025: | Train Loss: 1.63384 | Test Loss: 1.41357\n",
      "Epoch 026: | Train Loss: 1.40025 | Test Loss: 1.39665\n",
      "Epoch 027: | Train Loss: 1.29331 | Test Loss: 1.32167\n",
      "Epoch 028: | Train Loss: 1.81590 | Test Loss: 1.28565\n",
      "Epoch 029: | Train Loss: 1.26365 | Test Loss: 1.20527\n",
      "Epoch 030: | Train Loss: 1.21721 | Test Loss: 1.17907\n",
      "Epoch 031: | Train Loss: 1.19606 | Test Loss: 1.12534\n",
      "Epoch 032: | Train Loss: 1.80127 | Test Loss: 1.19159\n",
      "Epoch 033: | Train Loss: 1.12554 | Test Loss: 1.08329\n",
      "Epoch 034: | Train Loss: 1.03728 | Test Loss: 1.03859\n",
      "Epoch 035: | Train Loss: 1.06263 | Test Loss: 1.01399\n",
      "Epoch 036: | Train Loss: 1.93713 | Test Loss: 1.06186\n",
      "Epoch 037: | Train Loss: 1.05267 | Test Loss: 1.02667\n",
      "Epoch 038: | Train Loss: 1.19971 | Test Loss: 0.99191\n",
      "Epoch 039: | Train Loss: 1.05594 | Test Loss: 0.96742\n",
      "Epoch 040: | Train Loss: 0.98964 | Test Loss: 1.02330\n",
      "Epoch 041: | Train Loss: 0.97083 | Test Loss: 0.97309\n",
      "Epoch 042: | Train Loss: 1.05009 | Test Loss: 0.94088\n",
      "Epoch 043: | Train Loss: 0.90851 | Test Loss: 0.93783\n",
      "Epoch 044: | Train Loss: 1.16053 | Test Loss: 0.97554\n",
      "Epoch 045: | Train Loss: 1.09480 | Test Loss: 0.99508\n",
      "Epoch 046: | Train Loss: 0.89256 | Test Loss: 0.89222\n",
      "Epoch 047: | Train Loss: 1.28473 | Test Loss: 0.90875\n",
      "Epoch 048: | Train Loss: 0.91883 | Test Loss: 0.90155\n",
      "Epoch 049: | Train Loss: 0.94122 | Test Loss: 0.94343\n",
      "Epoch 050: | Train Loss: 1.12564 | Test Loss: 0.86922\n",
      "Epoch 051: | Train Loss: 0.92451 | Test Loss: 0.95173\n",
      "Epoch 052: | Train Loss: 0.87146 | Test Loss: 0.91870\n",
      "Epoch 053: | Train Loss: 0.86601 | Test Loss: 0.87536\n",
      "Epoch 054: | Train Loss: 0.81199 | Test Loss: 0.84363\n",
      "Epoch 055: | Train Loss: 0.78526 | Test Loss: 0.84725\n",
      "Epoch 056: | Train Loss: 0.77923 | Test Loss: 0.84275\n",
      "Epoch 057: | Train Loss: 0.81336 | Test Loss: 0.83740\n",
      "Epoch 058: | Train Loss: 0.84920 | Test Loss: 0.84434\n",
      "Epoch 059: | Train Loss: 0.75384 | Test Loss: 0.83524\n",
      "Epoch 060: | Train Loss: 0.79404 | Test Loss: 0.84227\n",
      "Epoch 061: | Train Loss: 0.80894 | Test Loss: 0.84304\n",
      "Epoch 062: | Train Loss: 0.75790 | Test Loss: 0.83227\n",
      "Epoch 063: | Train Loss: 0.88377 | Test Loss: 0.87136\n",
      "Epoch 064: | Train Loss: 0.78456 | Test Loss: 0.86343\n",
      "Epoch 065: | Train Loss: 0.83182 | Test Loss: 0.80748\n",
      "Epoch 066: | Train Loss: 0.73751 | Test Loss: 0.84599\n",
      "Epoch 067: | Train Loss: 0.72851 | Test Loss: 0.79864\n",
      "Epoch 068: | Train Loss: 0.75236 | Test Loss: 0.79797\n",
      "Epoch 069: | Train Loss: 0.75713 | Test Loss: 0.81891\n",
      "Epoch 070: | Train Loss: 0.80692 | Test Loss: 0.80494\n",
      "Epoch 071: | Train Loss: 0.73303 | Test Loss: 0.90406\n",
      "Epoch 072: | Train Loss: 0.75093 | Test Loss: 0.83586\n",
      "Epoch 073: | Train Loss: 0.96509 | Test Loss: 0.82955\n",
      "Epoch 074: | Train Loss: 0.75816 | Test Loss: 0.89045\n",
      "Epoch 075: | Train Loss: 0.70012 | Test Loss: 0.83537\n",
      "Epoch 076: | Train Loss: 0.70990 | Test Loss: 0.81993\n",
      "Epoch 077: | Train Loss: 0.67629 | Test Loss: 0.84793\n",
      "Epoch 078: | Train Loss: 0.92399 | Test Loss: 0.80255\n",
      "Epoch 079: | Train Loss: 0.74938 | Test Loss: 0.81728\n",
      "Epoch 080: | Train Loss: 0.67902 | Test Loss: 0.93858\n",
      "Epoch 081: | Train Loss: 0.74537 | Test Loss: 0.81648\n",
      "Epoch 082: | Train Loss: 0.69046 | Test Loss: 0.82061\n",
      "Epoch 083: | Train Loss: 0.65774 | Test Loss: 0.82101\n",
      "Epoch 084: | Train Loss: 0.68002 | Test Loss: 0.78217\n",
      "Epoch 085: | Train Loss: 1.02413 | Test Loss: 0.78585\n",
      "Epoch 086: | Train Loss: 0.71285 | Test Loss: 0.87009\n",
      "Epoch 087: | Train Loss: 0.64957 | Test Loss: 0.80758\n",
      "Epoch 088: | Train Loss: 0.65625 | Test Loss: 0.79933\n",
      "Epoch 089: | Train Loss: 0.67376 | Test Loss: 0.83632\n",
      "Epoch 090: | Train Loss: 0.88294 | Test Loss: 0.81732\n",
      "Epoch 091: | Train Loss: 0.69361 | Test Loss: 0.84352\n",
      "Epoch 092: | Train Loss: 1.07593 | Test Loss: 0.78697\n",
      "Epoch 093: | Train Loss: 0.90582 | Test Loss: 0.80887\n",
      "Epoch 094: | Train Loss: 0.69426 | Test Loss: 0.92153\n",
      "Epoch 095: | Train Loss: 0.63621 | Test Loss: 0.77377\n",
      "Epoch 096: | Train Loss: 0.62926 | Test Loss: 0.78267\n",
      "Epoch 097: | Train Loss: 1.07855 | Test Loss: 0.78049\n",
      "Epoch 098: | Train Loss: 0.65675 | Test Loss: 0.79294\n",
      "Epoch 099: | Train Loss: 0.73385 | Test Loss: 0.84584\n",
      "Epoch 100: | Train Loss: 0.64646 | Test Loss: 0.79948\n",
      "Epoch 101: | Train Loss: 0.58018 | Test Loss: 0.86693\n",
      "Epoch 102: | Train Loss: 0.60140 | Test Loss: 0.80941\n",
      "Epoch 103: | Train Loss: 0.59702 | Test Loss: 0.79621\n",
      "Epoch 104: | Train Loss: 0.65412 | Test Loss: 0.84238\n",
      "Epoch 105: | Train Loss: 0.59907 | Test Loss: 0.88057\n",
      "Epoch 106: | Train Loss: 0.56405 | Test Loss: 0.82079\n",
      "Epoch 107: | Train Loss: 0.56092 | Test Loss: 0.83545\n",
      "Epoch 108: | Train Loss: 1.06233 | Test Loss: 0.81701\n",
      "Epoch 109: | Train Loss: 0.70273 | Test Loss: 0.86695\n",
      "Epoch 110: | Train Loss: 0.61983 | Test Loss: 0.88759\n",
      "Epoch 111: | Train Loss: 0.56598 | Test Loss: 0.84291\n",
      "Epoch 112: | Train Loss: 0.58168 | Test Loss: 0.83105\n",
      "Epoch 113: | Train Loss: 0.58739 | Test Loss: 0.82023\n",
      "Epoch 114: | Train Loss: 0.55739 | Test Loss: 0.83625\n",
      "Epoch 115: | Train Loss: 0.68533 | Test Loss: 0.80502\n",
      "Epoch 116: | Train Loss: 0.57421 | Test Loss: 0.80930\n",
      "Epoch 117: | Train Loss: 0.65753 | Test Loss: 0.86061\n",
      "Epoch 118: | Train Loss: 0.53865 | Test Loss: 0.80747\n",
      "Epoch 119: | Train Loss: 0.66090 | Test Loss: 0.82685\n",
      "Epoch 120: | Train Loss: 0.55526 | Test Loss: 0.80537\n",
      "Epoch 121: | Train Loss: 0.57824 | Test Loss: 0.83404\n",
      "Epoch 122: | Train Loss: 0.54175 | Test Loss: 0.80005\n",
      "Epoch 123: | Train Loss: 0.52199 | Test Loss: 0.82395\n",
      "Epoch 124: | Train Loss: 0.94842 | Test Loss: 0.79623\n",
      "Epoch 125: | Train Loss: 0.64487 | Test Loss: 0.84000\n",
      "Epoch 126: | Train Loss: 0.55047 | Test Loss: 0.94405\n",
      "Epoch 127: | Train Loss: 0.55534 | Test Loss: 0.81743\n",
      "Epoch 128: | Train Loss: 0.53542 | Test Loss: 0.80853\n",
      "Epoch 129: | Train Loss: 0.64110 | Test Loss: 0.81178\n",
      "Epoch 130: | Train Loss: 0.53497 | Test Loss: 0.81056\n",
      "Epoch 131: | Train Loss: 0.51713 | Test Loss: 0.83651\n",
      "Epoch 132: | Train Loss: 0.51611 | Test Loss: 0.82196\n",
      "Epoch 133: | Train Loss: 0.65594 | Test Loss: 0.81106\n",
      "Epoch 134: | Train Loss: 0.81875 | Test Loss: 0.83215\n",
      "Epoch 135: | Train Loss: 0.64269 | Test Loss: 0.98152\n",
      "Epoch 136: | Train Loss: 0.54850 | Test Loss: 0.80779\n",
      "Epoch 137: | Train Loss: 0.52920 | Test Loss: 0.81169\n",
      "Epoch 138: | Train Loss: 0.61682 | Test Loss: 0.81086\n",
      "Epoch 139: | Train Loss: 0.53067 | Test Loss: 0.78726\n",
      "Epoch 140: | Train Loss: 0.48782 | Test Loss: 0.81242\n",
      "Epoch 141: | Train Loss: 0.48010 | Test Loss: 0.79835\n",
      "Epoch 142: | Train Loss: 0.48284 | Test Loss: 0.79889\n",
      "Epoch 143: | Train Loss: 0.69842 | Test Loss: 0.78422\n",
      "Epoch 144: | Train Loss: 0.52906 | Test Loss: 0.80429\n",
      "Epoch 145: | Train Loss: 0.56873 | Test Loss: 0.84672\n",
      "Epoch 146: | Train Loss: 0.50367 | Test Loss: 0.81343\n",
      "Epoch 147: | Train Loss: 0.46644 | Test Loss: 0.85136\n",
      "Epoch 148: | Train Loss: 0.50784 | Test Loss: 0.81382\n",
      "Epoch 149: | Train Loss: 0.46380 | Test Loss: 0.84360\n",
      "Epoch 150: | Train Loss: 0.73910 | Test Loss: 0.80112\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015677928924560547,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbb75c0d5d24ad08025a80254b94d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 39.20443 | Test Loss: 36.09669\n",
      "Epoch 002: | Train Loss: 37.14779 | Test Loss: 34.80772\n",
      "Epoch 003: | Train Loss: 34.00085 | Test Loss: 33.17464\n",
      "Epoch 004: | Train Loss: 30.88955 | Test Loss: 29.56308\n",
      "Epoch 005: | Train Loss: 27.85116 | Test Loss: 22.17299\n",
      "Epoch 006: | Train Loss: 17.25932 | Test Loss: 10.72310\n",
      "Epoch 007: | Train Loss: 7.89205 | Test Loss: 4.12405\n",
      "Epoch 008: | Train Loss: 4.51573 | Test Loss: 6.16311\n",
      "Epoch 009: | Train Loss: 4.15451 | Test Loss: 3.68990\n",
      "Epoch 010: | Train Loss: 3.79139 | Test Loss: 3.59320\n",
      "Epoch 011: | Train Loss: 3.54587 | Test Loss: 3.35010\n",
      "Epoch 012: | Train Loss: 3.42541 | Test Loss: 3.36959\n",
      "Epoch 013: | Train Loss: 3.75177 | Test Loss: 3.10666\n",
      "Epoch 014: | Train Loss: 2.71411 | Test Loss: 2.99488\n",
      "Epoch 015: | Train Loss: 4.06815 | Test Loss: 2.75716\n",
      "Epoch 016: | Train Loss: 2.64207 | Test Loss: 2.91428\n",
      "Epoch 017: | Train Loss: 2.61541 | Test Loss: 2.57173\n",
      "Epoch 018: | Train Loss: 2.19777 | Test Loss: 2.36011\n",
      "Epoch 019: | Train Loss: 2.05835 | Test Loss: 2.22475\n",
      "Epoch 020: | Train Loss: 1.94503 | Test Loss: 2.13209\n",
      "Epoch 021: | Train Loss: 1.81199 | Test Loss: 1.96770\n",
      "Epoch 022: | Train Loss: 1.96387 | Test Loss: 1.83453\n",
      "Epoch 023: | Train Loss: 1.68580 | Test Loss: 1.72156\n",
      "Epoch 024: | Train Loss: 1.61416 | Test Loss: 1.62202\n",
      "Epoch 025: | Train Loss: 1.84808 | Test Loss: 1.53857\n",
      "Epoch 026: | Train Loss: 2.61270 | Test Loss: 1.47261\n",
      "Epoch 027: | Train Loss: 2.01606 | Test Loss: 1.60192\n",
      "Epoch 028: | Train Loss: 1.47215 | Test Loss: 1.44712\n",
      "Epoch 029: | Train Loss: 1.21956 | Test Loss: 1.31781\n",
      "Epoch 030: | Train Loss: 1.36638 | Test Loss: 1.27675\n",
      "Epoch 031: | Train Loss: 1.18521 | Test Loss: 1.24274\n",
      "Epoch 032: | Train Loss: 1.22686 | Test Loss: 1.26857\n",
      "Epoch 033: | Train Loss: 1.42115 | Test Loss: 1.22166\n",
      "Epoch 034: | Train Loss: 1.08466 | Test Loss: 1.13852\n",
      "Epoch 035: | Train Loss: 1.33727 | Test Loss: 1.10761\n",
      "Epoch 036: | Train Loss: 1.51112 | Test Loss: 1.14633\n",
      "Epoch 037: | Train Loss: 1.06890 | Test Loss: 1.12084\n",
      "Epoch 038: | Train Loss: 1.03930 | Test Loss: 1.01735\n",
      "Epoch 039: | Train Loss: 1.25849 | Test Loss: 1.03323\n",
      "Epoch 040: | Train Loss: 0.97683 | Test Loss: 1.02873\n",
      "Epoch 041: | Train Loss: 0.91685 | Test Loss: 0.96149\n",
      "Epoch 042: | Train Loss: 0.92965 | Test Loss: 0.95693\n",
      "Epoch 043: | Train Loss: 1.26422 | Test Loss: 1.00969\n",
      "Epoch 044: | Train Loss: 0.94384 | Test Loss: 0.95852\n",
      "Epoch 045: | Train Loss: 1.62127 | Test Loss: 0.92496\n",
      "Epoch 046: | Train Loss: 1.51161 | Test Loss: 0.94077\n",
      "Epoch 047: | Train Loss: 1.18130 | Test Loss: 1.05488\n",
      "Epoch 048: | Train Loss: 0.93278 | Test Loss: 0.98317\n",
      "Epoch 049: | Train Loss: 0.91429 | Test Loss: 0.94377\n",
      "Epoch 050: | Train Loss: 0.83822 | Test Loss: 0.90277\n",
      "Epoch 051: | Train Loss: 1.09478 | Test Loss: 0.90306\n",
      "Epoch 052: | Train Loss: 0.81711 | Test Loss: 0.89809\n",
      "Epoch 053: | Train Loss: 1.20497 | Test Loss: 0.89597\n",
      "Epoch 054: | Train Loss: 0.80403 | Test Loss: 0.93187\n",
      "Epoch 055: | Train Loss: 0.78406 | Test Loss: 0.87100\n",
      "Epoch 056: | Train Loss: 0.92250 | Test Loss: 0.88352\n",
      "Epoch 057: | Train Loss: 0.82362 | Test Loss: 0.89825\n",
      "Epoch 058: | Train Loss: 0.75719 | Test Loss: 0.86104\n",
      "Epoch 059: | Train Loss: 0.89405 | Test Loss: 0.85690\n",
      "Epoch 060: | Train Loss: 0.79133 | Test Loss: 0.85037\n",
      "Epoch 061: | Train Loss: 1.11613 | Test Loss: 0.86326\n",
      "Epoch 062: | Train Loss: 1.00639 | Test Loss: 0.85944\n",
      "Epoch 063: | Train Loss: 1.49997 | Test Loss: 0.89561\n",
      "Epoch 064: | Train Loss: 0.91799 | Test Loss: 0.93705\n",
      "Epoch 065: | Train Loss: 0.81055 | Test Loss: 0.95217\n",
      "Epoch 066: | Train Loss: 0.75256 | Test Loss: 0.86310\n",
      "Epoch 067: | Train Loss: 0.83405 | Test Loss: 0.85218\n",
      "Epoch 068: | Train Loss: 0.77974 | Test Loss: 0.85001\n",
      "Epoch 069: | Train Loss: 0.88667 | Test Loss: 0.87107\n",
      "Epoch 070: | Train Loss: 0.72930 | Test Loss: 0.86471\n",
      "Epoch 071: | Train Loss: 0.77283 | Test Loss: 0.88418\n",
      "Epoch 072: | Train Loss: 0.92907 | Test Loss: 0.90837\n",
      "Epoch 073: | Train Loss: 0.79228 | Test Loss: 0.85339\n",
      "Epoch 074: | Train Loss: 0.74333 | Test Loss: 0.86257\n",
      "Epoch 075: | Train Loss: 0.70338 | Test Loss: 0.86391\n",
      "Epoch 076: | Train Loss: 0.73679 | Test Loss: 0.85374\n",
      "Epoch 077: | Train Loss: 0.69104 | Test Loss: 0.83805\n",
      "Epoch 078: | Train Loss: 0.74100 | Test Loss: 0.86790\n",
      "Epoch 079: | Train Loss: 0.69868 | Test Loss: 0.87545\n",
      "Epoch 080: | Train Loss: 0.66344 | Test Loss: 0.82983\n",
      "Epoch 081: | Train Loss: 0.75659 | Test Loss: 0.82289\n",
      "Epoch 082: | Train Loss: 0.67283 | Test Loss: 0.82671\n",
      "Epoch 083: | Train Loss: 0.67283 | Test Loss: 0.83837\n",
      "Epoch 084: | Train Loss: 0.78682 | Test Loss: 0.81681\n",
      "Epoch 085: | Train Loss: 0.90722 | Test Loss: 0.81793\n",
      "Epoch 086: | Train Loss: 0.71630 | Test Loss: 0.81156\n",
      "Epoch 087: | Train Loss: 0.71980 | Test Loss: 0.83757\n",
      "Epoch 088: | Train Loss: 0.65102 | Test Loss: 0.83129\n",
      "Epoch 089: | Train Loss: 0.82094 | Test Loss: 0.79811\n",
      "Epoch 090: | Train Loss: 0.81480 | Test Loss: 0.79839\n",
      "Epoch 091: | Train Loss: 0.68502 | Test Loss: 0.88813\n",
      "Epoch 092: | Train Loss: 0.63336 | Test Loss: 0.80073\n",
      "Epoch 093: | Train Loss: 0.84209 | Test Loss: 0.83258\n",
      "Epoch 094: | Train Loss: 0.66451 | Test Loss: 0.87101\n",
      "Epoch 095: | Train Loss: 0.62292 | Test Loss: 0.79856\n",
      "Epoch 096: | Train Loss: 0.62853 | Test Loss: 0.80811\n",
      "Epoch 097: | Train Loss: 0.60881 | Test Loss: 0.81914\n",
      "Epoch 098: | Train Loss: 0.63368 | Test Loss: 0.79333\n",
      "Epoch 099: | Train Loss: 0.73364 | Test Loss: 0.79528\n",
      "Epoch 100: | Train Loss: 0.67874 | Test Loss: 0.80237\n",
      "Epoch 101: | Train Loss: 0.81394 | Test Loss: 0.80562\n",
      "Epoch 102: | Train Loss: 0.75566 | Test Loss: 0.80447\n",
      "Epoch 103: | Train Loss: 0.84054 | Test Loss: 0.85002\n",
      "Epoch 104: | Train Loss: 0.64127 | Test Loss: 0.81682\n",
      "Epoch 105: | Train Loss: 0.61919 | Test Loss: 0.86980\n",
      "Epoch 106: | Train Loss: 0.83387 | Test Loss: 0.81400\n",
      "Epoch 107: | Train Loss: 0.63547 | Test Loss: 0.86932\n",
      "Epoch 108: | Train Loss: 0.60586 | Test Loss: 0.79874\n",
      "Epoch 109: | Train Loss: 0.91121 | Test Loss: 0.80264\n",
      "Epoch 110: | Train Loss: 0.67194 | Test Loss: 0.89123\n",
      "Epoch 111: | Train Loss: 0.60260 | Test Loss: 0.82368\n",
      "Epoch 112: | Train Loss: 0.64208 | Test Loss: 0.86176\n",
      "Epoch 113: | Train Loss: 0.65270 | Test Loss: 0.82785\n",
      "Epoch 114: | Train Loss: 0.59797 | Test Loss: 0.80594\n",
      "Epoch 115: | Train Loss: 0.61788 | Test Loss: 0.83448\n",
      "Epoch 116: | Train Loss: 1.17843 | Test Loss: 0.81389\n",
      "Epoch 117: | Train Loss: 0.79029 | Test Loss: 0.95132\n",
      "Epoch 118: | Train Loss: 0.61394 | Test Loss: 0.81907\n",
      "Epoch 119: | Train Loss: 0.56252 | Test Loss: 0.81886\n",
      "Epoch 120: | Train Loss: 0.62123 | Test Loss: 0.84191\n",
      "Epoch 121: | Train Loss: 0.57111 | Test Loss: 0.82571\n",
      "Epoch 122: | Train Loss: 0.64026 | Test Loss: 0.81000\n",
      "Epoch 123: | Train Loss: 0.63506 | Test Loss: 0.87035\n",
      "Epoch 124: | Train Loss: 0.56333 | Test Loss: 0.80387\n",
      "Epoch 125: | Train Loss: 0.55497 | Test Loss: 0.81051\n",
      "Epoch 126: | Train Loss: 0.66995 | Test Loss: 0.87103\n",
      "Epoch 127: | Train Loss: 0.58361 | Test Loss: 0.81751\n",
      "Epoch 128: | Train Loss: 0.53768 | Test Loss: 0.80904\n",
      "Epoch 129: | Train Loss: 0.52006 | Test Loss: 0.83092\n",
      "Epoch 130: | Train Loss: 0.57480 | Test Loss: 0.83333\n",
      "Epoch 131: | Train Loss: 0.52780 | Test Loss: 0.81112\n",
      "Epoch 132: | Train Loss: 0.54939 | Test Loss: 0.80464\n",
      "Epoch 133: | Train Loss: 0.54000 | Test Loss: 0.85599\n",
      "Epoch 134: | Train Loss: 0.54791 | Test Loss: 0.81621\n",
      "Epoch 135: | Train Loss: 0.55300 | Test Loss: 0.81760\n",
      "Epoch 136: | Train Loss: 0.53404 | Test Loss: 0.84495\n",
      "Epoch 137: | Train Loss: 0.56599 | Test Loss: 0.81398\n",
      "Epoch 138: | Train Loss: 0.54061 | Test Loss: 0.81429\n",
      "Epoch 139: | Train Loss: 0.58666 | Test Loss: 0.82274\n",
      "Epoch 140: | Train Loss: 0.51309 | Test Loss: 0.80935\n",
      "Epoch 141: | Train Loss: 0.65149 | Test Loss: 0.81777\n",
      "Epoch 142: | Train Loss: 0.52569 | Test Loss: 0.79801\n",
      "Epoch 143: | Train Loss: 0.49568 | Test Loss: 0.82821\n",
      "Epoch 144: | Train Loss: 0.59117 | Test Loss: 0.79210\n",
      "Epoch 145: | Train Loss: 0.53918 | Test Loss: 0.79474\n",
      "Epoch 146: | Train Loss: 0.52577 | Test Loss: 0.81362\n",
      "Epoch 147: | Train Loss: 0.61311 | Test Loss: 0.78495\n",
      "Epoch 148: | Train Loss: 0.70938 | Test Loss: 0.79095\n",
      "Epoch 149: | Train Loss: 0.56900 | Test Loss: 0.81156\n",
      "Epoch 150: | Train Loss: 0.50814 | Test Loss: 0.89333\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0158541202545166,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457ff983f31443a4ad386814881ea7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 38.01619 | Test Loss: 37.34634\n",
      "Epoch 002: | Train Loss: 40.02786 | Test Loss: 36.17129\n",
      "Epoch 003: | Train Loss: 38.43905 | Test Loss: 34.16525\n",
      "Epoch 004: | Train Loss: 31.40361 | Test Loss: 30.01441\n",
      "Epoch 005: | Train Loss: 27.86139 | Test Loss: 21.51811\n",
      "Epoch 006: | Train Loss: 15.92105 | Test Loss: 8.83564\n",
      "Epoch 007: | Train Loss: 5.35376 | Test Loss: 4.84386\n",
      "Epoch 008: | Train Loss: 4.72726 | Test Loss: 4.57265\n",
      "Epoch 009: | Train Loss: 3.76982 | Test Loss: 3.55674\n",
      "Epoch 010: | Train Loss: 3.34460 | Test Loss: 3.35959\n",
      "Epoch 011: | Train Loss: 4.33081 | Test Loss: 3.31736\n",
      "Epoch 012: | Train Loss: 4.13832 | Test Loss: 3.29286\n",
      "Epoch 013: | Train Loss: 2.70624 | Test Loss: 2.82718\n",
      "Epoch 014: | Train Loss: 2.68338 | Test Loss: 2.62283\n",
      "Epoch 015: | Train Loss: 2.43122 | Test Loss: 2.53609\n",
      "Epoch 016: | Train Loss: 2.21286 | Test Loss: 2.39955\n",
      "Epoch 017: | Train Loss: 2.11656 | Test Loss: 2.24811\n",
      "Epoch 018: | Train Loss: 2.36314 | Test Loss: 2.14785\n",
      "Epoch 019: | Train Loss: 2.82230 | Test Loss: 2.04139\n",
      "Epoch 020: | Train Loss: 1.81884 | Test Loss: 2.08745\n",
      "Epoch 021: | Train Loss: 1.77116 | Test Loss: 1.80096\n",
      "Epoch 022: | Train Loss: 1.64280 | Test Loss: 1.72662\n",
      "Epoch 023: | Train Loss: 1.52920 | Test Loss: 1.65736\n",
      "Epoch 024: | Train Loss: 1.46957 | Test Loss: 1.56847\n",
      "Epoch 025: | Train Loss: 1.42841 | Test Loss: 1.48111\n",
      "Epoch 026: | Train Loss: 1.33839 | Test Loss: 1.41823\n",
      "Epoch 027: | Train Loss: 1.32830 | Test Loss: 1.35784\n",
      "Epoch 028: | Train Loss: 1.25840 | Test Loss: 1.32314\n",
      "Epoch 029: | Train Loss: 1.27898 | Test Loss: 1.25702\n",
      "Epoch 030: | Train Loss: 1.25946 | Test Loss: 1.24308\n",
      "Epoch 031: | Train Loss: 1.28523 | Test Loss: 1.19242\n",
      "Epoch 032: | Train Loss: 1.22058 | Test Loss: 1.20193\n",
      "Epoch 033: | Train Loss: 1.20790 | Test Loss: 1.13677\n",
      "Epoch 034: | Train Loss: 1.11004 | Test Loss: 1.12160\n",
      "Epoch 035: | Train Loss: 1.63722 | Test Loss: 1.07745\n",
      "Epoch 036: | Train Loss: 1.09946 | Test Loss: 1.08724\n",
      "Epoch 037: | Train Loss: 1.06435 | Test Loss: 1.00119\n",
      "Epoch 038: | Train Loss: 0.95583 | Test Loss: 1.04441\n",
      "Epoch 039: | Train Loss: 1.24637 | Test Loss: 0.96584\n",
      "Epoch 040: | Train Loss: 0.92843 | Test Loss: 0.95160\n",
      "Epoch 041: | Train Loss: 0.89698 | Test Loss: 0.93904\n",
      "Epoch 042: | Train Loss: 1.03065 | Test Loss: 0.93985\n",
      "Epoch 043: | Train Loss: 1.05537 | Test Loss: 0.90395\n",
      "Epoch 044: | Train Loss: 0.89956 | Test Loss: 0.91001\n",
      "Epoch 045: | Train Loss: 0.85069 | Test Loss: 0.94967\n",
      "Epoch 046: | Train Loss: 0.82578 | Test Loss: 0.88684\n",
      "Epoch 047: | Train Loss: 0.89496 | Test Loss: 0.88257\n",
      "Epoch 048: | Train Loss: 0.85001 | Test Loss: 0.87847\n",
      "Epoch 049: | Train Loss: 0.86602 | Test Loss: 0.92083\n",
      "Epoch 050: | Train Loss: 0.79890 | Test Loss: 0.86959\n",
      "Epoch 051: | Train Loss: 0.86146 | Test Loss: 0.86091\n",
      "Epoch 052: | Train Loss: 0.89895 | Test Loss: 0.86137\n",
      "Epoch 053: | Train Loss: 0.76823 | Test Loss: 0.87197\n",
      "Epoch 054: | Train Loss: 0.80643 | Test Loss: 0.92353\n",
      "Epoch 055: | Train Loss: 0.87244 | Test Loss: 0.88722\n",
      "Epoch 056: | Train Loss: 0.75030 | Test Loss: 0.86864\n",
      "Epoch 057: | Train Loss: 0.71708 | Test Loss: 0.85311\n",
      "Epoch 058: | Train Loss: 0.84993 | Test Loss: 0.87352\n",
      "Epoch 059: | Train Loss: 0.82251 | Test Loss: 0.85959\n",
      "Epoch 060: | Train Loss: 0.79170 | Test Loss: 0.85762\n",
      "Epoch 061: | Train Loss: 0.74345 | Test Loss: 0.85671\n",
      "Epoch 062: | Train Loss: 0.80600 | Test Loss: 0.85160\n",
      "Epoch 063: | Train Loss: 1.19642 | Test Loss: 0.83001\n",
      "Epoch 064: | Train Loss: 0.75059 | Test Loss: 0.90500\n",
      "Epoch 065: | Train Loss: 0.68365 | Test Loss: 0.83365\n",
      "Epoch 066: | Train Loss: 1.02231 | Test Loss: 0.83458\n",
      "Epoch 067: | Train Loss: 0.70629 | Test Loss: 0.83801\n",
      "Epoch 068: | Train Loss: 0.67018 | Test Loss: 0.87622\n",
      "Epoch 069: | Train Loss: 0.67952 | Test Loss: 0.84027\n",
      "Epoch 070: | Train Loss: 0.72962 | Test Loss: 0.83163\n",
      "Epoch 071: | Train Loss: 0.65856 | Test Loss: 0.88670\n",
      "Epoch 072: | Train Loss: 0.64068 | Test Loss: 0.82479\n",
      "Epoch 073: | Train Loss: 0.95963 | Test Loss: 0.82219\n",
      "Epoch 074: | Train Loss: 0.81574 | Test Loss: 0.85434\n",
      "Epoch 075: | Train Loss: 1.03153 | Test Loss: 0.85348\n",
      "Epoch 076: | Train Loss: 0.84374 | Test Loss: 0.88254\n",
      "Epoch 077: | Train Loss: 0.70226 | Test Loss: 0.88995\n",
      "Epoch 078: | Train Loss: 0.62704 | Test Loss: 0.84617\n",
      "Epoch 079: | Train Loss: 0.70408 | Test Loss: 0.90222\n",
      "Epoch 080: | Train Loss: 0.63101 | Test Loss: 0.83710\n",
      "Epoch 081: | Train Loss: 0.70060 | Test Loss: 0.80940\n",
      "Epoch 082: | Train Loss: 0.67008 | Test Loss: 0.84698\n",
      "Epoch 083: | Train Loss: 0.69547 | Test Loss: 0.82048\n",
      "Epoch 084: | Train Loss: 0.61376 | Test Loss: 0.95629\n",
      "Epoch 085: | Train Loss: 0.63241 | Test Loss: 0.84211\n",
      "Epoch 086: | Train Loss: 0.74257 | Test Loss: 0.85912\n",
      "Epoch 087: | Train Loss: 0.65338 | Test Loss: 0.92501\n",
      "Epoch 088: | Train Loss: 0.59319 | Test Loss: 0.82957\n",
      "Epoch 089: | Train Loss: 0.82667 | Test Loss: 0.83200\n",
      "Epoch 090: | Train Loss: 0.62096 | Test Loss: 0.83827\n",
      "Epoch 091: | Train Loss: 0.60661 | Test Loss: 0.85560\n",
      "Epoch 092: | Train Loss: 0.57533 | Test Loss: 0.82548\n",
      "Epoch 093: | Train Loss: 0.55236 | Test Loss: 0.86704\n",
      "Epoch 094: | Train Loss: 0.69408 | Test Loss: 0.84004\n",
      "Epoch 095: | Train Loss: 0.59024 | Test Loss: 0.86777\n",
      "Epoch 096: | Train Loss: 0.56229 | Test Loss: 0.84534\n",
      "Epoch 097: | Train Loss: 0.56898 | Test Loss: 0.86953\n",
      "Epoch 098: | Train Loss: 0.54115 | Test Loss: 0.83508\n",
      "Epoch 099: | Train Loss: 0.69075 | Test Loss: 0.83583\n",
      "Epoch 100: | Train Loss: 0.55400 | Test Loss: 0.84556\n",
      "Epoch 101: | Train Loss: 0.53652 | Test Loss: 0.89002\n",
      "Epoch 102: | Train Loss: 0.52126 | Test Loss: 0.83835\n",
      "Epoch 103: | Train Loss: 0.52797 | Test Loss: 0.83833\n",
      "Epoch 104: | Train Loss: 0.50790 | Test Loss: 0.85952\n",
      "Epoch 105: | Train Loss: 0.54244 | Test Loss: 0.83012\n",
      "Epoch 106: | Train Loss: 0.52419 | Test Loss: 0.83391\n",
      "Epoch 107: | Train Loss: 0.54305 | Test Loss: 0.86723\n",
      "Epoch 108: | Train Loss: 0.55153 | Test Loss: 0.81904\n",
      "Epoch 109: | Train Loss: 0.57706 | Test Loss: 0.82035\n",
      "Epoch 110: | Train Loss: 0.53069 | Test Loss: 0.85793\n",
      "Epoch 111: | Train Loss: 0.65969 | Test Loss: 0.81444\n",
      "Epoch 112: | Train Loss: 0.71621 | Test Loss: 0.82438\n",
      "Epoch 113: | Train Loss: 0.73325 | Test Loss: 0.85089\n",
      "Epoch 114: | Train Loss: 0.55933 | Test Loss: 0.95484\n",
      "Epoch 115: | Train Loss: 0.52668 | Test Loss: 0.81810\n",
      "Epoch 116: | Train Loss: 0.58434 | Test Loss: 0.94018\n",
      "Epoch 117: | Train Loss: 0.56586 | Test Loss: 0.82589\n",
      "Epoch 118: | Train Loss: 0.56756 | Test Loss: 0.82385\n",
      "Epoch 119: | Train Loss: 0.53225 | Test Loss: 0.86310\n",
      "Epoch 120: | Train Loss: 0.53767 | Test Loss: 0.79959\n",
      "Epoch 121: | Train Loss: 0.51231 | Test Loss: 0.84039\n",
      "Epoch 122: | Train Loss: 0.57709 | Test Loss: 0.80228\n",
      "Epoch 123: | Train Loss: 0.52313 | Test Loss: 0.82065\n",
      "Epoch 124: | Train Loss: 0.56649 | Test Loss: 0.89212\n",
      "Epoch 125: | Train Loss: 0.51254 | Test Loss: 0.84200\n",
      "Epoch 126: | Train Loss: 0.48066 | Test Loss: 0.82440\n",
      "Epoch 127: | Train Loss: 0.46671 | Test Loss: 0.86274\n",
      "Epoch 128: | Train Loss: 0.49820 | Test Loss: 0.82720\n",
      "Epoch 129: | Train Loss: 0.61496 | Test Loss: 0.82299\n",
      "Epoch 130: | Train Loss: 0.70608 | Test Loss: 0.85277\n",
      "Epoch 131: | Train Loss: 0.68352 | Test Loss: 0.82677\n",
      "Epoch 132: | Train Loss: 0.63610 | Test Loss: 0.83226\n",
      "Epoch 133: | Train Loss: 0.56710 | Test Loss: 0.96055\n",
      "Epoch 134: | Train Loss: 0.49117 | Test Loss: 0.84002\n",
      "Epoch 135: | Train Loss: 0.49735 | Test Loss: 0.91448\n",
      "Epoch 136: | Train Loss: 0.46287 | Test Loss: 0.82749\n",
      "Epoch 137: | Train Loss: 0.47375 | Test Loss: 0.84647\n",
      "Epoch 138: | Train Loss: 0.48864 | Test Loss: 0.84861\n",
      "Epoch 139: | Train Loss: 0.45919 | Test Loss: 0.84154\n",
      "Epoch 140: | Train Loss: 0.48889 | Test Loss: 0.81850\n",
      "Epoch 141: | Train Loss: 0.56767 | Test Loss: 0.82524\n",
      "Epoch 142: | Train Loss: 0.45796 | Test Loss: 0.84393\n",
      "Epoch 143: | Train Loss: 0.45061 | Test Loss: 0.87147\n",
      "Epoch 144: | Train Loss: 0.45189 | Test Loss: 0.83744\n",
      "Epoch 145: | Train Loss: 0.42575 | Test Loss: 0.83419\n",
      "Epoch 146: | Train Loss: 0.47341 | Test Loss: 0.84431\n",
      "Epoch 147: | Train Loss: 0.50472 | Test Loss: 0.87833\n",
      "Epoch 148: | Train Loss: 0.60261 | Test Loss: 0.87214\n",
      "Epoch 149: | Train Loss: 0.43946 | Test Loss: 0.84293\n",
      "Epoch 150: | Train Loss: 0.49302 | Test Loss: 0.85585\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016025781631469727,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544eefaef0be415cbd06f0bd29949249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 38.97132 | Test Loss: 36.00553\n",
      "Epoch 002: | Train Loss: 36.05379 | Test Loss: 34.79288\n",
      "Epoch 003: | Train Loss: 33.51428 | Test Loss: 32.41258\n",
      "Epoch 004: | Train Loss: 30.51495 | Test Loss: 27.71301\n",
      "Epoch 005: | Train Loss: 25.58187 | Test Loss: 19.53843\n",
      "Epoch 006: | Train Loss: 15.91261 | Test Loss: 8.97619\n",
      "Epoch 007: | Train Loss: 5.61376 | Test Loss: 3.85070\n",
      "Epoch 008: | Train Loss: 4.17146 | Test Loss: 5.52805\n",
      "Epoch 009: | Train Loss: 3.72408 | Test Loss: 3.55504\n",
      "Epoch 010: | Train Loss: 3.45995 | Test Loss: 3.35084\n",
      "Epoch 011: | Train Loss: 3.10019 | Test Loss: 3.16285\n",
      "Epoch 012: | Train Loss: 2.80269 | Test Loss: 3.14996\n",
      "Epoch 013: | Train Loss: 2.72868 | Test Loss: 2.89779\n",
      "Epoch 014: | Train Loss: 2.61176 | Test Loss: 2.79720\n",
      "Epoch 015: | Train Loss: 3.58920 | Test Loss: 2.65864\n",
      "Epoch 016: | Train Loss: 2.28538 | Test Loss: 2.62996\n",
      "Epoch 017: | Train Loss: 2.18313 | Test Loss: 2.42512\n",
      "Epoch 018: | Train Loss: 2.26089 | Test Loss: 2.32109\n",
      "Epoch 019: | Train Loss: 2.08183 | Test Loss: 2.28966\n",
      "Epoch 020: | Train Loss: 1.92108 | Test Loss: 2.13688\n",
      "Epoch 021: | Train Loss: 1.84894 | Test Loss: 2.06320\n",
      "Epoch 022: | Train Loss: 1.77831 | Test Loss: 1.99400\n",
      "Epoch 023: | Train Loss: 2.07819 | Test Loss: 1.91981\n",
      "Epoch 024: | Train Loss: 1.77194 | Test Loss: 1.91208\n",
      "Epoch 025: | Train Loss: 1.60106 | Test Loss: 1.77237\n",
      "Epoch 026: | Train Loss: 2.38247 | Test Loss: 1.73120\n",
      "Epoch 027: | Train Loss: 1.75269 | Test Loss: 1.72220\n",
      "Epoch 028: | Train Loss: 1.57323 | Test Loss: 1.58631\n",
      "Epoch 029: | Train Loss: 1.57412 | Test Loss: 1.52783\n",
      "Epoch 030: | Train Loss: 1.31351 | Test Loss: 1.46886\n",
      "Epoch 031: | Train Loss: 2.16023 | Test Loss: 1.46773\n",
      "Epoch 032: | Train Loss: 1.35689 | Test Loss: 1.46643\n",
      "Epoch 033: | Train Loss: 1.90781 | Test Loss: 1.30287\n",
      "Epoch 034: | Train Loss: 1.82857 | Test Loss: 1.34665\n",
      "Epoch 035: | Train Loss: 1.20464 | Test Loss: 1.27234\n",
      "Epoch 036: | Train Loss: 1.20266 | Test Loss: 1.14194\n",
      "Epoch 037: | Train Loss: 1.03476 | Test Loss: 1.13742\n",
      "Epoch 038: | Train Loss: 1.01666 | Test Loss: 1.09691\n",
      "Epoch 039: | Train Loss: 1.18758 | Test Loss: 1.07353\n",
      "Epoch 040: | Train Loss: 1.08943 | Test Loss: 1.07251\n",
      "Epoch 041: | Train Loss: 1.01156 | Test Loss: 1.02666\n",
      "Epoch 042: | Train Loss: 1.06221 | Test Loss: 1.03656\n",
      "Epoch 043: | Train Loss: 0.96610 | Test Loss: 1.00136\n",
      "Epoch 044: | Train Loss: 0.97660 | Test Loss: 0.99681\n",
      "Epoch 045: | Train Loss: 1.07327 | Test Loss: 0.99608\n",
      "Epoch 046: | Train Loss: 0.91597 | Test Loss: 1.02911\n",
      "Epoch 047: | Train Loss: 0.95648 | Test Loss: 0.97596\n",
      "Epoch 048: | Train Loss: 0.89888 | Test Loss: 0.96346\n",
      "Epoch 049: | Train Loss: 1.19027 | Test Loss: 0.94679\n",
      "Epoch 050: | Train Loss: 0.88058 | Test Loss: 0.95781\n",
      "Epoch 051: | Train Loss: 0.89256 | Test Loss: 0.92519\n",
      "Epoch 052: | Train Loss: 0.78949 | Test Loss: 0.90776\n",
      "Epoch 053: | Train Loss: 0.79738 | Test Loss: 0.89938\n",
      "Epoch 054: | Train Loss: 0.77119 | Test Loss: 0.89411\n",
      "Epoch 055: | Train Loss: 0.89494 | Test Loss: 0.88014\n",
      "Epoch 056: | Train Loss: 0.82568 | Test Loss: 0.88313\n",
      "Epoch 057: | Train Loss: 0.93711 | Test Loss: 0.88485\n",
      "Epoch 058: | Train Loss: 0.76229 | Test Loss: 0.88519\n",
      "Epoch 059: | Train Loss: 0.73349 | Test Loss: 0.90406\n",
      "Epoch 060: | Train Loss: 1.36886 | Test Loss: 0.86430\n",
      "Epoch 061: | Train Loss: 0.86792 | Test Loss: 0.93280\n",
      "Epoch 062: | Train Loss: 0.74762 | Test Loss: 0.99355\n",
      "Epoch 063: | Train Loss: 0.76606 | Test Loss: 0.85392\n",
      "Epoch 064: | Train Loss: 0.67164 | Test Loss: 0.85779\n",
      "Epoch 065: | Train Loss: 0.71578 | Test Loss: 0.86572\n",
      "Epoch 066: | Train Loss: 0.75443 | Test Loss: 0.85838\n",
      "Epoch 067: | Train Loss: 0.68145 | Test Loss: 0.89480\n",
      "Epoch 068: | Train Loss: 0.68901 | Test Loss: 0.85395\n",
      "Epoch 069: | Train Loss: 0.65678 | Test Loss: 0.85150\n",
      "Epoch 070: | Train Loss: 0.64163 | Test Loss: 0.88161\n",
      "Epoch 071: | Train Loss: 0.65258 | Test Loss: 0.85814\n",
      "Epoch 072: | Train Loss: 0.61913 | Test Loss: 0.86080\n",
      "Epoch 073: | Train Loss: 0.92428 | Test Loss: 0.86224\n",
      "Epoch 074: | Train Loss: 0.63313 | Test Loss: 0.89748\n",
      "Epoch 075: | Train Loss: 0.76022 | Test Loss: 0.85036\n",
      "Epoch 076: | Train Loss: 0.62783 | Test Loss: 0.89806\n",
      "Epoch 077: | Train Loss: 0.59585 | Test Loss: 0.85189\n",
      "Epoch 078: | Train Loss: 0.62682 | Test Loss: 0.86403\n",
      "Epoch 079: | Train Loss: 0.64606 | Test Loss: 0.89159\n",
      "Epoch 080: | Train Loss: 0.60020 | Test Loss: 0.86184\n",
      "Epoch 081: | Train Loss: 0.63525 | Test Loss: 0.85798\n",
      "Epoch 082: | Train Loss: 0.57454 | Test Loss: 0.89908\n",
      "Epoch 083: | Train Loss: 0.56332 | Test Loss: 0.85907\n",
      "Epoch 084: | Train Loss: 0.63318 | Test Loss: 0.89477\n",
      "Epoch 085: | Train Loss: 0.58829 | Test Loss: 0.87788\n",
      "Epoch 086: | Train Loss: 0.56077 | Test Loss: 0.85745\n",
      "Epoch 087: | Train Loss: 0.54475 | Test Loss: 0.89062\n",
      "Epoch 088: | Train Loss: 0.72736 | Test Loss: 0.85308\n",
      "Epoch 089: | Train Loss: 0.60358 | Test Loss: 0.86646\n",
      "Epoch 090: | Train Loss: 0.55649 | Test Loss: 0.89463\n",
      "Epoch 091: | Train Loss: 0.56019 | Test Loss: 0.85710\n",
      "Epoch 092: | Train Loss: 0.53397 | Test Loss: 0.87263\n",
      "Epoch 093: | Train Loss: 0.70963 | Test Loss: 0.85746\n",
      "Epoch 094: | Train Loss: 0.53913 | Test Loss: 0.85808\n",
      "Epoch 095: | Train Loss: 0.59397 | Test Loss: 0.88868\n",
      "Epoch 096: | Train Loss: 0.51430 | Test Loss: 0.85567\n",
      "Epoch 097: | Train Loss: 1.24031 | Test Loss: 0.85687\n",
      "Epoch 098: | Train Loss: 0.67608 | Test Loss: 0.90978\n",
      "Epoch 099: | Train Loss: 0.72666 | Test Loss: 0.90026\n",
      "Epoch 100: | Train Loss: 0.51976 | Test Loss: 0.85765\n",
      "Epoch 101: | Train Loss: 0.52314 | Test Loss: 0.89272\n",
      "Epoch 102: | Train Loss: 0.52780 | Test Loss: 0.88925\n",
      "Epoch 103: | Train Loss: 0.62118 | Test Loss: 0.86308\n",
      "Epoch 104: | Train Loss: 1.17826 | Test Loss: 0.86910\n",
      "Epoch 105: | Train Loss: 0.57000 | Test Loss: 0.88008\n",
      "Epoch 106: | Train Loss: 0.51755 | Test Loss: 0.93773\n",
      "Epoch 107: | Train Loss: 0.51847 | Test Loss: 0.88418\n",
      "Epoch 108: | Train Loss: 0.69168 | Test Loss: 0.91464\n",
      "Epoch 109: | Train Loss: 0.81278 | Test Loss: 0.90853\n",
      "Epoch 110: | Train Loss: 0.87999 | Test Loss: 0.99900\n",
      "Epoch 111: | Train Loss: 0.64154 | Test Loss: 0.96050\n",
      "Epoch 112: | Train Loss: 0.51103 | Test Loss: 1.00692\n",
      "Epoch 113: | Train Loss: 0.52018 | Test Loss: 0.95543\n",
      "Epoch 114: | Train Loss: 0.52757 | Test Loss: 0.94774\n",
      "Epoch 115: | Train Loss: 0.53142 | Test Loss: 0.95993\n",
      "Epoch 116: | Train Loss: 0.55995 | Test Loss: 1.00677\n",
      "Epoch 117: | Train Loss: 0.49530 | Test Loss: 0.92980\n",
      "Epoch 118: | Train Loss: 0.50115 | Test Loss: 0.95302\n",
      "Epoch 119: | Train Loss: 0.49707 | Test Loss: 0.96601\n",
      "Epoch 120: | Train Loss: 0.53018 | Test Loss: 0.94276\n",
      "Epoch 121: | Train Loss: 0.54182 | Test Loss: 0.97768\n",
      "Epoch 122: | Train Loss: 0.51590 | Test Loss: 1.00104\n",
      "Epoch 123: | Train Loss: 0.49004 | Test Loss: 0.92897\n",
      "Epoch 124: | Train Loss: 0.51424 | Test Loss: 0.93689\n",
      "Epoch 125: | Train Loss: 0.48389 | Test Loss: 0.92045\n",
      "Epoch 126: | Train Loss: 0.46247 | Test Loss: 0.93246\n",
      "Epoch 127: | Train Loss: 0.45953 | Test Loss: 0.92902\n",
      "Epoch 128: | Train Loss: 0.47217 | Test Loss: 0.92259\n",
      "Epoch 129: | Train Loss: 0.47370 | Test Loss: 0.94048\n",
      "Epoch 130: | Train Loss: 0.62110 | Test Loss: 0.93210\n",
      "Epoch 131: | Train Loss: 0.53197 | Test Loss: 1.00800\n",
      "Epoch 132: | Train Loss: 0.47931 | Test Loss: 0.91754\n",
      "Epoch 133: | Train Loss: 0.47289 | Test Loss: 0.92258\n",
      "Epoch 134: | Train Loss: 0.47703 | Test Loss: 0.90408\n",
      "Epoch 135: | Train Loss: 0.50761 | Test Loss: 0.92429\n",
      "Epoch 136: | Train Loss: 0.46488 | Test Loss: 0.94616\n",
      "Epoch 137: | Train Loss: 0.43756 | Test Loss: 0.91136\n",
      "Epoch 138: | Train Loss: 0.45753 | Test Loss: 0.92111\n",
      "Epoch 139: | Train Loss: 0.43628 | Test Loss: 0.92744\n",
      "Epoch 140: | Train Loss: 0.43985 | Test Loss: 0.93091\n",
      "Epoch 141: | Train Loss: 0.55891 | Test Loss: 0.93106\n",
      "Epoch 142: | Train Loss: 0.47486 | Test Loss: 0.97671\n",
      "Epoch 143: | Train Loss: 0.43377 | Test Loss: 0.91120\n",
      "Epoch 144: | Train Loss: 0.75093 | Test Loss: 0.91997\n",
      "Epoch 145: | Train Loss: 0.45073 | Test Loss: 0.93711\n",
      "Epoch 146: | Train Loss: 0.43678 | Test Loss: 0.95263\n",
      "Epoch 147: | Train Loss: 0.64412 | Test Loss: 0.93179\n",
      "Epoch 148: | Train Loss: 0.48553 | Test Loss: 0.93530\n",
      "Epoch 149: | Train Loss: 0.62511 | Test Loss: 1.01035\n",
      "Epoch 150: | Train Loss: 0.49912 | Test Loss: 0.94240\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01646280288696289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ae56b5eb1946d7933306c6116803c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 37.51965 | Test Loss: 38.41592\n",
      "Epoch 002: | Train Loss: 38.95042 | Test Loss: 37.45041\n",
      "Epoch 003: | Train Loss: 36.91114 | Test Loss: 35.87227\n",
      "Epoch 004: | Train Loss: 37.44651 | Test Loss: 33.22080\n",
      "Epoch 005: | Train Loss: 33.98095 | Test Loss: 28.16352\n",
      "Epoch 006: | Train Loss: 28.23754 | Test Loss: 19.54292\n",
      "Epoch 007: | Train Loss: 16.37432 | Test Loss: 8.84146\n",
      "Epoch 008: | Train Loss: 5.77002 | Test Loss: 3.96318\n",
      "Epoch 009: | Train Loss: 3.92883 | Test Loss: 5.40565\n",
      "Epoch 010: | Train Loss: 3.89996 | Test Loss: 3.66315\n",
      "Epoch 011: | Train Loss: 3.04591 | Test Loss: 3.35616\n",
      "Epoch 012: | Train Loss: 3.46073 | Test Loss: 3.17819\n",
      "Epoch 013: | Train Loss: 2.79710 | Test Loss: 3.09650\n",
      "Epoch 014: | Train Loss: 2.63038 | Test Loss: 2.92832\n",
      "Epoch 015: | Train Loss: 2.91995 | Test Loss: 2.65156\n",
      "Epoch 016: | Train Loss: 2.42109 | Test Loss: 2.46612\n",
      "Epoch 017: | Train Loss: 2.19946 | Test Loss: 2.24874\n",
      "Epoch 018: | Train Loss: 1.98721 | Test Loss: 2.09314\n",
      "Epoch 019: | Train Loss: 1.83765 | Test Loss: 1.98385\n",
      "Epoch 020: | Train Loss: 1.77298 | Test Loss: 1.82130\n",
      "Epoch 021: | Train Loss: 1.62177 | Test Loss: 1.68722\n",
      "Epoch 022: | Train Loss: 1.51375 | Test Loss: 1.60116\n",
      "Epoch 023: | Train Loss: 1.94053 | Test Loss: 1.52343\n",
      "Epoch 024: | Train Loss: 1.59174 | Test Loss: 1.40101\n",
      "Epoch 025: | Train Loss: 1.33681 | Test Loss: 1.29707\n",
      "Epoch 026: | Train Loss: 1.25756 | Test Loss: 1.28151\n",
      "Epoch 027: | Train Loss: 1.14267 | Test Loss: 1.18393\n",
      "Epoch 028: | Train Loss: 1.46396 | Test Loss: 1.16139\n",
      "Epoch 029: | Train Loss: 1.60600 | Test Loss: 1.14334\n",
      "Epoch 030: | Train Loss: 1.15875 | Test Loss: 1.15192\n",
      "Epoch 031: | Train Loss: 1.10349 | Test Loss: 1.11126\n",
      "Epoch 032: | Train Loss: 1.07811 | Test Loss: 1.09007\n",
      "Epoch 033: | Train Loss: 1.13307 | Test Loss: 1.00351\n",
      "Epoch 034: | Train Loss: 1.44342 | Test Loss: 0.97639\n",
      "Epoch 035: | Train Loss: 0.95811 | Test Loss: 1.07127\n",
      "Epoch 036: | Train Loss: 1.10012 | Test Loss: 0.92622\n",
      "Epoch 037: | Train Loss: 1.01219 | Test Loss: 0.93425\n",
      "Epoch 038: | Train Loss: 1.08047 | Test Loss: 0.93953\n",
      "Epoch 039: | Train Loss: 1.37542 | Test Loss: 0.88507\n",
      "Epoch 040: | Train Loss: 0.87871 | Test Loss: 0.94238\n",
      "Epoch 041: | Train Loss: 1.07409 | Test Loss: 0.88563\n",
      "Epoch 042: | Train Loss: 1.17062 | Test Loss: 0.89905\n",
      "Epoch 043: | Train Loss: 1.00120 | Test Loss: 0.86088\n",
      "Epoch 044: | Train Loss: 0.83574 | Test Loss: 0.93801\n",
      "Epoch 045: | Train Loss: 0.77175 | Test Loss: 0.84269\n",
      "Epoch 046: | Train Loss: 0.76693 | Test Loss: 0.84188\n",
      "Epoch 047: | Train Loss: 0.73984 | Test Loss: 0.84822\n",
      "Epoch 048: | Train Loss: 0.82820 | Test Loss: 0.84555\n",
      "Epoch 049: | Train Loss: 0.77557 | Test Loss: 0.84275\n",
      "Epoch 050: | Train Loss: 0.74761 | Test Loss: 0.84076\n",
      "Epoch 051: | Train Loss: 0.72079 | Test Loss: 0.88676\n",
      "Epoch 052: | Train Loss: 0.85595 | Test Loss: 0.82850\n",
      "Epoch 053: | Train Loss: 0.70021 | Test Loss: 0.81571\n",
      "Epoch 054: | Train Loss: 0.64949 | Test Loss: 0.82525\n",
      "Epoch 055: | Train Loss: 0.68337 | Test Loss: 0.82267\n",
      "Epoch 056: | Train Loss: 0.62027 | Test Loss: 0.81521\n",
      "Epoch 057: | Train Loss: 0.81052 | Test Loss: 0.86669\n",
      "Epoch 058: | Train Loss: 0.67256 | Test Loss: 0.86299\n",
      "Epoch 059: | Train Loss: 0.59660 | Test Loss: 0.83119\n",
      "Epoch 060: | Train Loss: 0.74339 | Test Loss: 0.83945\n",
      "Epoch 061: | Train Loss: 0.82458 | Test Loss: 0.83142\n",
      "Epoch 062: | Train Loss: 0.66921 | Test Loss: 0.85874\n",
      "Epoch 063: | Train Loss: 0.64270 | Test Loss: 0.81797\n",
      "Epoch 064: | Train Loss: 0.66922 | Test Loss: 0.81325\n",
      "Epoch 065: | Train Loss: 0.60118 | Test Loss: 0.83453\n",
      "Epoch 066: | Train Loss: 0.54954 | Test Loss: 0.78525\n",
      "Epoch 067: | Train Loss: 0.54573 | Test Loss: 0.79168\n",
      "Epoch 068: | Train Loss: 0.52612 | Test Loss: 0.77444\n",
      "Epoch 069: | Train Loss: 0.59904 | Test Loss: 0.76203\n",
      "Epoch 070: | Train Loss: 0.50703 | Test Loss: 0.76531\n",
      "Epoch 071: | Train Loss: 0.51439 | Test Loss: 0.74990\n",
      "Epoch 072: | Train Loss: 0.49826 | Test Loss: 0.75252\n",
      "Epoch 073: | Train Loss: 0.48374 | Test Loss: 0.75756\n",
      "Epoch 074: | Train Loss: 0.47392 | Test Loss: 0.76106\n",
      "Epoch 075: | Train Loss: 0.87891 | Test Loss: 0.81560\n",
      "Epoch 076: | Train Loss: 0.59490 | Test Loss: 0.82958\n",
      "Epoch 077: | Train Loss: 0.61159 | Test Loss: 0.77460\n",
      "Epoch 078: | Train Loss: 0.49454 | Test Loss: 0.79251\n",
      "Epoch 079: | Train Loss: 0.49134 | Test Loss: 0.75706\n",
      "Epoch 080: | Train Loss: 0.49717 | Test Loss: 0.75960\n",
      "Epoch 081: | Train Loss: 0.62410 | Test Loss: 0.78819\n",
      "Epoch 082: | Train Loss: 0.49821 | Test Loss: 0.73948\n",
      "Epoch 083: | Train Loss: 1.54943 | Test Loss: 0.74739\n",
      "Epoch 084: | Train Loss: 0.51957 | Test Loss: 0.84675\n",
      "Epoch 085: | Train Loss: 0.50683 | Test Loss: 0.74608\n",
      "Epoch 086: | Train Loss: 0.46467 | Test Loss: 0.77165\n",
      "Epoch 087: | Train Loss: 0.46766 | Test Loss: 0.75147\n",
      "Epoch 088: | Train Loss: 0.43036 | Test Loss: 0.75243\n",
      "Epoch 089: | Train Loss: 0.48589 | Test Loss: 0.75875\n",
      "Epoch 090: | Train Loss: 0.42376 | Test Loss: 0.72794\n",
      "Epoch 091: | Train Loss: 0.44288 | Test Loss: 0.72652\n",
      "Epoch 092: | Train Loss: 0.44407 | Test Loss: 0.75207\n",
      "Epoch 093: | Train Loss: 0.42663 | Test Loss: 0.74502\n",
      "Epoch 094: | Train Loss: 0.40714 | Test Loss: 0.73270\n",
      "Epoch 095: | Train Loss: 0.38860 | Test Loss: 0.76134\n",
      "Epoch 096: | Train Loss: 0.38648 | Test Loss: 0.72726\n",
      "Epoch 097: | Train Loss: 0.37668 | Test Loss: 0.72750\n",
      "Epoch 098: | Train Loss: 0.37292 | Test Loss: 0.74233\n",
      "Epoch 099: | Train Loss: 0.40954 | Test Loss: 0.73712\n",
      "Epoch 100: | Train Loss: 0.36805 | Test Loss: 0.74375\n",
      "Epoch 101: | Train Loss: 0.49785 | Test Loss: 0.74246\n",
      "Epoch 102: | Train Loss: 0.43771 | Test Loss: 0.76368\n",
      "Epoch 103: | Train Loss: 0.38970 | Test Loss: 0.75521\n",
      "Epoch 104: | Train Loss: 0.36486 | Test Loss: 0.80021\n",
      "Epoch 105: | Train Loss: 0.50154 | Test Loss: 0.75324\n",
      "Epoch 106: | Train Loss: 0.38618 | Test Loss: 0.75838\n",
      "Epoch 107: | Train Loss: 0.38195 | Test Loss: 0.77151\n",
      "Epoch 108: | Train Loss: 0.35401 | Test Loss: 0.74691\n",
      "Epoch 109: | Train Loss: 0.34701 | Test Loss: 0.74989\n",
      "Epoch 110: | Train Loss: 0.38063 | Test Loss: 0.73989\n",
      "Epoch 111: | Train Loss: 0.40442 | Test Loss: 0.74635\n",
      "Epoch 112: | Train Loss: 0.33935 | Test Loss: 0.75233\n",
      "Epoch 113: | Train Loss: 0.40264 | Test Loss: 0.77101\n",
      "Epoch 114: | Train Loss: 0.40146 | Test Loss: 0.75914\n",
      "Epoch 115: | Train Loss: 0.35419 | Test Loss: 0.74501\n",
      "Epoch 116: | Train Loss: 0.35774 | Test Loss: 0.74737\n",
      "Epoch 117: | Train Loss: 0.40140 | Test Loss: 0.74650\n",
      "Epoch 118: | Train Loss: 0.35580 | Test Loss: 0.74170\n",
      "Epoch 119: | Train Loss: 0.33383 | Test Loss: 0.77228\n",
      "Epoch 120: | Train Loss: 0.33264 | Test Loss: 0.73829\n",
      "Epoch 121: | Train Loss: 0.39591 | Test Loss: 0.78677\n",
      "Epoch 122: | Train Loss: 0.33730 | Test Loss: 0.75345\n",
      "Epoch 123: | Train Loss: 0.38546 | Test Loss: 0.75154\n",
      "Epoch 124: | Train Loss: 0.31543 | Test Loss: 0.77893\n",
      "Epoch 125: | Train Loss: 0.31727 | Test Loss: 0.77106\n",
      "Epoch 126: | Train Loss: 0.32157 | Test Loss: 0.76422\n",
      "Epoch 127: | Train Loss: 0.34157 | Test Loss: 0.77781\n",
      "Epoch 128: | Train Loss: 0.33628 | Test Loss: 0.74426\n",
      "Epoch 129: | Train Loss: 0.32658 | Test Loss: 0.77013\n",
      "Epoch 130: | Train Loss: 0.35531 | Test Loss: 0.77812\n",
      "Epoch 131: | Train Loss: 0.30876 | Test Loss: 0.72970\n",
      "Epoch 132: | Train Loss: 0.29650 | Test Loss: 0.74947\n",
      "Epoch 133: | Train Loss: 0.29032 | Test Loss: 0.75967\n",
      "Epoch 134: | Train Loss: 0.28089 | Test Loss: 0.75967\n",
      "Epoch 135: | Train Loss: 0.30827 | Test Loss: 0.76374\n",
      "Epoch 136: | Train Loss: 0.32252 | Test Loss: 0.78978\n",
      "Epoch 137: | Train Loss: 0.30848 | Test Loss: 0.77004\n",
      "Epoch 138: | Train Loss: 0.28176 | Test Loss: 0.76013\n",
      "Epoch 139: | Train Loss: 0.29285 | Test Loss: 0.79043\n",
      "Epoch 140: | Train Loss: 0.28507 | Test Loss: 0.75435\n",
      "Epoch 141: | Train Loss: 0.36886 | Test Loss: 0.75435\n",
      "Epoch 142: | Train Loss: 0.57868 | Test Loss: 0.78064\n",
      "Epoch 143: | Train Loss: 0.35321 | Test Loss: 0.76408\n",
      "Epoch 144: | Train Loss: 0.28288 | Test Loss: 0.78296\n",
      "Epoch 145: | Train Loss: 0.29435 | Test Loss: 0.76867\n",
      "Epoch 146: | Train Loss: 0.34252 | Test Loss: 0.77749\n",
      "Epoch 147: | Train Loss: 0.47233 | Test Loss: 0.74615\n",
      "Epoch 148: | Train Loss: 0.48837 | Test Loss: 0.77504\n",
      "Epoch 149: | Train Loss: 0.39994 | Test Loss: 0.76576\n",
      "Epoch 150: | Train Loss: 0.33133 | Test Loss: 0.74646\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016977310180664062,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75be51e3c494992aba6b1fbcceca5ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 40.62650 | Test Loss: 38.57833\n",
      "Epoch 002: | Train Loss: 37.45530 | Test Loss: 36.88839\n",
      "Epoch 003: | Train Loss: 35.26295 | Test Loss: 33.87010\n",
      "Epoch 004: | Train Loss: 33.31262 | Test Loss: 27.75938\n",
      "Epoch 005: | Train Loss: 22.93095 | Test Loss: 16.58704\n",
      "Epoch 006: | Train Loss: 10.94744 | Test Loss: 4.58400\n",
      "Epoch 007: | Train Loss: 4.02153 | Test Loss: 6.30094\n",
      "Epoch 008: | Train Loss: 4.46519 | Test Loss: 3.63448\n",
      "Epoch 009: | Train Loss: 3.11740 | Test Loss: 3.47908\n",
      "Epoch 010: | Train Loss: 3.68828 | Test Loss: 3.14778\n",
      "Epoch 011: | Train Loss: 3.13844 | Test Loss: 2.92711\n",
      "Epoch 012: | Train Loss: 2.59607 | Test Loss: 2.97096\n",
      "Epoch 013: | Train Loss: 2.46264 | Test Loss: 2.64937\n",
      "Epoch 014: | Train Loss: 2.30594 | Test Loss: 2.50394\n",
      "Epoch 015: | Train Loss: 2.19211 | Test Loss: 2.38336\n",
      "Epoch 016: | Train Loss: 2.06109 | Test Loss: 2.31121\n",
      "Epoch 017: | Train Loss: 1.97221 | Test Loss: 2.18612\n",
      "Epoch 018: | Train Loss: 2.00670 | Test Loss: 2.06293\n",
      "Epoch 019: | Train Loss: 1.84301 | Test Loss: 1.97687\n",
      "Epoch 020: | Train Loss: 1.77985 | Test Loss: 1.84060\n",
      "Epoch 021: | Train Loss: 1.62518 | Test Loss: 1.74558\n",
      "Epoch 022: | Train Loss: 1.55134 | Test Loss: 1.68346\n",
      "Epoch 023: | Train Loss: 1.45819 | Test Loss: 1.57068\n",
      "Epoch 024: | Train Loss: 1.62346 | Test Loss: 1.49862\n",
      "Epoch 025: | Train Loss: 1.45665 | Test Loss: 1.50686\n",
      "Epoch 026: | Train Loss: 1.32598 | Test Loss: 1.31775\n",
      "Epoch 027: | Train Loss: 1.25766 | Test Loss: 1.26060\n",
      "Epoch 028: | Train Loss: 1.19141 | Test Loss: 1.28287\n",
      "Epoch 029: | Train Loss: 1.21631 | Test Loss: 1.18120\n",
      "Epoch 030: | Train Loss: 1.09153 | Test Loss: 1.14929\n",
      "Epoch 031: | Train Loss: 1.04553 | Test Loss: 1.09423\n",
      "Epoch 032: | Train Loss: 1.08534 | Test Loss: 1.08524\n",
      "Epoch 033: | Train Loss: 1.00443 | Test Loss: 1.03804\n",
      "Epoch 034: | Train Loss: 0.95559 | Test Loss: 0.99949\n",
      "Epoch 035: | Train Loss: 0.97354 | Test Loss: 0.98265\n",
      "Epoch 036: | Train Loss: 1.13374 | Test Loss: 0.95507\n",
      "Epoch 037: | Train Loss: 0.98984 | Test Loss: 0.92909\n",
      "Epoch 038: | Train Loss: 1.10624 | Test Loss: 1.02317\n",
      "Epoch 039: | Train Loss: 1.10825 | Test Loss: 0.90879\n",
      "Epoch 040: | Train Loss: 0.85291 | Test Loss: 0.88470\n",
      "Epoch 041: | Train Loss: 1.20573 | Test Loss: 0.91069\n",
      "Epoch 042: | Train Loss: 0.88353 | Test Loss: 0.89778\n",
      "Epoch 043: | Train Loss: 0.95419 | Test Loss: 0.89238\n",
      "Epoch 044: | Train Loss: 0.88577 | Test Loss: 0.86500\n",
      "Epoch 045: | Train Loss: 0.84261 | Test Loss: 0.91331\n",
      "Epoch 046: | Train Loss: 0.83702 | Test Loss: 0.87225\n",
      "Epoch 047: | Train Loss: 0.83909 | Test Loss: 0.86790\n",
      "Epoch 048: | Train Loss: 0.80937 | Test Loss: 0.91246\n",
      "Epoch 049: | Train Loss: 0.75677 | Test Loss: 0.85562\n",
      "Epoch 050: | Train Loss: 0.76503 | Test Loss: 0.85125\n",
      "Epoch 051: | Train Loss: 0.73633 | Test Loss: 0.84980\n",
      "Epoch 052: | Train Loss: 0.76086 | Test Loss: 0.84836\n",
      "Epoch 053: | Train Loss: 0.80093 | Test Loss: 0.88930\n",
      "Epoch 054: | Train Loss: 0.75211 | Test Loss: 0.85915\n",
      "Epoch 055: | Train Loss: 0.74852 | Test Loss: 0.84952\n",
      "Epoch 056: | Train Loss: 0.73286 | Test Loss: 0.88187\n",
      "Epoch 057: | Train Loss: 0.87358 | Test Loss: 0.85198\n",
      "Epoch 058: | Train Loss: 0.70529 | Test Loss: 0.91993\n",
      "Epoch 059: | Train Loss: 0.74294 | Test Loss: 0.84431\n",
      "Epoch 060: | Train Loss: 1.25518 | Test Loss: 0.83164\n",
      "Epoch 061: | Train Loss: 1.02668 | Test Loss: 0.82932\n",
      "Epoch 062: | Train Loss: 0.92257 | Test Loss: 0.88610\n",
      "Epoch 063: | Train Loss: 0.71766 | Test Loss: 0.87755\n",
      "Epoch 064: | Train Loss: 0.71594 | Test Loss: 0.87887\n",
      "Epoch 065: | Train Loss: 0.70219 | Test Loss: 0.82201\n",
      "Epoch 066: | Train Loss: 0.68573 | Test Loss: 0.81438\n",
      "Epoch 067: | Train Loss: 0.70791 | Test Loss: 0.85982\n",
      "Epoch 068: | Train Loss: 0.88498 | Test Loss: 0.85673\n",
      "Epoch 069: | Train Loss: 0.65609 | Test Loss: 0.82881\n",
      "Epoch 070: | Train Loss: 0.64047 | Test Loss: 0.81029\n",
      "Epoch 071: | Train Loss: 0.66244 | Test Loss: 0.81517\n",
      "Epoch 072: | Train Loss: 0.64331 | Test Loss: 0.81400\n",
      "Epoch 073: | Train Loss: 0.65222 | Test Loss: 0.82065\n",
      "Epoch 074: | Train Loss: 0.62797 | Test Loss: 0.80063\n",
      "Epoch 075: | Train Loss: 0.61379 | Test Loss: 0.83339\n",
      "Epoch 076: | Train Loss: 0.60696 | Test Loss: 0.80400\n",
      "Epoch 077: | Train Loss: 0.60192 | Test Loss: 0.80965\n",
      "Epoch 078: | Train Loss: 0.59153 | Test Loss: 0.79893\n",
      "Epoch 079: | Train Loss: 0.60320 | Test Loss: 0.80859\n",
      "Epoch 080: | Train Loss: 0.69196 | Test Loss: 0.82344\n",
      "Epoch 081: | Train Loss: 0.63517 | Test Loss: 0.83627\n",
      "Epoch 082: | Train Loss: 0.63530 | Test Loss: 0.80272\n",
      "Epoch 083: | Train Loss: 1.01310 | Test Loss: 0.78750\n",
      "Epoch 084: | Train Loss: 0.76621 | Test Loss: 0.83533\n",
      "Epoch 085: | Train Loss: 0.64754 | Test Loss: 0.93647\n",
      "Epoch 086: | Train Loss: 0.59659 | Test Loss: 0.80578\n",
      "Epoch 087: | Train Loss: 0.59412 | Test Loss: 0.82707\n",
      "Epoch 088: | Train Loss: 0.59480 | Test Loss: 0.82204\n",
      "Epoch 089: | Train Loss: 0.56717 | Test Loss: 0.81289\n",
      "Epoch 090: | Train Loss: 0.56594 | Test Loss: 0.80207\n",
      "Epoch 091: | Train Loss: 0.55270 | Test Loss: 0.81564\n",
      "Epoch 092: | Train Loss: 0.64552 | Test Loss: 0.80594\n",
      "Epoch 093: | Train Loss: 0.55736 | Test Loss: 0.81114\n",
      "Epoch 094: | Train Loss: 0.64720 | Test Loss: 0.83212\n",
      "Epoch 095: | Train Loss: 0.58663 | Test Loss: 0.82392\n",
      "Epoch 096: | Train Loss: 0.61389 | Test Loss: 0.85881\n",
      "Epoch 097: | Train Loss: 0.53783 | Test Loss: 0.81045\n",
      "Epoch 098: | Train Loss: 0.57693 | Test Loss: 0.85368\n",
      "Epoch 099: | Train Loss: 0.56958 | Test Loss: 0.79801\n",
      "Epoch 100: | Train Loss: 0.76802 | Test Loss: 0.80437\n",
      "Epoch 101: | Train Loss: 0.62992 | Test Loss: 0.92121\n",
      "Epoch 102: | Train Loss: 0.60996 | Test Loss: 0.81555\n",
      "Epoch 103: | Train Loss: 0.56125 | Test Loss: 0.81430\n",
      "Epoch 104: | Train Loss: 0.53831 | Test Loss: 0.82836\n",
      "Epoch 105: | Train Loss: 0.50715 | Test Loss: 0.78933\n",
      "Epoch 106: | Train Loss: 0.85439 | Test Loss: 0.85840\n",
      "Epoch 107: | Train Loss: 0.60399 | Test Loss: 0.83518\n",
      "Epoch 108: | Train Loss: 0.50586 | Test Loss: 0.78149\n",
      "Epoch 109: | Train Loss: 0.55549 | Test Loss: 0.77908\n",
      "Epoch 110: | Train Loss: 0.70025 | Test Loss: 0.77398\n",
      "Epoch 111: | Train Loss: 0.61998 | Test Loss: 0.76657\n",
      "Epoch 112: | Train Loss: 0.50991 | Test Loss: 0.81917\n",
      "Epoch 113: | Train Loss: 0.54436 | Test Loss: 0.84607\n",
      "Epoch 114: | Train Loss: 0.53431 | Test Loss: 0.83671\n",
      "Epoch 115: | Train Loss: 0.62911 | Test Loss: 0.81775\n",
      "Epoch 116: | Train Loss: 0.61179 | Test Loss: 0.81854\n",
      "Epoch 117: | Train Loss: 0.50697 | Test Loss: 0.87571\n",
      "Epoch 118: | Train Loss: 0.54517 | Test Loss: 0.89255\n",
      "Epoch 119: | Train Loss: 0.47212 | Test Loss: 0.90093\n",
      "Epoch 120: | Train Loss: 0.45306 | Test Loss: 0.82448\n",
      "Epoch 121: | Train Loss: 0.47819 | Test Loss: 0.85498\n",
      "Epoch 122: | Train Loss: 0.46313 | Test Loss: 0.86636\n",
      "Epoch 123: | Train Loss: 0.47268 | Test Loss: 0.82462\n",
      "Epoch 124: | Train Loss: 0.46348 | Test Loss: 0.79957\n",
      "Epoch 125: | Train Loss: 0.45919 | Test Loss: 0.82595\n",
      "Epoch 126: | Train Loss: 0.45476 | Test Loss: 0.87014\n",
      "Epoch 127: | Train Loss: 0.40333 | Test Loss: 0.91653\n",
      "Epoch 128: | Train Loss: 0.43550 | Test Loss: 0.87475\n",
      "Epoch 129: | Train Loss: 0.39781 | Test Loss: 0.92162\n",
      "Epoch 130: | Train Loss: 0.37361 | Test Loss: 0.88329\n",
      "Epoch 131: | Train Loss: 0.44104 | Test Loss: 0.92104\n",
      "Epoch 132: | Train Loss: 0.40448 | Test Loss: 0.90335\n",
      "Epoch 133: | Train Loss: 0.38146 | Test Loss: 0.89283\n",
      "Epoch 134: | Train Loss: 0.41927 | Test Loss: 0.88287\n",
      "Epoch 135: | Train Loss: 0.36379 | Test Loss: 0.89221\n",
      "Epoch 136: | Train Loss: 0.35000 | Test Loss: 0.93991\n",
      "Epoch 137: | Train Loss: 0.38156 | Test Loss: 0.91199\n",
      "Epoch 138: | Train Loss: 0.39307 | Test Loss: 0.93123\n",
      "Epoch 139: | Train Loss: 0.34932 | Test Loss: 0.90397\n",
      "Epoch 140: | Train Loss: 0.36480 | Test Loss: 0.95906\n",
      "Epoch 141: | Train Loss: 0.34805 | Test Loss: 0.96145\n",
      "Epoch 142: | Train Loss: 0.32415 | Test Loss: 0.93156\n",
      "Epoch 143: | Train Loss: 0.31846 | Test Loss: 0.93580\n",
      "Epoch 144: | Train Loss: 0.50256 | Test Loss: 0.99769\n",
      "Epoch 145: | Train Loss: 0.50418 | Test Loss: 0.98589\n",
      "Epoch 146: | Train Loss: 0.47475 | Test Loss: 0.95199\n",
      "Epoch 147: | Train Loss: 0.92877 | Test Loss: 1.00004\n",
      "Epoch 148: | Train Loss: 0.34371 | Test Loss: 0.96412\n",
      "Epoch 149: | Train Loss: 0.33162 | Test Loss: 0.96986\n",
      "Epoch 150: | Train Loss: 0.31740 | Test Loss: 0.94836\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018969058990478516,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c84752cda4c47798d29cebe7a1cf91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 39.31251 | Test Loss: 40.16409\n",
      "Epoch 002: | Train Loss: 42.42137 | Test Loss: 39.48617\n",
      "Epoch 003: | Train Loss: 39.88014 | Test Loss: 38.32342\n",
      "Epoch 004: | Train Loss: 39.22267 | Test Loss: 36.07559\n",
      "Epoch 005: | Train Loss: 38.20980 | Test Loss: 31.59742\n",
      "Epoch 006: | Train Loss: 27.89582 | Test Loss: 23.43295\n",
      "Epoch 007: | Train Loss: 20.71763 | Test Loss: 11.59261\n",
      "Epoch 008: | Train Loss: 7.26651 | Test Loss: 4.03998\n",
      "Epoch 009: | Train Loss: 4.59269 | Test Loss: 5.62007\n",
      "Epoch 010: | Train Loss: 4.96603 | Test Loss: 3.93830\n",
      "Epoch 011: | Train Loss: 4.95657 | Test Loss: 3.49127\n",
      "Epoch 012: | Train Loss: 4.40289 | Test Loss: 3.36039\n",
      "Epoch 013: | Train Loss: 4.44324 | Test Loss: 3.34108\n",
      "Epoch 014: | Train Loss: 3.23735 | Test Loss: 3.00963\n",
      "Epoch 015: | Train Loss: 2.50993 | Test Loss: 2.73762\n",
      "Epoch 016: | Train Loss: 2.34832 | Test Loss: 2.56210\n",
      "Epoch 017: | Train Loss: 3.39753 | Test Loss: 2.42161\n",
      "Epoch 018: | Train Loss: 2.22775 | Test Loss: 2.35017\n",
      "Epoch 019: | Train Loss: 2.14213 | Test Loss: 2.21624\n",
      "Epoch 020: | Train Loss: 2.42466 | Test Loss: 2.27913\n",
      "Epoch 021: | Train Loss: 2.21058 | Test Loss: 1.99709\n",
      "Epoch 022: | Train Loss: 1.79857 | Test Loss: 1.93318\n",
      "Epoch 023: | Train Loss: 1.73664 | Test Loss: 1.80755\n",
      "Epoch 024: | Train Loss: 1.66913 | Test Loss: 1.77296\n",
      "Epoch 025: | Train Loss: 1.72470 | Test Loss: 1.65936\n",
      "Epoch 026: | Train Loss: 2.15451 | Test Loss: 1.67808\n",
      "Epoch 027: | Train Loss: 1.57165 | Test Loss: 1.58273\n",
      "Epoch 028: | Train Loss: 1.77492 | Test Loss: 1.49186\n",
      "Epoch 029: | Train Loss: 1.31626 | Test Loss: 1.43628\n",
      "Epoch 030: | Train Loss: 1.26294 | Test Loss: 1.35687\n",
      "Epoch 031: | Train Loss: 1.25871 | Test Loss: 1.31326\n",
      "Epoch 032: | Train Loss: 1.20510 | Test Loss: 1.30785\n",
      "Epoch 033: | Train Loss: 1.17236 | Test Loss: 1.27151\n",
      "Epoch 034: | Train Loss: 1.27545 | Test Loss: 1.23268\n",
      "Epoch 035: | Train Loss: 1.19133 | Test Loss: 1.21400\n",
      "Epoch 036: | Train Loss: 1.11160 | Test Loss: 1.20740\n",
      "Epoch 037: | Train Loss: 1.05981 | Test Loss: 1.14513\n",
      "Epoch 038: | Train Loss: 1.10390 | Test Loss: 1.12381\n",
      "Epoch 039: | Train Loss: 1.03256 | Test Loss: 1.10469\n",
      "Epoch 040: | Train Loss: 1.02378 | Test Loss: 1.11784\n",
      "Epoch 041: | Train Loss: 1.01721 | Test Loss: 1.08678\n",
      "Epoch 042: | Train Loss: 0.96903 | Test Loss: 1.05100\n",
      "Epoch 043: | Train Loss: 0.95348 | Test Loss: 1.03753\n",
      "Epoch 044: | Train Loss: 0.93991 | Test Loss: 1.03399\n",
      "Epoch 045: | Train Loss: 1.00314 | Test Loss: 1.01214\n",
      "Epoch 046: | Train Loss: 1.00895 | Test Loss: 0.99946\n",
      "Epoch 047: | Train Loss: 1.16864 | Test Loss: 0.99727\n",
      "Epoch 048: | Train Loss: 1.03557 | Test Loss: 0.98391\n",
      "Epoch 049: | Train Loss: 1.11406 | Test Loss: 1.09014\n",
      "Epoch 050: | Train Loss: 1.06140 | Test Loss: 0.98528\n",
      "Epoch 051: | Train Loss: 0.93530 | Test Loss: 0.94866\n",
      "Epoch 052: | Train Loss: 0.86180 | Test Loss: 0.95847\n",
      "Epoch 053: | Train Loss: 0.90616 | Test Loss: 0.91677\n",
      "Epoch 054: | Train Loss: 0.82353 | Test Loss: 0.95851\n",
      "Epoch 055: | Train Loss: 0.91372 | Test Loss: 0.91998\n",
      "Epoch 056: | Train Loss: 0.79262 | Test Loss: 0.94725\n",
      "Epoch 057: | Train Loss: 0.78914 | Test Loss: 0.91270\n",
      "Epoch 058: | Train Loss: 0.86959 | Test Loss: 0.90248\n",
      "Epoch 059: | Train Loss: 0.79065 | Test Loss: 0.88943\n",
      "Epoch 060: | Train Loss: 0.85161 | Test Loss: 0.95195\n",
      "Epoch 061: | Train Loss: 0.94525 | Test Loss: 0.89327\n",
      "Epoch 062: | Train Loss: 0.78386 | Test Loss: 0.88296\n",
      "Epoch 063: | Train Loss: 1.37699 | Test Loss: 0.88370\n",
      "Epoch 064: | Train Loss: 1.30761 | Test Loss: 0.87951\n",
      "Epoch 065: | Train Loss: 0.74770 | Test Loss: 1.02057\n",
      "Epoch 066: | Train Loss: 0.79950 | Test Loss: 0.86866\n",
      "Epoch 067: | Train Loss: 0.71741 | Test Loss: 0.85535\n",
      "Epoch 068: | Train Loss: 0.72461 | Test Loss: 0.85058\n",
      "Epoch 069: | Train Loss: 0.69441 | Test Loss: 0.87810\n",
      "Epoch 070: | Train Loss: 0.69089 | Test Loss: 0.86164\n",
      "Epoch 071: | Train Loss: 0.68946 | Test Loss: 0.85993\n",
      "Epoch 072: | Train Loss: 0.74273 | Test Loss: 0.86939\n",
      "Epoch 073: | Train Loss: 0.68543 | Test Loss: 0.88306\n",
      "Epoch 074: | Train Loss: 0.67051 | Test Loss: 0.84284\n",
      "Epoch 075: | Train Loss: 0.90237 | Test Loss: 0.86379\n",
      "Epoch 076: | Train Loss: 0.68813 | Test Loss: 0.90014\n",
      "Epoch 077: | Train Loss: 0.71940 | Test Loss: 0.84283\n",
      "Epoch 078: | Train Loss: 0.65041 | Test Loss: 0.85788\n",
      "Epoch 079: | Train Loss: 0.78565 | Test Loss: 0.84018\n",
      "Epoch 080: | Train Loss: 0.65439 | Test Loss: 0.89041\n",
      "Epoch 081: | Train Loss: 0.62564 | Test Loss: 0.83707\n",
      "Epoch 082: | Train Loss: 0.73836 | Test Loss: 0.90782\n",
      "Epoch 083: | Train Loss: 0.72429 | Test Loss: 0.86123\n",
      "Epoch 084: | Train Loss: 0.65715 | Test Loss: 0.84811\n",
      "Epoch 085: | Train Loss: 0.64011 | Test Loss: 0.89436\n",
      "Epoch 086: | Train Loss: 0.64775 | Test Loss: 0.83552\n",
      "Epoch 087: | Train Loss: 0.64438 | Test Loss: 0.83266\n",
      "Epoch 088: | Train Loss: 0.94360 | Test Loss: 0.84121\n",
      "Epoch 089: | Train Loss: 0.66447 | Test Loss: 0.85625\n",
      "Epoch 090: | Train Loss: 0.73128 | Test Loss: 0.90167\n",
      "Epoch 091: | Train Loss: 0.60241 | Test Loss: 0.83128\n",
      "Epoch 092: | Train Loss: 0.58537 | Test Loss: 0.89440\n",
      "Epoch 093: | Train Loss: 0.62458 | Test Loss: 0.84541\n",
      "Epoch 094: | Train Loss: 0.69165 | Test Loss: 0.83172\n",
      "Epoch 095: | Train Loss: 0.59819 | Test Loss: 0.88241\n",
      "Epoch 096: | Train Loss: 0.57958 | Test Loss: 0.83681\n",
      "Epoch 097: | Train Loss: 0.57596 | Test Loss: 0.82876\n",
      "Epoch 098: | Train Loss: 0.55034 | Test Loss: 0.84319\n",
      "Epoch 099: | Train Loss: 0.61839 | Test Loss: 0.80956\n",
      "Epoch 100: | Train Loss: 0.68860 | Test Loss: 0.79892\n",
      "Epoch 101: | Train Loss: 0.90373 | Test Loss: 0.82225\n",
      "Epoch 102: | Train Loss: 0.62749 | Test Loss: 0.86692\n",
      "Epoch 103: | Train Loss: 0.59412 | Test Loss: 0.98768\n",
      "Epoch 104: | Train Loss: 0.62147 | Test Loss: 0.83082\n",
      "Epoch 105: | Train Loss: 0.58843 | Test Loss: 0.82963\n",
      "Epoch 106: | Train Loss: 0.53713 | Test Loss: 0.82698\n",
      "Epoch 107: | Train Loss: 0.73554 | Test Loss: 0.86579\n",
      "Epoch 108: | Train Loss: 0.71519 | Test Loss: 0.81751\n",
      "Epoch 109: | Train Loss: 0.76575 | Test Loss: 0.81219\n",
      "Epoch 110: | Train Loss: 0.64647 | Test Loss: 0.93056\n",
      "Epoch 111: | Train Loss: 0.53400 | Test Loss: 0.84323\n",
      "Epoch 112: | Train Loss: 0.50958 | Test Loss: 0.88575\n",
      "Epoch 113: | Train Loss: 0.60885 | Test Loss: 0.90502\n",
      "Epoch 114: | Train Loss: 0.58472 | Test Loss: 0.92919\n",
      "Epoch 115: | Train Loss: 0.53644 | Test Loss: 0.94302\n",
      "Epoch 116: | Train Loss: 0.60097 | Test Loss: 0.90923\n",
      "Epoch 117: | Train Loss: 0.60766 | Test Loss: 0.88599\n",
      "Epoch 118: | Train Loss: 0.57188 | Test Loss: 0.95034\n",
      "Epoch 119: | Train Loss: 0.54822 | Test Loss: 0.88871\n",
      "Epoch 120: | Train Loss: 0.48120 | Test Loss: 0.97366\n",
      "Epoch 121: | Train Loss: 0.67228 | Test Loss: 0.92096\n",
      "Epoch 122: | Train Loss: 0.52823 | Test Loss: 0.95344\n",
      "Epoch 123: | Train Loss: 0.48345 | Test Loss: 0.90359\n",
      "Epoch 124: | Train Loss: 0.47489 | Test Loss: 0.93901\n",
      "Epoch 125: | Train Loss: 0.45693 | Test Loss: 0.88812\n",
      "Epoch 126: | Train Loss: 0.47438 | Test Loss: 0.89708\n",
      "Epoch 127: | Train Loss: 0.59255 | Test Loss: 0.94187\n",
      "Epoch 128: | Train Loss: 0.51078 | Test Loss: 0.91891\n",
      "Epoch 129: | Train Loss: 0.44657 | Test Loss: 0.86901\n",
      "Epoch 130: | Train Loss: 0.45921 | Test Loss: 0.85510\n",
      "Epoch 131: | Train Loss: 0.43936 | Test Loss: 0.86211\n",
      "Epoch 132: | Train Loss: 0.51995 | Test Loss: 0.86856\n",
      "Epoch 133: | Train Loss: 0.65219 | Test Loss: 0.88570\n",
      "Epoch 134: | Train Loss: 0.71235 | Test Loss: 0.86448\n",
      "Epoch 135: | Train Loss: 0.47597 | Test Loss: 1.03837\n",
      "Epoch 136: | Train Loss: 0.49619 | Test Loss: 0.88197\n",
      "Epoch 137: | Train Loss: 0.43491 | Test Loss: 0.92637\n",
      "Epoch 138: | Train Loss: 0.45042 | Test Loss: 0.87584\n",
      "Epoch 139: | Train Loss: 0.44676 | Test Loss: 0.87365\n",
      "Epoch 140: | Train Loss: 0.50587 | Test Loss: 0.89392\n",
      "Epoch 141: | Train Loss: 0.56958 | Test Loss: 0.88159\n",
      "Epoch 142: | Train Loss: 0.48698 | Test Loss: 0.98784\n",
      "Epoch 143: | Train Loss: 0.43662 | Test Loss: 0.89269\n",
      "Epoch 144: | Train Loss: 0.42761 | Test Loss: 0.88856\n",
      "Epoch 145: | Train Loss: 0.44002 | Test Loss: 0.87102\n",
      "Epoch 146: | Train Loss: 0.42201 | Test Loss: 0.87796\n",
      "Epoch 147: | Train Loss: 0.55066 | Test Loss: 0.87739\n",
      "Epoch 148: | Train Loss: 0.58719 | Test Loss: 0.95153\n",
      "Epoch 149: | Train Loss: 0.71037 | Test Loss: 0.88466\n",
      "Epoch 150: | Train Loss: 0.49023 | Test Loss: 0.87914\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01774907112121582,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efba2a8a69ce43c281884c9990df135d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 37.52360 | Test Loss: 38.19217\n",
      "Epoch 002: | Train Loss: 38.52815 | Test Loss: 37.63549\n",
      "Epoch 003: | Train Loss: 36.92995 | Test Loss: 36.94481\n",
      "Epoch 004: | Train Loss: 38.46786 | Test Loss: 35.67950\n",
      "Epoch 005: | Train Loss: 34.19648 | Test Loss: 33.28534\n",
      "Epoch 006: | Train Loss: 32.28597 | Test Loss: 28.80646\n",
      "Epoch 007: | Train Loss: 26.39180 | Test Loss: 20.76811\n",
      "Epoch 008: | Train Loss: 15.81494 | Test Loss: 9.77060\n",
      "Epoch 009: | Train Loss: 6.49003 | Test Loss: 4.10434\n",
      "Epoch 010: | Train Loss: 4.76785 | Test Loss: 5.38222\n",
      "Epoch 011: | Train Loss: 4.04678 | Test Loss: 3.67715\n",
      "Epoch 012: | Train Loss: 4.14730 | Test Loss: 3.84005\n",
      "Epoch 013: | Train Loss: 3.31537 | Test Loss: 3.38464\n",
      "Epoch 014: | Train Loss: 3.26914 | Test Loss: 3.39672\n",
      "Epoch 015: | Train Loss: 3.51371 | Test Loss: 3.22583\n",
      "Epoch 016: | Train Loss: 2.74128 | Test Loss: 2.96028\n",
      "Epoch 017: | Train Loss: 2.90278 | Test Loss: 2.70310\n",
      "Epoch 018: | Train Loss: 3.73297 | Test Loss: 2.53447\n",
      "Epoch 019: | Train Loss: 2.22511 | Test Loss: 2.44221\n",
      "Epoch 020: | Train Loss: 2.17503 | Test Loss: 2.23693\n",
      "Epoch 021: | Train Loss: 1.96271 | Test Loss: 2.08059\n",
      "Epoch 022: | Train Loss: 1.86431 | Test Loss: 1.93568\n",
      "Epoch 023: | Train Loss: 1.79300 | Test Loss: 1.81579\n",
      "Epoch 024: | Train Loss: 2.93099 | Test Loss: 1.71077\n",
      "Epoch 025: | Train Loss: 1.61255 | Test Loss: 1.67979\n",
      "Epoch 026: | Train Loss: 1.54758 | Test Loss: 1.54415\n",
      "Epoch 027: | Train Loss: 1.39053 | Test Loss: 1.46059\n",
      "Epoch 028: | Train Loss: 1.54499 | Test Loss: 1.37964\n",
      "Epoch 029: | Train Loss: 1.32994 | Test Loss: 1.36028\n",
      "Epoch 030: | Train Loss: 1.25788 | Test Loss: 1.28951\n",
      "Epoch 031: | Train Loss: 1.44027 | Test Loss: 1.25143\n",
      "Epoch 032: | Train Loss: 1.19174 | Test Loss: 1.24566\n",
      "Epoch 033: | Train Loss: 1.19699 | Test Loss: 1.18819\n",
      "Epoch 034: | Train Loss: 1.23133 | Test Loss: 1.15920\n",
      "Epoch 035: | Train Loss: 1.29786 | Test Loss: 1.13184\n",
      "Epoch 036: | Train Loss: 1.07199 | Test Loss: 1.14244\n",
      "Epoch 037: | Train Loss: 1.07456 | Test Loss: 1.08906\n",
      "Epoch 038: | Train Loss: 1.08431 | Test Loss: 1.05691\n",
      "Epoch 039: | Train Loss: 1.47498 | Test Loss: 1.09981\n",
      "Epoch 040: | Train Loss: 1.32677 | Test Loss: 1.03720\n",
      "Epoch 041: | Train Loss: 0.99774 | Test Loss: 1.04815\n",
      "Epoch 042: | Train Loss: 0.95406 | Test Loss: 1.00851\n",
      "Epoch 043: | Train Loss: 0.99635 | Test Loss: 0.99679\n",
      "Epoch 044: | Train Loss: 1.17739 | Test Loss: 0.98849\n",
      "Epoch 045: | Train Loss: 1.03126 | Test Loss: 1.00403\n",
      "Epoch 046: | Train Loss: 1.33184 | Test Loss: 0.98486\n",
      "Epoch 047: | Train Loss: 0.91684 | Test Loss: 0.98239\n",
      "Epoch 048: | Train Loss: 0.88485 | Test Loss: 0.99977\n",
      "Epoch 049: | Train Loss: 0.87681 | Test Loss: 0.97067\n",
      "Epoch 050: | Train Loss: 0.93134 | Test Loss: 0.96845\n",
      "Epoch 051: | Train Loss: 0.86135 | Test Loss: 0.96757\n",
      "Epoch 052: | Train Loss: 0.83647 | Test Loss: 0.93816\n",
      "Epoch 053: | Train Loss: 0.87245 | Test Loss: 0.92886\n",
      "Epoch 054: | Train Loss: 0.82495 | Test Loss: 0.91789\n",
      "Epoch 055: | Train Loss: 0.85322 | Test Loss: 0.91950\n",
      "Epoch 056: | Train Loss: 0.95769 | Test Loss: 0.92256\n",
      "Epoch 057: | Train Loss: 1.17777 | Test Loss: 0.91394\n",
      "Epoch 058: | Train Loss: 0.87581 | Test Loss: 1.01533\n",
      "Epoch 059: | Train Loss: 0.95334 | Test Loss: 0.90705\n",
      "Epoch 060: | Train Loss: 0.85064 | Test Loss: 0.90674\n",
      "Epoch 061: | Train Loss: 0.86502 | Test Loss: 0.88995\n",
      "Epoch 062: | Train Loss: 0.78537 | Test Loss: 0.93146\n",
      "Epoch 063: | Train Loss: 1.31407 | Test Loss: 0.89550\n",
      "Epoch 064: | Train Loss: 1.13590 | Test Loss: 0.97504\n",
      "Epoch 065: | Train Loss: 0.87278 | Test Loss: 0.93321\n",
      "Epoch 066: | Train Loss: 0.84402 | Test Loss: 0.92161\n",
      "Epoch 067: | Train Loss: 0.75810 | Test Loss: 0.89832\n",
      "Epoch 068: | Train Loss: 0.77018 | Test Loss: 0.92388\n",
      "Epoch 069: | Train Loss: 0.87887 | Test Loss: 0.90259\n",
      "Epoch 070: | Train Loss: 0.80612 | Test Loss: 0.90124\n",
      "Epoch 071: | Train Loss: 1.07635 | Test Loss: 0.88135\n",
      "Epoch 072: | Train Loss: 0.76865 | Test Loss: 0.88698\n",
      "Epoch 073: | Train Loss: 0.74088 | Test Loss: 0.92143\n",
      "Epoch 074: | Train Loss: 0.83541 | Test Loss: 0.88053\n",
      "Epoch 075: | Train Loss: 0.88404 | Test Loss: 0.89742\n",
      "Epoch 076: | Train Loss: 0.81272 | Test Loss: 0.90118\n",
      "Epoch 077: | Train Loss: 0.77423 | Test Loss: 0.96921\n",
      "Epoch 078: | Train Loss: 0.71726 | Test Loss: 0.86936\n",
      "Epoch 079: | Train Loss: 0.75089 | Test Loss: 0.86980\n",
      "Epoch 080: | Train Loss: 0.71794 | Test Loss: 0.85375\n",
      "Epoch 081: | Train Loss: 0.92275 | Test Loss: 0.84680\n",
      "Epoch 082: | Train Loss: 0.79982 | Test Loss: 0.85185\n",
      "Epoch 083: | Train Loss: 0.75561 | Test Loss: 0.95894\n",
      "Epoch 084: | Train Loss: 1.05347 | Test Loss: 0.87124\n",
      "Epoch 085: | Train Loss: 1.06772 | Test Loss: 0.86835\n",
      "Epoch 086: | Train Loss: 2.62547 | Test Loss: 1.20484\n",
      "Epoch 087: | Train Loss: 0.78659 | Test Loss: 0.88308\n",
      "Epoch 088: | Train Loss: 0.68196 | Test Loss: 0.86559\n",
      "Epoch 089: | Train Loss: 0.68431 | Test Loss: 0.89418\n",
      "Epoch 090: | Train Loss: 0.85741 | Test Loss: 0.90393\n",
      "Epoch 091: | Train Loss: 0.66989 | Test Loss: 0.91827\n",
      "Epoch 092: | Train Loss: 0.64786 | Test Loss: 0.86131\n",
      "Epoch 093: | Train Loss: 0.80534 | Test Loss: 0.89925\n",
      "Epoch 094: | Train Loss: 0.91396 | Test Loss: 0.88790\n",
      "Epoch 095: | Train Loss: 0.73053 | Test Loss: 0.87073\n",
      "Epoch 096: | Train Loss: 0.81251 | Test Loss: 0.93742\n",
      "Epoch 097: | Train Loss: 0.64261 | Test Loss: 0.85298\n",
      "Epoch 098: | Train Loss: 0.68103 | Test Loss: 0.90193\n",
      "Epoch 099: | Train Loss: 0.66381 | Test Loss: 0.88311\n",
      "Epoch 100: | Train Loss: 0.65967 | Test Loss: 0.85390\n",
      "Epoch 101: | Train Loss: 0.67788 | Test Loss: 0.88526\n",
      "Epoch 102: | Train Loss: 0.64355 | Test Loss: 0.92390\n",
      "Epoch 103: | Train Loss: 0.62060 | Test Loss: 0.85792\n",
      "Epoch 104: | Train Loss: 0.70223 | Test Loss: 0.88020\n",
      "Epoch 105: | Train Loss: 0.64468 | Test Loss: 0.92930\n",
      "Epoch 106: | Train Loss: 0.64185 | Test Loss: 0.86520\n",
      "Epoch 107: | Train Loss: 0.76955 | Test Loss: 0.88906\n",
      "Epoch 108: | Train Loss: 0.73906 | Test Loss: 0.92794\n",
      "Epoch 109: | Train Loss: 0.64985 | Test Loss: 0.87628\n",
      "Epoch 110: | Train Loss: 0.62607 | Test Loss: 0.93755\n",
      "Epoch 111: | Train Loss: 0.62129 | Test Loss: 0.89167\n",
      "Epoch 112: | Train Loss: 0.63121 | Test Loss: 0.87856\n",
      "Epoch 113: | Train Loss: 1.12895 | Test Loss: 0.96165\n",
      "Epoch 114: | Train Loss: 0.65663 | Test Loss: 0.88721\n",
      "Epoch 115: | Train Loss: 0.66492 | Test Loss: 0.84901\n",
      "Epoch 116: | Train Loss: 0.63903 | Test Loss: 0.89431\n",
      "Epoch 117: | Train Loss: 0.61287 | Test Loss: 0.90965\n",
      "Epoch 118: | Train Loss: 0.79505 | Test Loss: 0.87865\n",
      "Epoch 119: | Train Loss: 1.77605 | Test Loss: 0.87314\n",
      "Epoch 120: | Train Loss: 0.77290 | Test Loss: 0.94377\n",
      "Epoch 121: | Train Loss: 0.90836 | Test Loss: 1.02208\n",
      "Epoch 122: | Train Loss: 0.80765 | Test Loss: 0.88313\n",
      "Epoch 123: | Train Loss: 0.62784 | Test Loss: 0.93221\n",
      "Epoch 124: | Train Loss: 0.58063 | Test Loss: 0.87289\n",
      "Epoch 125: | Train Loss: 0.70747 | Test Loss: 0.87235\n",
      "Epoch 126: | Train Loss: 0.62809 | Test Loss: 0.86098\n",
      "Epoch 127: | Train Loss: 0.76673 | Test Loss: 0.90468\n",
      "Epoch 128: | Train Loss: 0.63093 | Test Loss: 0.85736\n",
      "Epoch 129: | Train Loss: 0.63831 | Test Loss: 0.98930\n",
      "Epoch 130: | Train Loss: 0.60216 | Test Loss: 0.88969\n",
      "Epoch 131: | Train Loss: 0.71832 | Test Loss: 0.88222\n",
      "Epoch 132: | Train Loss: 0.74914 | Test Loss: 1.02304\n",
      "Epoch 133: | Train Loss: 0.60317 | Test Loss: 0.88045\n",
      "Epoch 134: | Train Loss: 0.56344 | Test Loss: 0.85884\n",
      "Epoch 135: | Train Loss: 1.33456 | Test Loss: 0.92993\n",
      "Epoch 136: | Train Loss: 0.60681 | Test Loss: 0.91731\n",
      "Epoch 137: | Train Loss: 0.55386 | Test Loss: 0.85936\n",
      "Epoch 138: | Train Loss: 0.57025 | Test Loss: 0.91789\n",
      "Epoch 139: | Train Loss: 0.57594 | Test Loss: 0.89762\n",
      "Epoch 140: | Train Loss: 0.57771 | Test Loss: 0.86062\n",
      "Epoch 141: | Train Loss: 0.53847 | Test Loss: 0.90446\n",
      "Epoch 142: | Train Loss: 0.58111 | Test Loss: 0.86472\n",
      "Epoch 143: | Train Loss: 0.54060 | Test Loss: 0.85443\n",
      "Epoch 144: | Train Loss: 0.53841 | Test Loss: 0.87677\n",
      "Epoch 145: | Train Loss: 0.55032 | Test Loss: 0.87862\n",
      "Epoch 146: | Train Loss: 0.53080 | Test Loss: 0.86256\n",
      "Epoch 147: | Train Loss: 0.58875 | Test Loss: 0.87329\n",
      "Epoch 148: | Train Loss: 0.53517 | Test Loss: 0.85079\n",
      "Epoch 149: | Train Loss: 0.66679 | Test Loss: 0.88335\n",
      "Epoch 150: | Train Loss: 0.64578 | Test Loss: 0.88025\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01958775520324707,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0231ee24277f4f169165176d2d4da6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 35.08092 | Test Loss: 35.94545\n",
      "Epoch 002: | Train Loss: 35.03453 | Test Loss: 34.78408\n",
      "Epoch 003: | Train Loss: 33.26054 | Test Loss: 32.57786\n",
      "Epoch 004: | Train Loss: 31.48397 | Test Loss: 27.79848\n",
      "Epoch 005: | Train Loss: 25.22644 | Test Loss: 19.07921\n",
      "Epoch 006: | Train Loss: 14.26338 | Test Loss: 7.99561\n",
      "Epoch 007: | Train Loss: 4.95643 | Test Loss: 4.35180\n",
      "Epoch 008: | Train Loss: 4.51223 | Test Loss: 5.25850\n",
      "Epoch 009: | Train Loss: 3.54913 | Test Loss: 3.50733\n",
      "Epoch 010: | Train Loss: 4.16501 | Test Loss: 3.37834\n",
      "Epoch 011: | Train Loss: 4.02114 | Test Loss: 3.17737\n",
      "Epoch 012: | Train Loss: 2.76078 | Test Loss: 3.14169\n",
      "Epoch 013: | Train Loss: 2.58245 | Test Loss: 2.69866\n",
      "Epoch 014: | Train Loss: 3.66050 | Test Loss: 2.52147\n",
      "Epoch 015: | Train Loss: 2.22628 | Test Loss: 2.50611\n",
      "Epoch 016: | Train Loss: 2.16112 | Test Loss: 2.24239\n",
      "Epoch 017: | Train Loss: 3.10193 | Test Loss: 2.09151\n",
      "Epoch 018: | Train Loss: 1.90358 | Test Loss: 2.05474\n",
      "Epoch 019: | Train Loss: 1.89182 | Test Loss: 1.85625\n",
      "Epoch 020: | Train Loss: 1.93857 | Test Loss: 1.76819\n",
      "Epoch 021: | Train Loss: 1.66712 | Test Loss: 1.68958\n",
      "Epoch 022: | Train Loss: 1.63631 | Test Loss: 1.63567\n",
      "Epoch 023: | Train Loss: 1.51389 | Test Loss: 1.54220\n",
      "Epoch 024: | Train Loss: 1.44478 | Test Loss: 1.47933\n",
      "Epoch 025: | Train Loss: 1.40681 | Test Loss: 1.43347\n",
      "Epoch 026: | Train Loss: 1.53869 | Test Loss: 1.38245\n",
      "Epoch 027: | Train Loss: 1.45558 | Test Loss: 1.30568\n",
      "Epoch 028: | Train Loss: 1.27676 | Test Loss: 1.24422\n",
      "Epoch 029: | Train Loss: 1.31842 | Test Loss: 1.20878\n",
      "Epoch 030: | Train Loss: 1.18333 | Test Loss: 1.15721\n",
      "Epoch 031: | Train Loss: 1.20499 | Test Loss: 1.14721\n",
      "Epoch 032: | Train Loss: 1.10236 | Test Loss: 1.09409\n",
      "Epoch 033: | Train Loss: 1.10522 | Test Loss: 1.05812\n",
      "Epoch 034: | Train Loss: 1.04919 | Test Loss: 1.03541\n",
      "Epoch 035: | Train Loss: 1.46036 | Test Loss: 1.00612\n",
      "Epoch 036: | Train Loss: 1.37903 | Test Loss: 1.00020\n",
      "Epoch 037: | Train Loss: 1.02913 | Test Loss: 1.02372\n",
      "Epoch 038: | Train Loss: 1.01847 | Test Loss: 0.94433\n",
      "Epoch 039: | Train Loss: 0.93706 | Test Loss: 0.95578\n",
      "Epoch 040: | Train Loss: 1.07050 | Test Loss: 0.93529\n",
      "Epoch 041: | Train Loss: 0.89911 | Test Loss: 0.94184\n",
      "Epoch 042: | Train Loss: 1.42447 | Test Loss: 0.92869\n",
      "Epoch 043: | Train Loss: 1.08867 | Test Loss: 0.94609\n",
      "Epoch 044: | Train Loss: 1.15543 | Test Loss: 1.08767\n",
      "Epoch 045: | Train Loss: 1.03831 | Test Loss: 0.93597\n",
      "Epoch 046: | Train Loss: 1.15836 | Test Loss: 0.91591\n",
      "Epoch 047: | Train Loss: 0.90883 | Test Loss: 1.00586\n",
      "Epoch 048: | Train Loss: 0.97133 | Test Loss: 0.91886\n",
      "Epoch 049: | Train Loss: 0.87836 | Test Loss: 0.88104\n",
      "Epoch 050: | Train Loss: 0.81723 | Test Loss: 0.90070\n",
      "Epoch 051: | Train Loss: 0.82653 | Test Loss: 0.87937\n",
      "Epoch 052: | Train Loss: 1.09326 | Test Loss: 0.86334\n",
      "Epoch 053: | Train Loss: 0.82932 | Test Loss: 0.87091\n",
      "Epoch 054: | Train Loss: 0.79947 | Test Loss: 0.89456\n",
      "Epoch 055: | Train Loss: 0.79333 | Test Loss: 0.86262\n",
      "Epoch 056: | Train Loss: 0.77777 | Test Loss: 0.84259\n",
      "Epoch 057: | Train Loss: 0.85802 | Test Loss: 0.85026\n",
      "Epoch 058: | Train Loss: 0.97532 | Test Loss: 0.83818\n",
      "Epoch 059: | Train Loss: 0.85605 | Test Loss: 0.86109\n",
      "Epoch 060: | Train Loss: 0.91847 | Test Loss: 0.86227\n",
      "Epoch 061: | Train Loss: 1.02731 | Test Loss: 0.93774\n",
      "Epoch 062: | Train Loss: 1.00930 | Test Loss: 0.84252\n",
      "Epoch 063: | Train Loss: 0.76627 | Test Loss: 0.80166\n",
      "Epoch 064: | Train Loss: 0.74144 | Test Loss: 0.79597\n",
      "Epoch 065: | Train Loss: 0.73557 | Test Loss: 0.82489\n",
      "Epoch 066: | Train Loss: 0.73481 | Test Loss: 0.82346\n",
      "Epoch 067: | Train Loss: 0.73026 | Test Loss: 0.79036\n",
      "Epoch 068: | Train Loss: 0.85556 | Test Loss: 0.85048\n",
      "Epoch 069: | Train Loss: 1.24189 | Test Loss: 0.79561\n",
      "Epoch 070: | Train Loss: 0.77272 | Test Loss: 0.85495\n",
      "Epoch 071: | Train Loss: 0.75148 | Test Loss: 0.80977\n",
      "Epoch 072: | Train Loss: 0.68370 | Test Loss: 0.79918\n",
      "Epoch 073: | Train Loss: 0.68196 | Test Loss: 0.78696\n",
      "Epoch 074: | Train Loss: 0.66657 | Test Loss: 0.80319\n",
      "Epoch 075: | Train Loss: 0.67043 | Test Loss: 0.77949\n",
      "Epoch 076: | Train Loss: 0.65498 | Test Loss: 0.79211\n",
      "Epoch 077: | Train Loss: 0.64547 | Test Loss: 0.77979\n",
      "Epoch 078: | Train Loss: 0.64120 | Test Loss: 0.77615\n",
      "Epoch 079: | Train Loss: 0.73389 | Test Loss: 0.77876\n",
      "Epoch 080: | Train Loss: 0.65942 | Test Loss: 0.77529\n",
      "Epoch 081: | Train Loss: 0.62891 | Test Loss: 0.80859\n",
      "Epoch 082: | Train Loss: 0.62881 | Test Loss: 0.78072\n",
      "Epoch 083: | Train Loss: 0.67665 | Test Loss: 0.77668\n",
      "Epoch 084: | Train Loss: 0.64150 | Test Loss: 0.76817\n",
      "Epoch 085: | Train Loss: 0.72801 | Test Loss: 0.79419\n",
      "Epoch 086: | Train Loss: 0.73930 | Test Loss: 0.78764\n",
      "Epoch 087: | Train Loss: 0.68369 | Test Loss: 0.78909\n",
      "Epoch 088: | Train Loss: 1.03942 | Test Loss: 0.91353\n",
      "Epoch 089: | Train Loss: 0.96312 | Test Loss: 0.83722\n",
      "Epoch 090: | Train Loss: 0.66927 | Test Loss: 0.77438\n",
      "Epoch 091: | Train Loss: 0.67978 | Test Loss: 0.77848\n",
      "Epoch 092: | Train Loss: 0.65978 | Test Loss: 0.76325\n",
      "Epoch 093: | Train Loss: 0.59069 | Test Loss: 0.75068\n",
      "Epoch 094: | Train Loss: 0.62011 | Test Loss: 0.77339\n",
      "Epoch 095: | Train Loss: 0.59646 | Test Loss: 0.76330\n",
      "Epoch 096: | Train Loss: 0.66603 | Test Loss: 0.75789\n",
      "Epoch 097: | Train Loss: 0.57323 | Test Loss: 0.75742\n",
      "Epoch 098: | Train Loss: 0.58586 | Test Loss: 0.74946\n",
      "Epoch 099: | Train Loss: 0.56090 | Test Loss: 0.75202\n",
      "Epoch 100: | Train Loss: 0.57515 | Test Loss: 0.77681\n",
      "Epoch 101: | Train Loss: 0.57253 | Test Loss: 0.76179\n",
      "Epoch 102: | Train Loss: 0.54734 | Test Loss: 0.74746\n",
      "Epoch 103: | Train Loss: 0.87593 | Test Loss: 0.74827\n",
      "Epoch 104: | Train Loss: 0.60447 | Test Loss: 0.76189\n",
      "Epoch 105: | Train Loss: 0.59994 | Test Loss: 0.85706\n",
      "Epoch 106: | Train Loss: 0.57082 | Test Loss: 0.75260\n",
      "Epoch 107: | Train Loss: 0.53709 | Test Loss: 0.75230\n",
      "Epoch 108: | Train Loss: 0.72337 | Test Loss: 0.74986\n",
      "Epoch 109: | Train Loss: 0.56256 | Test Loss: 0.74028\n",
      "Epoch 110: | Train Loss: 0.55123 | Test Loss: 0.76199\n",
      "Epoch 111: | Train Loss: 0.82780 | Test Loss: 0.76368\n",
      "Epoch 112: | Train Loss: 0.62386 | Test Loss: 0.83033\n",
      "Epoch 113: | Train Loss: 0.56388 | Test Loss: 0.76710\n",
      "Epoch 114: | Train Loss: 0.58236 | Test Loss: 0.74602\n",
      "Epoch 115: | Train Loss: 0.54889 | Test Loss: 0.73347\n",
      "Epoch 116: | Train Loss: 0.51797 | Test Loss: 0.78136\n",
      "Epoch 117: | Train Loss: 0.52287 | Test Loss: 0.76030\n",
      "Epoch 118: | Train Loss: 0.60776 | Test Loss: 0.73869\n",
      "Epoch 119: | Train Loss: 0.56055 | Test Loss: 0.79338\n",
      "Epoch 120: | Train Loss: 0.51418 | Test Loss: 0.73408\n",
      "Epoch 121: | Train Loss: 0.51831 | Test Loss: 0.77115\n",
      "Epoch 122: | Train Loss: 0.50021 | Test Loss: 0.72755\n",
      "Epoch 123: | Train Loss: 0.46631 | Test Loss: 0.77856\n",
      "Epoch 124: | Train Loss: 0.46626 | Test Loss: 0.73812\n",
      "Epoch 125: | Train Loss: 0.66837 | Test Loss: 0.72748\n",
      "Epoch 126: | Train Loss: 0.64701 | Test Loss: 0.75318\n",
      "Epoch 127: | Train Loss: 0.53718 | Test Loss: 0.79477\n",
      "Epoch 128: | Train Loss: 0.66764 | Test Loss: 0.76122\n",
      "Epoch 129: | Train Loss: 0.48583 | Test Loss: 0.88475\n",
      "Epoch 130: | Train Loss: 0.47636 | Test Loss: 0.76877\n",
      "Epoch 131: | Train Loss: 0.46931 | Test Loss: 0.76714\n",
      "Epoch 132: | Train Loss: 0.44527 | Test Loss: 0.77546\n",
      "Epoch 133: | Train Loss: 0.60343 | Test Loss: 0.76841\n",
      "Epoch 134: | Train Loss: 0.48292 | Test Loss: 0.78922\n",
      "Epoch 135: | Train Loss: 0.72256 | Test Loss: 0.76036\n",
      "Epoch 136: | Train Loss: 0.48456 | Test Loss: 0.85421\n",
      "Epoch 137: | Train Loss: 0.45657 | Test Loss: 0.74237\n",
      "Epoch 138: | Train Loss: 0.42748 | Test Loss: 0.74723\n",
      "Epoch 139: | Train Loss: 0.50424 | Test Loss: 0.75156\n",
      "Epoch 140: | Train Loss: 0.44279 | Test Loss: 0.76882\n",
      "Epoch 141: | Train Loss: 0.40582 | Test Loss: 0.73915\n",
      "Epoch 142: | Train Loss: 0.40201 | Test Loss: 0.73534\n",
      "Epoch 143: | Train Loss: 0.48286 | Test Loss: 0.72921\n",
      "Epoch 144: | Train Loss: 0.42605 | Test Loss: 0.72909\n",
      "Epoch 145: | Train Loss: 0.40377 | Test Loss: 0.78195\n",
      "Epoch 146: | Train Loss: 0.39119 | Test Loss: 0.72848\n",
      "Epoch 147: | Train Loss: 0.37818 | Test Loss: 0.73713\n",
      "Epoch 148: | Train Loss: 0.38480 | Test Loss: 0.73926\n",
      "Epoch 149: | Train Loss: 0.46025 | Test Loss: 0.72805\n",
      "Epoch 150: | Train Loss: 0.38571 | Test Loss: 0.72111\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020883798599243164,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ad8682342f4aca983fdf37f2b7049d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 34.26068 | Test Loss: 34.83308\n",
      "Epoch 002: | Train Loss: 38.00318 | Test Loss: 33.07056\n",
      "Epoch 003: | Train Loss: 32.74761 | Test Loss: 30.36642\n",
      "Epoch 004: | Train Loss: 28.55920 | Test Loss: 25.70160\n",
      "Epoch 005: | Train Loss: 23.13722 | Test Loss: 18.07479\n",
      "Epoch 006: | Train Loss: 15.09275 | Test Loss: 8.24756\n",
      "Epoch 007: | Train Loss: 5.58941 | Test Loss: 3.56460\n",
      "Epoch 008: | Train Loss: 3.88799 | Test Loss: 4.96713\n",
      "Epoch 009: | Train Loss: 4.79886 | Test Loss: 3.27554\n",
      "Epoch 010: | Train Loss: 3.39339 | Test Loss: 3.36943\n",
      "Epoch 011: | Train Loss: 3.26122 | Test Loss: 3.14159\n",
      "Epoch 012: | Train Loss: 2.70654 | Test Loss: 2.82151\n",
      "Epoch 013: | Train Loss: 2.51912 | Test Loss: 2.77487\n",
      "Epoch 014: | Train Loss: 2.67069 | Test Loss: 2.55918\n",
      "Epoch 015: | Train Loss: 3.30920 | Test Loss: 2.53157\n",
      "Epoch 016: | Train Loss: 2.18011 | Test Loss: 2.43241\n",
      "Epoch 017: | Train Loss: 2.07227 | Test Loss: 2.19441\n",
      "Epoch 018: | Train Loss: 2.18627 | Test Loss: 2.08820\n",
      "Epoch 019: | Train Loss: 1.86859 | Test Loss: 2.06571\n",
      "Epoch 020: | Train Loss: 1.73901 | Test Loss: 1.86175\n",
      "Epoch 021: | Train Loss: 1.97584 | Test Loss: 1.74186\n",
      "Epoch 022: | Train Loss: 1.56797 | Test Loss: 1.65717\n",
      "Epoch 023: | Train Loss: 1.43552 | Test Loss: 1.53440\n",
      "Epoch 024: | Train Loss: 1.37202 | Test Loss: 1.44882\n",
      "Epoch 025: | Train Loss: 1.33734 | Test Loss: 1.39866\n",
      "Epoch 026: | Train Loss: 1.83783 | Test Loss: 1.40094\n",
      "Epoch 027: | Train Loss: 1.21344 | Test Loss: 1.28452\n",
      "Epoch 028: | Train Loss: 1.10933 | Test Loss: 1.17062\n",
      "Epoch 029: | Train Loss: 1.05659 | Test Loss: 1.12282\n",
      "Epoch 030: | Train Loss: 1.06667 | Test Loss: 1.08922\n",
      "Epoch 031: | Train Loss: 1.01702 | Test Loss: 1.08032\n",
      "Epoch 032: | Train Loss: 1.04462 | Test Loss: 1.02101\n",
      "Epoch 033: | Train Loss: 0.98342 | Test Loss: 1.00134\n",
      "Epoch 034: | Train Loss: 1.99116 | Test Loss: 1.06744\n",
      "Epoch 035: | Train Loss: 1.13628 | Test Loss: 0.94987\n",
      "Epoch 036: | Train Loss: 0.92273 | Test Loss: 0.92743\n",
      "Epoch 037: | Train Loss: 0.83867 | Test Loss: 0.96336\n",
      "Epoch 038: | Train Loss: 0.84134 | Test Loss: 0.89862\n",
      "Epoch 039: | Train Loss: 0.85529 | Test Loss: 0.88852\n",
      "Epoch 040: | Train Loss: 1.10808 | Test Loss: 0.88137\n",
      "Epoch 041: | Train Loss: 0.92502 | Test Loss: 0.86588\n",
      "Epoch 042: | Train Loss: 0.83099 | Test Loss: 0.98217\n",
      "Epoch 043: | Train Loss: 0.77856 | Test Loss: 0.84466\n",
      "Epoch 044: | Train Loss: 0.97150 | Test Loss: 0.83720\n",
      "Epoch 045: | Train Loss: 0.77112 | Test Loss: 0.82697\n",
      "Epoch 046: | Train Loss: 0.67849 | Test Loss: 0.83987\n",
      "Epoch 047: | Train Loss: 0.67465 | Test Loss: 0.83769\n",
      "Epoch 048: | Train Loss: 0.71661 | Test Loss: 0.81605\n",
      "Epoch 049: | Train Loss: 0.94101 | Test Loss: 0.81871\n",
      "Epoch 050: | Train Loss: 0.88689 | Test Loss: 0.85276\n",
      "Epoch 051: | Train Loss: 0.69810 | Test Loss: 0.91696\n",
      "Epoch 052: | Train Loss: 0.78129 | Test Loss: 0.83709\n",
      "Epoch 053: | Train Loss: 0.75473 | Test Loss: 0.86941\n",
      "Epoch 054: | Train Loss: 0.68443 | Test Loss: 0.82169\n",
      "Epoch 055: | Train Loss: 0.70747 | Test Loss: 0.87000\n",
      "Epoch 056: | Train Loss: 0.61843 | Test Loss: 0.82221\n",
      "Epoch 057: | Train Loss: 0.63291 | Test Loss: 0.82710\n",
      "Epoch 058: | Train Loss: 0.62156 | Test Loss: 0.81543\n",
      "Epoch 059: | Train Loss: 0.57516 | Test Loss: 0.81323\n",
      "Epoch 060: | Train Loss: 0.56248 | Test Loss: 0.80819\n",
      "Epoch 061: | Train Loss: 0.58246 | Test Loss: 0.78467\n",
      "Epoch 062: | Train Loss: 0.64327 | Test Loss: 0.84147\n",
      "Epoch 063: | Train Loss: 0.59383 | Test Loss: 0.79485\n",
      "Epoch 064: | Train Loss: 0.53015 | Test Loss: 0.77033\n",
      "Epoch 065: | Train Loss: 0.55127 | Test Loss: 0.80993\n",
      "Epoch 066: | Train Loss: 0.51983 | Test Loss: 0.80876\n",
      "Epoch 067: | Train Loss: 0.50505 | Test Loss: 0.79539\n",
      "Epoch 068: | Train Loss: 0.50130 | Test Loss: 0.80910\n",
      "Epoch 069: | Train Loss: 0.58358 | Test Loss: 0.82742\n",
      "Epoch 070: | Train Loss: 0.64579 | Test Loss: 0.85013\n",
      "Epoch 071: | Train Loss: 0.76281 | Test Loss: 0.79145\n",
      "Epoch 072: | Train Loss: 0.71584 | Test Loss: 0.83704\n",
      "Epoch 073: | Train Loss: 0.56106 | Test Loss: 1.04374\n",
      "Epoch 074: | Train Loss: 0.50808 | Test Loss: 0.83893\n",
      "Epoch 075: | Train Loss: 0.70513 | Test Loss: 0.87401\n",
      "Epoch 076: | Train Loss: 0.58681 | Test Loss: 0.85344\n",
      "Epoch 077: | Train Loss: 0.63015 | Test Loss: 0.82736\n",
      "Epoch 078: | Train Loss: 0.51743 | Test Loss: 0.93553\n",
      "Epoch 079: | Train Loss: 0.48728 | Test Loss: 0.81714\n",
      "Epoch 080: | Train Loss: 0.51458 | Test Loss: 0.84317\n",
      "Epoch 081: | Train Loss: 0.46640 | Test Loss: 0.84222\n",
      "Epoch 082: | Train Loss: 0.45991 | Test Loss: 0.81312\n",
      "Epoch 083: | Train Loss: 0.46001 | Test Loss: 0.82413\n",
      "Epoch 084: | Train Loss: 0.52183 | Test Loss: 0.84610\n",
      "Epoch 085: | Train Loss: 0.47205 | Test Loss: 0.83509\n",
      "Epoch 086: | Train Loss: 0.44147 | Test Loss: 0.80779\n",
      "Epoch 087: | Train Loss: 0.68570 | Test Loss: 0.79969\n",
      "Epoch 088: | Train Loss: 0.45990 | Test Loss: 0.81151\n",
      "Epoch 089: | Train Loss: 0.42407 | Test Loss: 0.84785\n",
      "Epoch 090: | Train Loss: 0.42201 | Test Loss: 0.82805\n",
      "Epoch 091: | Train Loss: 0.46431 | Test Loss: 0.85272\n",
      "Epoch 092: | Train Loss: 0.43180 | Test Loss: 0.82815\n",
      "Epoch 093: | Train Loss: 0.40873 | Test Loss: 0.81205\n",
      "Epoch 094: | Train Loss: 0.67399 | Test Loss: 0.88020\n",
      "Epoch 095: | Train Loss: 0.48978 | Test Loss: 0.80636\n",
      "Epoch 096: | Train Loss: 0.46072 | Test Loss: 0.78536\n",
      "Epoch 097: | Train Loss: 0.75495 | Test Loss: 0.81260\n",
      "Epoch 098: | Train Loss: 0.50300 | Test Loss: 0.81153\n",
      "Epoch 099: | Train Loss: 0.48584 | Test Loss: 0.95267\n",
      "Epoch 100: | Train Loss: 0.44430 | Test Loss: 0.81495\n",
      "Epoch 101: | Train Loss: 0.38886 | Test Loss: 0.82950\n",
      "Epoch 102: | Train Loss: 0.38804 | Test Loss: 0.83167\n",
      "Epoch 103: | Train Loss: 0.38215 | Test Loss: 0.81574\n",
      "Epoch 104: | Train Loss: 0.46653 | Test Loss: 0.85346\n",
      "Epoch 105: | Train Loss: 0.43116 | Test Loss: 0.83567\n",
      "Epoch 106: | Train Loss: 0.39759 | Test Loss: 0.81696\n",
      "Epoch 107: | Train Loss: 0.36987 | Test Loss: 0.87558\n",
      "Epoch 108: | Train Loss: 0.46815 | Test Loss: 0.81689\n",
      "Epoch 109: | Train Loss: 0.46058 | Test Loss: 0.82471\n",
      "Epoch 110: | Train Loss: 0.45829 | Test Loss: 0.86532\n",
      "Epoch 111: | Train Loss: 0.52609 | Test Loss: 0.83675\n",
      "Epoch 112: | Train Loss: 0.43507 | Test Loss: 0.86356\n",
      "Epoch 113: | Train Loss: 0.42274 | Test Loss: 0.82644\n",
      "Epoch 114: | Train Loss: 0.41031 | Test Loss: 0.86283\n",
      "Epoch 115: | Train Loss: 0.37738 | Test Loss: 0.81415\n",
      "Epoch 116: | Train Loss: 0.35130 | Test Loss: 0.85067\n",
      "Epoch 117: | Train Loss: 0.36559 | Test Loss: 0.81195\n",
      "Epoch 118: | Train Loss: 0.42243 | Test Loss: 0.84078\n",
      "Epoch 119: | Train Loss: 0.37876 | Test Loss: 0.81506\n",
      "Epoch 120: | Train Loss: 0.37353 | Test Loss: 0.80853\n",
      "Epoch 121: | Train Loss: 0.34406 | Test Loss: 0.85687\n",
      "Epoch 122: | Train Loss: 0.45270 | Test Loss: 0.81779\n",
      "Epoch 123: | Train Loss: 0.33836 | Test Loss: 0.84106\n",
      "Epoch 124: | Train Loss: 0.35235 | Test Loss: 0.81092\n",
      "Epoch 125: | Train Loss: 0.39176 | Test Loss: 0.87061\n",
      "Epoch 126: | Train Loss: 0.35287 | Test Loss: 0.84247\n",
      "Epoch 127: | Train Loss: 0.34967 | Test Loss: 0.83039\n",
      "Epoch 128: | Train Loss: 0.34259 | Test Loss: 0.85785\n",
      "Epoch 129: | Train Loss: 0.36871 | Test Loss: 0.85013\n",
      "Epoch 130: | Train Loss: 0.34321 | Test Loss: 0.85731\n",
      "Epoch 131: | Train Loss: 0.34523 | Test Loss: 0.83450\n",
      "Epoch 132: | Train Loss: 0.32889 | Test Loss: 0.88337\n",
      "Epoch 133: | Train Loss: 0.47479 | Test Loss: 0.82987\n",
      "Epoch 134: | Train Loss: 0.37369 | Test Loss: 0.87743\n",
      "Epoch 135: | Train Loss: 0.36940 | Test Loss: 0.93717\n",
      "Epoch 136: | Train Loss: 0.39995 | Test Loss: 0.85451\n",
      "Epoch 137: | Train Loss: 0.37696 | Test Loss: 0.86538\n",
      "Epoch 138: | Train Loss: 0.40846 | Test Loss: 0.81382\n",
      "Epoch 139: | Train Loss: 0.31258 | Test Loss: 0.82123\n",
      "Epoch 140: | Train Loss: 0.42155 | Test Loss: 0.80975\n",
      "Epoch 141: | Train Loss: 0.40025 | Test Loss: 0.84625\n",
      "Epoch 142: | Train Loss: 0.30946 | Test Loss: 0.87667\n",
      "Epoch 143: | Train Loss: 0.30832 | Test Loss: 0.84805\n",
      "Epoch 144: | Train Loss: 0.29904 | Test Loss: 0.84458\n",
      "Epoch 145: | Train Loss: 0.31390 | Test Loss: 0.87050\n",
      "Epoch 146: | Train Loss: 0.71362 | Test Loss: 0.83206\n",
      "Epoch 147: | Train Loss: 0.42526 | Test Loss: 0.89254\n",
      "Epoch 148: | Train Loss: 0.43106 | Test Loss: 0.95439\n",
      "Epoch 149: | Train Loss: 0.34809 | Test Loss: 0.90459\n",
      "Epoch 150: | Train Loss: 0.42206 | Test Loss: 0.92045\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0183260440826416,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1eae9f333e43d58edf7aa20ad80d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 35.94411 | Test Loss: 36.48756\n",
      "Epoch 002: | Train Loss: 36.40356 | Test Loss: 35.87778\n",
      "Epoch 003: | Train Loss: 36.11294 | Test Loss: 34.73019\n",
      "Epoch 004: | Train Loss: 36.01905 | Test Loss: 32.17502\n",
      "Epoch 005: | Train Loss: 33.43463 | Test Loss: 26.47507\n",
      "Epoch 006: | Train Loss: 25.19250 | Test Loss: 15.47910\n",
      "Epoch 007: | Train Loss: 9.97543 | Test Loss: 4.56168\n",
      "Epoch 008: | Train Loss: 5.69034 | Test Loss: 5.70186\n",
      "Epoch 009: | Train Loss: 3.94711 | Test Loss: 3.63926\n",
      "Epoch 010: | Train Loss: 3.31045 | Test Loss: 3.54124\n",
      "Epoch 011: | Train Loss: 3.07162 | Test Loss: 3.21270\n",
      "Epoch 012: | Train Loss: 3.92835 | Test Loss: 3.26306\n",
      "Epoch 013: | Train Loss: 2.74283 | Test Loss: 2.82953\n",
      "Epoch 014: | Train Loss: 2.65603 | Test Loss: 2.58571\n",
      "Epoch 015: | Train Loss: 2.15880 | Test Loss: 2.46904\n",
      "Epoch 016: | Train Loss: 2.12266 | Test Loss: 2.31520\n",
      "Epoch 017: | Train Loss: 2.08113 | Test Loss: 2.10398\n",
      "Epoch 018: | Train Loss: 1.81955 | Test Loss: 2.06589\n",
      "Epoch 019: | Train Loss: 1.95241 | Test Loss: 1.86915\n",
      "Epoch 020: | Train Loss: 2.15076 | Test Loss: 1.84362\n",
      "Epoch 021: | Train Loss: 1.74745 | Test Loss: 1.75026\n",
      "Epoch 022: | Train Loss: 2.50554 | Test Loss: 1.64078\n",
      "Epoch 023: | Train Loss: 1.66504 | Test Loss: 1.55687\n",
      "Epoch 024: | Train Loss: 1.29777 | Test Loss: 1.42563\n",
      "Epoch 025: | Train Loss: 1.24341 | Test Loss: 1.35204\n",
      "Epoch 026: | Train Loss: 1.47869 | Test Loss: 1.31225\n",
      "Epoch 027: | Train Loss: 1.81961 | Test Loss: 1.25687\n",
      "Epoch 028: | Train Loss: 1.24447 | Test Loss: 1.27043\n",
      "Epoch 029: | Train Loss: 1.12654 | Test Loss: 1.27768\n",
      "Epoch 030: | Train Loss: 1.53476 | Test Loss: 1.16603\n",
      "Epoch 031: | Train Loss: 1.16372 | Test Loss: 1.24711\n",
      "Epoch 032: | Train Loss: 1.06405 | Test Loss: 1.19784\n",
      "Epoch 033: | Train Loss: 1.66369 | Test Loss: 1.11617\n",
      "Epoch 034: | Train Loss: 1.07483 | Test Loss: 1.19765\n",
      "Epoch 035: | Train Loss: 1.01663 | Test Loss: 1.14673\n",
      "Epoch 036: | Train Loss: 1.02254 | Test Loss: 1.11792\n",
      "Epoch 037: | Train Loss: 0.94042 | Test Loss: 1.06306\n",
      "Epoch 038: | Train Loss: 1.04869 | Test Loss: 1.05494\n",
      "Epoch 039: | Train Loss: 0.91702 | Test Loss: 1.04278\n",
      "Epoch 040: | Train Loss: 0.94979 | Test Loss: 1.05442\n",
      "Epoch 041: | Train Loss: 0.91628 | Test Loss: 1.02614\n",
      "Epoch 042: | Train Loss: 1.31724 | Test Loss: 1.07807\n",
      "Epoch 043: | Train Loss: 0.98624 | Test Loss: 1.11252\n",
      "Epoch 044: | Train Loss: 0.87364 | Test Loss: 0.99422\n",
      "Epoch 045: | Train Loss: 0.85640 | Test Loss: 0.98653\n",
      "Epoch 046: | Train Loss: 0.83751 | Test Loss: 1.00431\n",
      "Epoch 047: | Train Loss: 0.88756 | Test Loss: 0.96667\n",
      "Epoch 048: | Train Loss: 0.93492 | Test Loss: 0.95826\n",
      "Epoch 049: | Train Loss: 1.18700 | Test Loss: 0.97494\n",
      "Epoch 050: | Train Loss: 0.85124 | Test Loss: 1.05042\n",
      "Epoch 051: | Train Loss: 0.85477 | Test Loss: 0.95371\n",
      "Epoch 052: | Train Loss: 0.78468 | Test Loss: 0.97156\n",
      "Epoch 053: | Train Loss: 0.94379 | Test Loss: 0.98695\n",
      "Epoch 054: | Train Loss: 0.79852 | Test Loss: 0.96569\n",
      "Epoch 055: | Train Loss: 0.76504 | Test Loss: 0.92846\n",
      "Epoch 056: | Train Loss: 0.76818 | Test Loss: 0.93312\n",
      "Epoch 057: | Train Loss: 1.23405 | Test Loss: 0.91507\n",
      "Epoch 058: | Train Loss: 0.90412 | Test Loss: 0.94771\n",
      "Epoch 059: | Train Loss: 1.09002 | Test Loss: 0.97188\n",
      "Epoch 060: | Train Loss: 0.77855 | Test Loss: 0.94030\n",
      "Epoch 061: | Train Loss: 0.91203 | Test Loss: 0.91169\n",
      "Epoch 062: | Train Loss: 1.62866 | Test Loss: 0.91601\n",
      "Epoch 063: | Train Loss: 0.79726 | Test Loss: 0.99692\n",
      "Epoch 064: | Train Loss: 0.84052 | Test Loss: 0.87733\n",
      "Epoch 065: | Train Loss: 0.79749 | Test Loss: 0.87577\n",
      "Epoch 066: | Train Loss: 0.74636 | Test Loss: 0.95442\n",
      "Epoch 067: | Train Loss: 0.82715 | Test Loss: 0.87767\n",
      "Epoch 068: | Train Loss: 0.78306 | Test Loss: 0.89457\n",
      "Epoch 069: | Train Loss: 0.71817 | Test Loss: 0.86697\n",
      "Epoch 070: | Train Loss: 0.70209 | Test Loss: 0.93613\n",
      "Epoch 071: | Train Loss: 0.70827 | Test Loss: 0.85829\n",
      "Epoch 072: | Train Loss: 0.72260 | Test Loss: 0.84846\n",
      "Epoch 073: | Train Loss: 0.67712 | Test Loss: 0.85335\n",
      "Epoch 074: | Train Loss: 0.74458 | Test Loss: 0.90382\n",
      "Epoch 075: | Train Loss: 0.66817 | Test Loss: 0.88119\n",
      "Epoch 076: | Train Loss: 0.65790 | Test Loss: 0.84604\n",
      "Epoch 077: | Train Loss: 0.75398 | Test Loss: 0.85768\n",
      "Epoch 078: | Train Loss: 0.65036 | Test Loss: 0.84451\n",
      "Epoch 079: | Train Loss: 0.88543 | Test Loss: 0.94635\n",
      "Epoch 080: | Train Loss: 0.97554 | Test Loss: 0.93172\n",
      "Epoch 081: | Train Loss: 0.64566 | Test Loss: 0.84521\n",
      "Epoch 082: | Train Loss: 0.69979 | Test Loss: 0.82693\n",
      "Epoch 083: | Train Loss: 0.67659 | Test Loss: 0.83230\n",
      "Epoch 084: | Train Loss: 0.65292 | Test Loss: 0.86139\n",
      "Epoch 085: | Train Loss: 0.60263 | Test Loss: 0.80919\n",
      "Epoch 086: | Train Loss: 0.71944 | Test Loss: 0.81278\n",
      "Epoch 087: | Train Loss: 0.79334 | Test Loss: 0.81949\n",
      "Epoch 088: | Train Loss: 0.99537 | Test Loss: 0.82347\n",
      "Epoch 089: | Train Loss: 0.74419 | Test Loss: 0.89614\n",
      "Epoch 090: | Train Loss: 0.66195 | Test Loss: 0.92763\n",
      "Epoch 091: | Train Loss: 0.92256 | Test Loss: 0.81655\n",
      "Epoch 092: | Train Loss: 0.79441 | Test Loss: 0.84411\n",
      "Epoch 093: | Train Loss: 0.69168 | Test Loss: 0.88032\n",
      "Epoch 094: | Train Loss: 0.77966 | Test Loss: 0.90222\n",
      "Epoch 095: | Train Loss: 0.64083 | Test Loss: 0.81630\n",
      "Epoch 096: | Train Loss: 0.64355 | Test Loss: 0.82174\n",
      "Epoch 097: | Train Loss: 0.58913 | Test Loss: 0.85162\n",
      "Epoch 098: | Train Loss: 0.76030 | Test Loss: 0.82041\n",
      "Epoch 099: | Train Loss: 0.67085 | Test Loss: 0.82082\n",
      "Epoch 100: | Train Loss: 0.69956 | Test Loss: 0.89568\n",
      "Epoch 101: | Train Loss: 0.60438 | Test Loss: 0.83704\n",
      "Epoch 102: | Train Loss: 0.92251 | Test Loss: 0.81930\n",
      "Epoch 103: | Train Loss: 0.62780 | Test Loss: 0.81748\n",
      "Epoch 104: | Train Loss: 0.55876 | Test Loss: 0.86617\n",
      "Epoch 105: | Train Loss: 0.60859 | Test Loss: 0.83680\n",
      "Epoch 106: | Train Loss: 0.55291 | Test Loss: 0.83095\n",
      "Epoch 107: | Train Loss: 0.81336 | Test Loss: 0.85318\n",
      "Epoch 108: | Train Loss: 0.65775 | Test Loss: 0.84115\n",
      "Epoch 109: | Train Loss: 0.70242 | Test Loss: 0.81862\n",
      "Epoch 110: | Train Loss: 0.57151 | Test Loss: 0.94242\n",
      "Epoch 111: | Train Loss: 0.63018 | Test Loss: 0.82525\n",
      "Epoch 112: | Train Loss: 0.54894 | Test Loss: 0.88274\n",
      "Epoch 113: | Train Loss: 0.53377 | Test Loss: 0.84635\n",
      "Epoch 114: | Train Loss: 0.72558 | Test Loss: 0.88480\n",
      "Epoch 115: | Train Loss: 0.71070 | Test Loss: 0.90635\n",
      "Epoch 116: | Train Loss: 0.54863 | Test Loss: 0.81452\n",
      "Epoch 117: | Train Loss: 0.53890 | Test Loss: 0.80834\n",
      "Epoch 118: | Train Loss: 0.53953 | Test Loss: 0.86168\n",
      "Epoch 119: | Train Loss: 0.55199 | Test Loss: 0.80658\n",
      "Epoch 120: | Train Loss: 0.63485 | Test Loss: 0.80572\n",
      "Epoch 121: | Train Loss: 0.53123 | Test Loss: 0.86089\n",
      "Epoch 122: | Train Loss: 0.50621 | Test Loss: 0.79527\n",
      "Epoch 123: | Train Loss: 0.50483 | Test Loss: 0.81639\n",
      "Epoch 124: | Train Loss: 1.47976 | Test Loss: 0.85125\n",
      "Epoch 125: | Train Loss: 0.91435 | Test Loss: 0.79038\n",
      "Epoch 126: | Train Loss: 0.56806 | Test Loss: 0.78979\n",
      "Epoch 127: | Train Loss: 0.48704 | Test Loss: 0.89106\n",
      "Epoch 128: | Train Loss: 0.54015 | Test Loss: 0.78664\n",
      "Epoch 129: | Train Loss: 0.55413 | Test Loss: 0.79150\n",
      "Epoch 130: | Train Loss: 0.49193 | Test Loss: 0.81169\n",
      "Epoch 131: | Train Loss: 0.50797 | Test Loss: 0.80439\n",
      "Epoch 132: | Train Loss: 0.48090 | Test Loss: 0.79994\n",
      "Epoch 133: | Train Loss: 0.56305 | Test Loss: 0.79871\n",
      "Epoch 134: | Train Loss: 0.49150 | Test Loss: 0.79300\n",
      "Epoch 135: | Train Loss: 0.48038 | Test Loss: 0.80758\n",
      "Epoch 136: | Train Loss: 0.49403 | Test Loss: 0.78976\n",
      "Epoch 137: | Train Loss: 0.82930 | Test Loss: 0.78354\n",
      "Epoch 138: | Train Loss: 0.54808 | Test Loss: 0.77463\n",
      "Epoch 139: | Train Loss: 0.47907 | Test Loss: 0.81334\n",
      "Epoch 140: | Train Loss: 0.47226 | Test Loss: 0.76630\n",
      "Epoch 141: | Train Loss: 0.47876 | Test Loss: 0.77631\n",
      "Epoch 142: | Train Loss: 0.50380 | Test Loss: 0.78795\n",
      "Epoch 143: | Train Loss: 0.57014 | Test Loss: 0.79295\n",
      "Epoch 144: | Train Loss: 0.49994 | Test Loss: 0.84221\n",
      "Epoch 145: | Train Loss: 0.61859 | Test Loss: 0.80065\n",
      "Epoch 146: | Train Loss: 0.48406 | Test Loss: 0.85918\n",
      "Epoch 147: | Train Loss: 0.45530 | Test Loss: 0.78878\n",
      "Epoch 148: | Train Loss: 0.47339 | Test Loss: 0.79816\n",
      "Epoch 149: | Train Loss: 0.47576 | Test Loss: 0.79723\n",
      "Epoch 150: | Train Loss: 0.46409 | Test Loss: 0.80354\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.030736923217773438,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51293774a88e4719ac5de4a2257fe1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 34.57385 | Test Loss: 34.91224\n",
      "Epoch 002: | Train Loss: 36.50652 | Test Loss: 33.93519\n",
      "Epoch 003: | Train Loss: 32.16410 | Test Loss: 32.44968\n",
      "Epoch 004: | Train Loss: 30.43973 | Test Loss: 29.95286\n",
      "Epoch 005: | Train Loss: 29.38495 | Test Loss: 25.63837\n",
      "Epoch 006: | Train Loss: 22.52503 | Test Loss: 18.34015\n",
      "Epoch 007: | Train Loss: 13.90749 | Test Loss: 8.49994\n",
      "Epoch 008: | Train Loss: 5.55976 | Test Loss: 3.75250\n",
      "Epoch 009: | Train Loss: 4.60225 | Test Loss: 4.77792\n",
      "Epoch 010: | Train Loss: 3.71761 | Test Loss: 3.52679\n",
      "Epoch 011: | Train Loss: 3.12165 | Test Loss: 3.23371\n",
      "Epoch 012: | Train Loss: 3.30437 | Test Loss: 3.11320\n",
      "Epoch 013: | Train Loss: 2.72332 | Test Loss: 3.12839\n",
      "Epoch 014: | Train Loss: 2.61479 | Test Loss: 2.92349\n",
      "Epoch 015: | Train Loss: 3.28679 | Test Loss: 2.71467\n",
      "Epoch 016: | Train Loss: 2.42779 | Test Loss: 2.61794\n",
      "Epoch 017: | Train Loss: 2.42931 | Test Loss: 2.43351\n",
      "Epoch 018: | Train Loss: 2.08532 | Test Loss: 2.33810\n",
      "Epoch 019: | Train Loss: 2.05042 | Test Loss: 2.15227\n",
      "Epoch 020: | Train Loss: 2.01289 | Test Loss: 1.98942\n",
      "Epoch 021: | Train Loss: 1.68865 | Test Loss: 1.79911\n",
      "Epoch 022: | Train Loss: 2.54199 | Test Loss: 1.63716\n",
      "Epoch 023: | Train Loss: 1.55332 | Test Loss: 1.51462\n",
      "Epoch 024: | Train Loss: 2.08768 | Test Loss: 1.44771\n",
      "Epoch 025: | Train Loss: 1.81383 | Test Loss: 1.37302\n",
      "Epoch 026: | Train Loss: 1.23406 | Test Loss: 1.38363\n",
      "Epoch 027: | Train Loss: 1.19853 | Test Loss: 1.22365\n",
      "Epoch 028: | Train Loss: 1.34087 | Test Loss: 1.16277\n",
      "Epoch 029: | Train Loss: 1.18918 | Test Loss: 1.14158\n",
      "Epoch 030: | Train Loss: 1.15017 | Test Loss: 1.08592\n",
      "Epoch 031: | Train Loss: 1.16781 | Test Loss: 1.09704\n",
      "Epoch 032: | Train Loss: 1.26426 | Test Loss: 1.03817\n",
      "Epoch 033: | Train Loss: 1.06694 | Test Loss: 1.03029\n",
      "Epoch 034: | Train Loss: 1.03108 | Test Loss: 1.06771\n",
      "Epoch 035: | Train Loss: 1.07160 | Test Loss: 1.00339\n",
      "Epoch 036: | Train Loss: 1.16269 | Test Loss: 1.00624\n",
      "Epoch 037: | Train Loss: 0.96130 | Test Loss: 1.00300\n",
      "Epoch 038: | Train Loss: 0.94036 | Test Loss: 0.99894\n",
      "Epoch 039: | Train Loss: 1.01538 | Test Loss: 1.03019\n",
      "Epoch 040: | Train Loss: 1.52157 | Test Loss: 0.99938\n",
      "Epoch 041: | Train Loss: 0.90202 | Test Loss: 0.93791\n",
      "Epoch 042: | Train Loss: 0.87000 | Test Loss: 0.89957\n",
      "Epoch 043: | Train Loss: 0.86340 | Test Loss: 0.89814\n",
      "Epoch 044: | Train Loss: 0.84211 | Test Loss: 0.88164\n",
      "Epoch 045: | Train Loss: 0.82863 | Test Loss: 0.88109\n",
      "Epoch 046: | Train Loss: 0.89362 | Test Loss: 0.89348\n",
      "Epoch 047: | Train Loss: 0.86908 | Test Loss: 0.89360\n",
      "Epoch 048: | Train Loss: 0.81404 | Test Loss: 0.87399\n",
      "Epoch 049: | Train Loss: 0.79467 | Test Loss: 0.86604\n",
      "Epoch 050: | Train Loss: 0.81560 | Test Loss: 0.87340\n",
      "Epoch 051: | Train Loss: 1.70327 | Test Loss: 0.88165\n",
      "Epoch 052: | Train Loss: 0.85692 | Test Loss: 0.92384\n",
      "Epoch 053: | Train Loss: 0.80529 | Test Loss: 0.85493\n",
      "Epoch 054: | Train Loss: 0.90011 | Test Loss: 0.92217\n",
      "Epoch 055: | Train Loss: 0.87813 | Test Loss: 0.86722\n",
      "Epoch 056: | Train Loss: 0.78897 | Test Loss: 0.84887\n",
      "Epoch 057: | Train Loss: 0.82446 | Test Loss: 0.84807\n",
      "Epoch 058: | Train Loss: 0.76615 | Test Loss: 0.92587\n",
      "Epoch 059: | Train Loss: 0.74218 | Test Loss: 0.85174\n",
      "Epoch 060: | Train Loss: 0.85828 | Test Loss: 0.87490\n",
      "Epoch 061: | Train Loss: 0.76883 | Test Loss: 0.89303\n",
      "Epoch 062: | Train Loss: 0.73036 | Test Loss: 0.84764\n",
      "Epoch 063: | Train Loss: 0.73222 | Test Loss: 0.87081\n",
      "Epoch 064: | Train Loss: 0.72077 | Test Loss: 0.86238\n",
      "Epoch 065: | Train Loss: 0.81100 | Test Loss: 0.84688\n",
      "Epoch 066: | Train Loss: 0.71669 | Test Loss: 0.86347\n",
      "Epoch 067: | Train Loss: 0.81372 | Test Loss: 0.86398\n",
      "Epoch 068: | Train Loss: 0.89284 | Test Loss: 0.85503\n",
      "Epoch 069: | Train Loss: 0.74217 | Test Loss: 0.94650\n",
      "Epoch 070: | Train Loss: 0.71149 | Test Loss: 0.84845\n",
      "Epoch 071: | Train Loss: 0.83532 | Test Loss: 0.89188\n",
      "Epoch 072: | Train Loss: 0.75533 | Test Loss: 0.86586\n",
      "Epoch 073: | Train Loss: 0.67282 | Test Loss: 0.84630\n",
      "Epoch 074: | Train Loss: 0.69180 | Test Loss: 0.86777\n",
      "Epoch 075: | Train Loss: 0.68345 | Test Loss: 0.85724\n",
      "Epoch 076: | Train Loss: 0.66260 | Test Loss: 0.88366\n",
      "Epoch 077: | Train Loss: 0.72205 | Test Loss: 0.87825\n",
      "Epoch 078: | Train Loss: 0.72050 | Test Loss: 0.87784\n",
      "Epoch 079: | Train Loss: 1.07890 | Test Loss: 0.87176\n",
      "Epoch 080: | Train Loss: 0.81132 | Test Loss: 0.86868\n",
      "Epoch 081: | Train Loss: 0.70100 | Test Loss: 0.96426\n",
      "Epoch 082: | Train Loss: 0.64109 | Test Loss: 0.88414\n",
      "Epoch 083: | Train Loss: 0.63175 | Test Loss: 0.89115\n",
      "Epoch 084: | Train Loss: 0.61496 | Test Loss: 0.90767\n",
      "Epoch 085: | Train Loss: 0.63792 | Test Loss: 0.88287\n",
      "Epoch 086: | Train Loss: 0.60533 | Test Loss: 0.93727\n",
      "Epoch 087: | Train Loss: 0.72269 | Test Loss: 0.90546\n",
      "Epoch 088: | Train Loss: 0.61814 | Test Loss: 0.88630\n",
      "Epoch 089: | Train Loss: 0.65390 | Test Loss: 0.86319\n",
      "Epoch 090: | Train Loss: 0.57299 | Test Loss: 0.90735\n",
      "Epoch 091: | Train Loss: 0.57433 | Test Loss: 0.85391\n",
      "Epoch 092: | Train Loss: 0.56417 | Test Loss: 0.87938\n",
      "Epoch 093: | Train Loss: 0.56016 | Test Loss: 0.84589\n",
      "Epoch 094: | Train Loss: 0.55112 | Test Loss: 0.84511\n",
      "Epoch 095: | Train Loss: 0.60237 | Test Loss: 0.83559\n",
      "Epoch 096: | Train Loss: 0.55241 | Test Loss: 0.84313\n",
      "Epoch 097: | Train Loss: 0.60149 | Test Loss: 0.85267\n",
      "Epoch 098: | Train Loss: 0.53491 | Test Loss: 0.82177\n",
      "Epoch 099: | Train Loss: 0.65220 | Test Loss: 0.90644\n",
      "Epoch 100: | Train Loss: 0.52248 | Test Loss: 0.83433\n",
      "Epoch 101: | Train Loss: 0.51712 | Test Loss: 0.84855\n",
      "Epoch 102: | Train Loss: 0.53984 | Test Loss: 0.83352\n",
      "Epoch 103: | Train Loss: 0.71001 | Test Loss: 0.87214\n",
      "Epoch 104: | Train Loss: 0.79670 | Test Loss: 0.90762\n",
      "Epoch 105: | Train Loss: 0.68556 | Test Loss: 0.85033\n",
      "Epoch 106: | Train Loss: 0.50949 | Test Loss: 0.82296\n",
      "Epoch 107: | Train Loss: 0.53553 | Test Loss: 0.80958\n",
      "Epoch 108: | Train Loss: 0.49426 | Test Loss: 0.82522\n",
      "Epoch 109: | Train Loss: 0.49078 | Test Loss: 0.77864\n",
      "Epoch 110: | Train Loss: 0.51793 | Test Loss: 0.78596\n",
      "Epoch 111: | Train Loss: 0.48414 | Test Loss: 0.79338\n",
      "Epoch 112: | Train Loss: 0.53456 | Test Loss: 0.78201\n",
      "Epoch 113: | Train Loss: 0.50774 | Test Loss: 0.81330\n",
      "Epoch 114: | Train Loss: 0.48229 | Test Loss: 0.84609\n",
      "Epoch 115: | Train Loss: 0.52378 | Test Loss: 0.82794\n",
      "Epoch 116: | Train Loss: 0.50105 | Test Loss: 0.81208\n",
      "Epoch 117: | Train Loss: 0.49760 | Test Loss: 0.79150\n",
      "Epoch 118: | Train Loss: 0.45592 | Test Loss: 0.84775\n",
      "Epoch 119: | Train Loss: 0.56394 | Test Loss: 0.79317\n",
      "Epoch 120: | Train Loss: 0.54935 | Test Loss: 0.82276\n",
      "Epoch 121: | Train Loss: 0.43904 | Test Loss: 0.85422\n",
      "Epoch 122: | Train Loss: 0.49552 | Test Loss: 0.82012\n",
      "Epoch 123: | Train Loss: 0.43490 | Test Loss: 0.84132\n",
      "Epoch 124: | Train Loss: 0.42499 | Test Loss: 0.85782\n",
      "Epoch 125: | Train Loss: 0.44491 | Test Loss: 0.83659\n",
      "Epoch 126: | Train Loss: 0.46941 | Test Loss: 0.83051\n",
      "Epoch 127: | Train Loss: 0.45107 | Test Loss: 0.86828\n",
      "Epoch 128: | Train Loss: 0.63503 | Test Loss: 0.79753\n",
      "Epoch 129: | Train Loss: 0.50009 | Test Loss: 0.84077\n",
      "Epoch 130: | Train Loss: 0.44876 | Test Loss: 0.84607\n",
      "Epoch 131: | Train Loss: 0.39180 | Test Loss: 0.82408\n",
      "Epoch 132: | Train Loss: 0.38844 | Test Loss: 0.87099\n",
      "Epoch 133: | Train Loss: 0.38802 | Test Loss: 0.82412\n",
      "Epoch 134: | Train Loss: 0.65238 | Test Loss: 0.81049\n",
      "Epoch 135: | Train Loss: 0.46348 | Test Loss: 0.82384\n",
      "Epoch 136: | Train Loss: 0.47912 | Test Loss: 0.81919\n",
      "Epoch 137: | Train Loss: 0.44111 | Test Loss: 0.80368\n",
      "Epoch 138: | Train Loss: 0.38836 | Test Loss: 0.85222\n",
      "Epoch 139: | Train Loss: 0.37730 | Test Loss: 0.80717\n",
      "Epoch 140: | Train Loss: 0.47822 | Test Loss: 0.87711\n",
      "Epoch 141: | Train Loss: 0.42511 | Test Loss: 0.82095\n",
      "Epoch 142: | Train Loss: 0.41713 | Test Loss: 0.82589\n",
      "Epoch 143: | Train Loss: 0.36535 | Test Loss: 0.86777\n",
      "Epoch 144: | Train Loss: 0.35785 | Test Loss: 0.84084\n",
      "Epoch 145: | Train Loss: 0.35137 | Test Loss: 0.84839\n",
      "Epoch 146: | Train Loss: 0.35937 | Test Loss: 0.85360\n",
      "Epoch 147: | Train Loss: 0.37949 | Test Loss: 0.83942\n",
      "Epoch 148: | Train Loss: 0.39210 | Test Loss: 0.85561\n",
      "Epoch 149: | Train Loss: 0.43031 | Test Loss: 0.84280\n",
      "Epoch 150: | Train Loss: 0.35593 | Test Loss: 0.85707\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015275955200195312,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b3f727da764184bda1b59bba67639f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 36.21864 | Test Loss: 36.90518\n",
      "Epoch 002: | Train Loss: 36.40333 | Test Loss: 36.64980\n",
      "Epoch 003: | Train Loss: 38.85960 | Test Loss: 36.38542\n",
      "Epoch 004: | Train Loss: 36.82250 | Test Loss: 36.05336\n",
      "Epoch 005: | Train Loss: 36.96111 | Test Loss: 35.52656\n",
      "Epoch 006: | Train Loss: 34.44602 | Test Loss: 34.59502\n",
      "Epoch 007: | Train Loss: 35.18945 | Test Loss: 32.08436\n",
      "Epoch 008: | Train Loss: 31.87916 | Test Loss: 26.54627\n",
      "Epoch 009: | Train Loss: 22.62040 | Test Loss: 17.24821\n",
      "Epoch 010: | Train Loss: 13.52279 | Test Loss: 6.32471\n",
      "Epoch 011: | Train Loss: 5.36112 | Test Loss: 5.00419\n",
      "Epoch 012: | Train Loss: 4.82598 | Test Loss: 4.66577\n",
      "Epoch 013: | Train Loss: 3.18108 | Test Loss: 3.44672\n",
      "Epoch 014: | Train Loss: 4.30113 | Test Loss: 3.26879\n",
      "Epoch 015: | Train Loss: 3.93489 | Test Loss: 3.35245\n",
      "Epoch 016: | Train Loss: 2.85137 | Test Loss: 3.05392\n",
      "Epoch 017: | Train Loss: 2.79326 | Test Loss: 2.74735\n",
      "Epoch 018: | Train Loss: 2.39107 | Test Loss: 2.59489\n",
      "Epoch 019: | Train Loss: 2.32057 | Test Loss: 2.45853\n",
      "Epoch 020: | Train Loss: 4.86677 | Test Loss: 2.29897\n",
      "Epoch 021: | Train Loss: 1.99120 | Test Loss: 2.22829\n",
      "Epoch 022: | Train Loss: 1.90984 | Test Loss: 1.99374\n",
      "Epoch 023: | Train Loss: 1.73292 | Test Loss: 1.81829\n",
      "Epoch 024: | Train Loss: 2.67021 | Test Loss: 1.65737\n",
      "Epoch 025: | Train Loss: 1.58834 | Test Loss: 1.62887\n",
      "Epoch 026: | Train Loss: 1.37974 | Test Loss: 1.39145\n",
      "Epoch 027: | Train Loss: 1.98585 | Test Loss: 1.34726\n",
      "Epoch 028: | Train Loss: 1.50352 | Test Loss: 1.25519\n",
      "Epoch 029: | Train Loss: 1.18268 | Test Loss: 1.29427\n",
      "Epoch 030: | Train Loss: 1.14251 | Test Loss: 1.11997\n",
      "Epoch 031: | Train Loss: 1.04090 | Test Loss: 1.08609\n",
      "Epoch 032: | Train Loss: 1.09462 | Test Loss: 1.04092\n",
      "Epoch 033: | Train Loss: 1.04895 | Test Loss: 1.01505\n",
      "Epoch 034: | Train Loss: 1.41210 | Test Loss: 1.01720\n",
      "Epoch 035: | Train Loss: 1.00693 | Test Loss: 0.98657\n",
      "Epoch 036: | Train Loss: 0.94389 | Test Loss: 0.98217\n",
      "Epoch 037: | Train Loss: 0.93427 | Test Loss: 0.94203\n",
      "Epoch 038: | Train Loss: 0.90834 | Test Loss: 0.94770\n",
      "Epoch 039: | Train Loss: 0.97595 | Test Loss: 0.93579\n",
      "Epoch 040: | Train Loss: 0.87952 | Test Loss: 0.92141\n",
      "Epoch 041: | Train Loss: 0.86079 | Test Loss: 0.90325\n",
      "Epoch 042: | Train Loss: 0.94931 | Test Loss: 0.90227\n",
      "Epoch 043: | Train Loss: 0.92436 | Test Loss: 0.90214\n",
      "Epoch 044: | Train Loss: 0.83943 | Test Loss: 0.87859\n",
      "Epoch 045: | Train Loss: 0.80842 | Test Loss: 0.87271\n",
      "Epoch 046: | Train Loss: 0.91923 | Test Loss: 0.85750\n",
      "Epoch 047: | Train Loss: 1.22098 | Test Loss: 0.84811\n",
      "Epoch 048: | Train Loss: 0.87770 | Test Loss: 0.88664\n",
      "Epoch 049: | Train Loss: 0.81597 | Test Loss: 0.84932\n",
      "Epoch 050: | Train Loss: 0.89945 | Test Loss: 0.83654\n",
      "Epoch 051: | Train Loss: 0.87469 | Test Loss: 0.81486\n",
      "Epoch 052: | Train Loss: 0.91876 | Test Loss: 0.82278\n",
      "Epoch 053: | Train Loss: 0.77569 | Test Loss: 0.84229\n",
      "Epoch 054: | Train Loss: 0.78642 | Test Loss: 0.83133\n",
      "Epoch 055: | Train Loss: 0.97759 | Test Loss: 0.82153\n",
      "Epoch 056: | Train Loss: 1.04130 | Test Loss: 0.79969\n",
      "Epoch 057: | Train Loss: 0.92070 | Test Loss: 0.91392\n",
      "Epoch 058: | Train Loss: 0.79173 | Test Loss: 0.78428\n",
      "Epoch 059: | Train Loss: 1.04133 | Test Loss: 0.78846\n",
      "Epoch 060: | Train Loss: 0.71363 | Test Loss: 0.79043\n",
      "Epoch 061: | Train Loss: 0.73009 | Test Loss: 0.79592\n",
      "Epoch 062: | Train Loss: 0.71533 | Test Loss: 0.78714\n",
      "Epoch 063: | Train Loss: 0.76888 | Test Loss: 0.78562\n",
      "Epoch 064: | Train Loss: 0.69011 | Test Loss: 0.77640\n",
      "Epoch 065: | Train Loss: 0.67489 | Test Loss: 0.82091\n",
      "Epoch 066: | Train Loss: 0.67025 | Test Loss: 0.77930\n",
      "Epoch 067: | Train Loss: 0.71686 | Test Loss: 0.77574\n",
      "Epoch 068: | Train Loss: 0.67282 | Test Loss: 0.78252\n",
      "Epoch 069: | Train Loss: 0.74248 | Test Loss: 0.78013\n",
      "Epoch 070: | Train Loss: 0.65602 | Test Loss: 0.77246\n",
      "Epoch 071: | Train Loss: 0.72942 | Test Loss: 0.77617\n",
      "Epoch 072: | Train Loss: 0.68996 | Test Loss: 0.77372\n",
      "Epoch 073: | Train Loss: 0.64575 | Test Loss: 0.84851\n",
      "Epoch 074: | Train Loss: 0.65254 | Test Loss: 0.77865\n",
      "Epoch 075: | Train Loss: 0.77832 | Test Loss: 0.78934\n",
      "Epoch 076: | Train Loss: 0.72282 | Test Loss: 0.80792\n",
      "Epoch 077: | Train Loss: 0.67129 | Test Loss: 0.76922\n",
      "Epoch 078: | Train Loss: 0.64090 | Test Loss: 0.80919\n",
      "Epoch 079: | Train Loss: 0.78208 | Test Loss: 0.75689\n",
      "Epoch 080: | Train Loss: 0.96433 | Test Loss: 0.77092\n",
      "Epoch 081: | Train Loss: 0.92202 | Test Loss: 0.80519\n",
      "Epoch 082: | Train Loss: 0.75826 | Test Loss: 0.76955\n",
      "Epoch 083: | Train Loss: 0.60680 | Test Loss: 0.80148\n",
      "Epoch 084: | Train Loss: 0.60332 | Test Loss: 0.76478\n",
      "Epoch 085: | Train Loss: 0.89796 | Test Loss: 0.81053\n",
      "Epoch 086: | Train Loss: 0.67312 | Test Loss: 0.79858\n",
      "Epoch 087: | Train Loss: 0.87207 | Test Loss: 0.76376\n",
      "Epoch 088: | Train Loss: 0.67871 | Test Loss: 0.85029\n",
      "Epoch 089: | Train Loss: 0.58057 | Test Loss: 0.77501\n",
      "Epoch 090: | Train Loss: 0.61358 | Test Loss: 0.81859\n",
      "Epoch 091: | Train Loss: 0.63244 | Test Loss: 0.77579\n",
      "Epoch 092: | Train Loss: 0.61825 | Test Loss: 0.74698\n",
      "Epoch 093: | Train Loss: 0.57944 | Test Loss: 0.75641\n",
      "Epoch 094: | Train Loss: 0.60372 | Test Loss: 0.77409\n",
      "Epoch 095: | Train Loss: 0.55912 | Test Loss: 0.75225\n",
      "Epoch 096: | Train Loss: 0.60008 | Test Loss: 0.76647\n",
      "Epoch 097: | Train Loss: 0.70524 | Test Loss: 0.75473\n",
      "Epoch 098: | Train Loss: 0.57923 | Test Loss: 0.77922\n",
      "Epoch 099: | Train Loss: 0.56247 | Test Loss: 0.78959\n",
      "Epoch 100: | Train Loss: 0.55727 | Test Loss: 0.76262\n",
      "Epoch 101: | Train Loss: 0.68773 | Test Loss: 0.76988\n",
      "Epoch 102: | Train Loss: 0.55770 | Test Loss: 0.77746\n",
      "Epoch 103: | Train Loss: 0.55906 | Test Loss: 0.82180\n",
      "Epoch 104: | Train Loss: 0.51248 | Test Loss: 0.77509\n",
      "Epoch 105: | Train Loss: 0.56088 | Test Loss: 0.82742\n",
      "Epoch 106: | Train Loss: 0.53237 | Test Loss: 0.79357\n",
      "Epoch 107: | Train Loss: 0.50771 | Test Loss: 0.77102\n",
      "Epoch 108: | Train Loss: 0.50661 | Test Loss: 0.78542\n",
      "Epoch 109: | Train Loss: 0.50135 | Test Loss: 0.76734\n",
      "Epoch 110: | Train Loss: 0.72982 | Test Loss: 0.81332\n",
      "Epoch 111: | Train Loss: 0.61575 | Test Loss: 0.76200\n",
      "Epoch 112: | Train Loss: 0.55304 | Test Loss: 0.74625\n",
      "Epoch 113: | Train Loss: 0.49810 | Test Loss: 0.81265\n",
      "Epoch 114: | Train Loss: 0.71268 | Test Loss: 0.75129\n",
      "Epoch 115: | Train Loss: 0.64822 | Test Loss: 0.75335\n",
      "Epoch 116: | Train Loss: 0.72376 | Test Loss: 0.77160\n",
      "Epoch 117: | Train Loss: 0.74990 | Test Loss: 0.81507\n",
      "Epoch 118: | Train Loss: 0.58543 | Test Loss: 0.79251\n",
      "Epoch 119: | Train Loss: 0.55384 | Test Loss: 0.77200\n",
      "Epoch 120: | Train Loss: 0.57654 | Test Loss: 0.76048\n",
      "Epoch 121: | Train Loss: 0.49556 | Test Loss: 0.76976\n",
      "Epoch 122: | Train Loss: 0.50106 | Test Loss: 0.73533\n",
      "Epoch 123: | Train Loss: 0.49696 | Test Loss: 0.74753\n",
      "Epoch 124: | Train Loss: 0.49094 | Test Loss: 0.74601\n",
      "Epoch 125: | Train Loss: 0.47110 | Test Loss: 0.76164\n",
      "Epoch 126: | Train Loss: 0.46777 | Test Loss: 0.74335\n",
      "Epoch 127: | Train Loss: 0.47644 | Test Loss: 0.75417\n",
      "Epoch 128: | Train Loss: 0.46126 | Test Loss: 0.74632\n",
      "Epoch 129: | Train Loss: 0.46182 | Test Loss: 0.75779\n",
      "Epoch 130: | Train Loss: 0.49320 | Test Loss: 0.76402\n",
      "Epoch 131: | Train Loss: 0.82155 | Test Loss: 0.74132\n",
      "Epoch 132: | Train Loss: 0.59836 | Test Loss: 0.74940\n",
      "Epoch 133: | Train Loss: 0.50672 | Test Loss: 0.84376\n",
      "Epoch 134: | Train Loss: 0.47930 | Test Loss: 0.74806\n",
      "Epoch 135: | Train Loss: 0.70084 | Test Loss: 0.84505\n",
      "Epoch 136: | Train Loss: 0.50484 | Test Loss: 0.77847\n",
      "Epoch 137: | Train Loss: 0.49202 | Test Loss: 0.76679\n",
      "Epoch 138: | Train Loss: 0.45219 | Test Loss: 0.77134\n",
      "Epoch 139: | Train Loss: 0.47970 | Test Loss: 0.83003\n",
      "Epoch 140: | Train Loss: 0.43523 | Test Loss: 0.77786\n",
      "Epoch 141: | Train Loss: 0.46821 | Test Loss: 0.78826\n",
      "Epoch 142: | Train Loss: 0.43208 | Test Loss: 0.79382\n",
      "Epoch 143: | Train Loss: 0.44760 | Test Loss: 0.80222\n",
      "Epoch 144: | Train Loss: 0.42179 | Test Loss: 0.80041\n",
      "Epoch 145: | Train Loss: 0.41728 | Test Loss: 0.80730\n",
      "Epoch 146: | Train Loss: 0.45254 | Test Loss: 0.79629\n",
      "Epoch 147: | Train Loss: 0.57384 | Test Loss: 0.79873\n",
      "Epoch 148: | Train Loss: 0.46412 | Test Loss: 0.82555\n",
      "Epoch 149: | Train Loss: 0.43186 | Test Loss: 0.82762\n",
      "Epoch 150: | Train Loss: 0.42769 | Test Loss: 0.82006\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015850067138671875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd162bdd35340dc98c28d69d7bc3be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 38.12828 | Test Loss: 37.68760\n",
      "Epoch 002: | Train Loss: 36.55499 | Test Loss: 37.03364\n",
      "Epoch 003: | Train Loss: 39.92767 | Test Loss: 36.20680\n",
      "Epoch 004: | Train Loss: 34.56140 | Test Loss: 35.14143\n",
      "Epoch 005: | Train Loss: 36.86598 | Test Loss: 33.40251\n",
      "Epoch 006: | Train Loss: 31.02192 | Test Loss: 29.58305\n",
      "Epoch 007: | Train Loss: 29.03265 | Test Loss: 22.19003\n",
      "Epoch 008: | Train Loss: 17.49634 | Test Loss: 11.21174\n",
      "Epoch 009: | Train Loss: 7.75315 | Test Loss: 4.07009\n",
      "Epoch 010: | Train Loss: 3.72475 | Test Loss: 4.76821\n",
      "Epoch 011: | Train Loss: 3.69364 | Test Loss: 3.81724\n",
      "Epoch 012: | Train Loss: 3.63629 | Test Loss: 3.53959\n",
      "Epoch 013: | Train Loss: 3.04981 | Test Loss: 3.39612\n",
      "Epoch 014: | Train Loss: 4.24693 | Test Loss: 3.21525\n",
      "Epoch 015: | Train Loss: 2.94697 | Test Loss: 3.10998\n",
      "Epoch 016: | Train Loss: 2.67881 | Test Loss: 2.92569\n",
      "Epoch 017: | Train Loss: 2.66422 | Test Loss: 2.88009\n",
      "Epoch 018: | Train Loss: 2.42592 | Test Loss: 2.71109\n",
      "Epoch 019: | Train Loss: 2.45025 | Test Loss: 2.60789\n",
      "Epoch 020: | Train Loss: 2.42864 | Test Loss: 2.51024\n",
      "Epoch 021: | Train Loss: 2.16726 | Test Loss: 2.40914\n",
      "Epoch 022: | Train Loss: 2.26077 | Test Loss: 2.31922\n",
      "Epoch 023: | Train Loss: 2.07454 | Test Loss: 2.31466\n",
      "Epoch 024: | Train Loss: 4.68025 | Test Loss: 2.13594\n",
      "Epoch 025: | Train Loss: 1.87020 | Test Loss: 2.01041\n",
      "Epoch 026: | Train Loss: 1.97100 | Test Loss: 1.91178\n",
      "Epoch 027: | Train Loss: 1.63028 | Test Loss: 1.83495\n",
      "Epoch 028: | Train Loss: 1.58110 | Test Loss: 1.69385\n",
      "Epoch 029: | Train Loss: 1.52588 | Test Loss: 1.61856\n",
      "Epoch 030: | Train Loss: 1.53373 | Test Loss: 1.64018\n",
      "Epoch 031: | Train Loss: 1.41909 | Test Loss: 1.53994\n",
      "Epoch 032: | Train Loss: 1.39065 | Test Loss: 1.44816\n",
      "Epoch 033: | Train Loss: 1.27880 | Test Loss: 1.41637\n",
      "Epoch 034: | Train Loss: 1.22679 | Test Loss: 1.35777\n",
      "Epoch 035: | Train Loss: 1.21909 | Test Loss: 1.32390\n",
      "Epoch 036: | Train Loss: 1.20108 | Test Loss: 1.27257\n",
      "Epoch 037: | Train Loss: 1.16602 | Test Loss: 1.24018\n",
      "Epoch 038: | Train Loss: 1.09390 | Test Loss: 1.24216\n",
      "Epoch 039: | Train Loss: 1.05684 | Test Loss: 1.17772\n",
      "Epoch 040: | Train Loss: 1.05288 | Test Loss: 1.14776\n",
      "Epoch 041: | Train Loss: 1.04638 | Test Loss: 1.13486\n",
      "Epoch 042: | Train Loss: 1.07959 | Test Loss: 1.08851\n",
      "Epoch 043: | Train Loss: 1.00309 | Test Loss: 1.06763\n",
      "Epoch 044: | Train Loss: 0.99953 | Test Loss: 1.06917\n",
      "Epoch 045: | Train Loss: 0.95669 | Test Loss: 1.03189\n",
      "Epoch 046: | Train Loss: 0.95724 | Test Loss: 1.02156\n",
      "Epoch 047: | Train Loss: 1.15786 | Test Loss: 1.02166\n",
      "Epoch 048: | Train Loss: 1.07449 | Test Loss: 1.03509\n",
      "Epoch 049: | Train Loss: 0.89629 | Test Loss: 1.07529\n",
      "Epoch 050: | Train Loss: 0.91933 | Test Loss: 1.00864\n",
      "Epoch 051: | Train Loss: 0.87974 | Test Loss: 0.99354\n",
      "Epoch 052: | Train Loss: 1.92925 | Test Loss: 1.01820\n",
      "Epoch 053: | Train Loss: 0.94308 | Test Loss: 1.02156\n",
      "Epoch 054: | Train Loss: 0.82868 | Test Loss: 0.94699\n",
      "Epoch 055: | Train Loss: 0.89739 | Test Loss: 0.93691\n",
      "Epoch 056: | Train Loss: 0.85065 | Test Loss: 0.96144\n",
      "Epoch 057: | Train Loss: 0.82460 | Test Loss: 0.90668\n",
      "Epoch 058: | Train Loss: 1.02011 | Test Loss: 0.90230\n",
      "Epoch 059: | Train Loss: 0.80975 | Test Loss: 0.91214\n",
      "Epoch 060: | Train Loss: 0.87537 | Test Loss: 0.89092\n",
      "Epoch 061: | Train Loss: 0.75752 | Test Loss: 0.88072\n",
      "Epoch 062: | Train Loss: 0.77850 | Test Loss: 0.88401\n",
      "Epoch 063: | Train Loss: 0.85151 | Test Loss: 0.92742\n",
      "Epoch 064: | Train Loss: 0.75045 | Test Loss: 0.88121\n",
      "Epoch 065: | Train Loss: 0.73799 | Test Loss: 0.86769\n",
      "Epoch 066: | Train Loss: 0.96197 | Test Loss: 0.86414\n",
      "Epoch 067: | Train Loss: 0.80201 | Test Loss: 0.86431\n",
      "Epoch 068: | Train Loss: 0.72234 | Test Loss: 0.91502\n",
      "Epoch 069: | Train Loss: 0.77437 | Test Loss: 0.85105\n",
      "Epoch 070: | Train Loss: 0.77731 | Test Loss: 0.85272\n",
      "Epoch 071: | Train Loss: 0.76654 | Test Loss: 0.93776\n",
      "Epoch 072: | Train Loss: 0.77043 | Test Loss: 0.87230\n",
      "Epoch 073: | Train Loss: 0.85292 | Test Loss: 0.84631\n",
      "Epoch 074: | Train Loss: 0.84187 | Test Loss: 0.84554\n",
      "Epoch 075: | Train Loss: 1.26503 | Test Loss: 0.86428\n",
      "Epoch 076: | Train Loss: 0.72404 | Test Loss: 0.93433\n",
      "Epoch 077: | Train Loss: 0.82876 | Test Loss: 0.84256\n",
      "Epoch 078: | Train Loss: 0.76310 | Test Loss: 0.91563\n",
      "Epoch 079: | Train Loss: 0.69310 | Test Loss: 0.86774\n",
      "Epoch 080: | Train Loss: 0.79248 | Test Loss: 0.83004\n",
      "Epoch 081: | Train Loss: 1.34809 | Test Loss: 0.84115\n",
      "Epoch 082: | Train Loss: 1.25449 | Test Loss: 0.84356\n",
      "Epoch 083: | Train Loss: 0.75338 | Test Loss: 0.97830\n",
      "Epoch 084: | Train Loss: 0.66550 | Test Loss: 0.80711\n",
      "Epoch 085: | Train Loss: 0.80676 | Test Loss: 0.80654\n",
      "Epoch 086: | Train Loss: 0.70556 | Test Loss: 0.81292\n",
      "Epoch 087: | Train Loss: 0.92786 | Test Loss: 0.84348\n",
      "Epoch 088: | Train Loss: 0.73490 | Test Loss: 0.83494\n",
      "Epoch 089: | Train Loss: 0.63118 | Test Loss: 0.89475\n",
      "Epoch 090: | Train Loss: 0.98437 | Test Loss: 0.85223\n",
      "Epoch 091: | Train Loss: 0.63420 | Test Loss: 0.86236\n",
      "Epoch 092: | Train Loss: 0.75981 | Test Loss: 0.83633\n",
      "Epoch 093: | Train Loss: 0.66637 | Test Loss: 0.89777\n",
      "Epoch 094: | Train Loss: 0.60167 | Test Loss: 0.82407\n",
      "Epoch 095: | Train Loss: 0.64677 | Test Loss: 0.82762\n",
      "Epoch 096: | Train Loss: 0.59517 | Test Loss: 0.87551\n",
      "Epoch 097: | Train Loss: 0.60314 | Test Loss: 0.83375\n",
      "Epoch 098: | Train Loss: 0.59687 | Test Loss: 0.83409\n",
      "Epoch 099: | Train Loss: 0.66833 | Test Loss: 0.85189\n",
      "Epoch 100: | Train Loss: 0.95959 | Test Loss: 0.82584\n",
      "Epoch 101: | Train Loss: 0.63275 | Test Loss: 0.84582\n",
      "Epoch 102: | Train Loss: 0.61740 | Test Loss: 0.89615\n",
      "Epoch 103: | Train Loss: 0.65892 | Test Loss: 0.90105\n",
      "Epoch 104: | Train Loss: 0.77073 | Test Loss: 0.81532\n",
      "Epoch 105: | Train Loss: 0.76482 | Test Loss: 0.82405\n",
      "Epoch 106: | Train Loss: 0.86046 | Test Loss: 1.00166\n",
      "Epoch 107: | Train Loss: 0.66159 | Test Loss: 0.88075\n",
      "Epoch 108: | Train Loss: 0.60396 | Test Loss: 0.83161\n",
      "Epoch 109: | Train Loss: 0.58902 | Test Loss: 0.81679\n",
      "Epoch 110: | Train Loss: 0.55901 | Test Loss: 0.86408\n",
      "Epoch 111: | Train Loss: 0.61128 | Test Loss: 0.81393\n",
      "Epoch 112: | Train Loss: 0.56196 | Test Loss: 0.80638\n",
      "Epoch 113: | Train Loss: 0.60267 | Test Loss: 0.82535\n",
      "Epoch 114: | Train Loss: 0.63769 | Test Loss: 0.81093\n",
      "Epoch 115: | Train Loss: 0.60809 | Test Loss: 0.86452\n",
      "Epoch 116: | Train Loss: 0.66917 | Test Loss: 0.84985\n",
      "Epoch 117: | Train Loss: 0.54999 | Test Loss: 0.82061\n",
      "Epoch 118: | Train Loss: 0.54921 | Test Loss: 0.80745\n",
      "Epoch 119: | Train Loss: 0.60633 | Test Loss: 0.83060\n",
      "Epoch 120: | Train Loss: 0.53809 | Test Loss: 0.80928\n",
      "Epoch 121: | Train Loss: 0.54999 | Test Loss: 0.84741\n",
      "Epoch 122: | Train Loss: 0.66067 | Test Loss: 0.87781\n",
      "Epoch 123: | Train Loss: 0.55892 | Test Loss: 0.85480\n",
      "Epoch 124: | Train Loss: 0.60599 | Test Loss: 0.82086\n",
      "Epoch 125: | Train Loss: 0.54345 | Test Loss: 0.85770\n",
      "Epoch 126: | Train Loss: 0.52141 | Test Loss: 0.79542\n",
      "Epoch 127: | Train Loss: 0.52589 | Test Loss: 0.80083\n",
      "Epoch 128: | Train Loss: 0.51278 | Test Loss: 0.80792\n",
      "Epoch 129: | Train Loss: 0.58474 | Test Loss: 0.80944\n",
      "Epoch 130: | Train Loss: 0.52415 | Test Loss: 0.83507\n",
      "Epoch 131: | Train Loss: 0.51577 | Test Loss: 0.81891\n",
      "Epoch 132: | Train Loss: 0.52847 | Test Loss: 0.81221\n",
      "Epoch 133: | Train Loss: 0.50029 | Test Loss: 0.84479\n",
      "Epoch 134: | Train Loss: 0.72016 | Test Loss: 0.80174\n",
      "Epoch 135: | Train Loss: 0.56037 | Test Loss: 0.81236\n",
      "Epoch 136: | Train Loss: 0.64912 | Test Loss: 0.85854\n",
      "Epoch 137: | Train Loss: 0.55641 | Test Loss: 0.81560\n",
      "Epoch 138: | Train Loss: 0.59536 | Test Loss: 0.90887\n",
      "Epoch 139: | Train Loss: 0.56639 | Test Loss: 0.84587\n",
      "Epoch 140: | Train Loss: 0.49866 | Test Loss: 0.81064\n",
      "Epoch 141: | Train Loss: 0.50137 | Test Loss: 0.81414\n",
      "Epoch 142: | Train Loss: 0.56529 | Test Loss: 0.86582\n",
      "Epoch 143: | Train Loss: 0.52119 | Test Loss: 0.84853\n",
      "Epoch 144: | Train Loss: 0.47364 | Test Loss: 0.80175\n",
      "Epoch 145: | Train Loss: 0.59483 | Test Loss: 0.80531\n",
      "Epoch 146: | Train Loss: 0.50110 | Test Loss: 0.78654\n",
      "Epoch 147: | Train Loss: 0.77700 | Test Loss: 0.85639\n",
      "Epoch 148: | Train Loss: 0.71397 | Test Loss: 0.88260\n",
      "Epoch 149: | Train Loss: 0.49440 | Test Loss: 0.80213\n",
      "Epoch 150: | Train Loss: 0.51575 | Test Loss: 0.80271\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015390872955322266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d423a9f064b64665929984728ca02e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 37.63210 | Test Loss: 38.41620\n",
      "Epoch 002: | Train Loss: 40.19936 | Test Loss: 37.60913\n",
      "Epoch 003: | Train Loss: 35.90848 | Test Loss: 36.68588\n",
      "Epoch 004: | Train Loss: 36.83293 | Test Loss: 34.74716\n",
      "Epoch 005: | Train Loss: 32.29147 | Test Loss: 30.52908\n",
      "Epoch 006: | Train Loss: 27.10233 | Test Loss: 22.80485\n",
      "Epoch 007: | Train Loss: 18.54940 | Test Loss: 11.40144\n",
      "Epoch 008: | Train Loss: 7.20149 | Test Loss: 3.79490\n",
      "Epoch 009: | Train Loss: 3.60277 | Test Loss: 4.90740\n",
      "Epoch 010: | Train Loss: 3.87307 | Test Loss: 3.47105\n",
      "Epoch 011: | Train Loss: 3.37119 | Test Loss: 3.45925\n",
      "Epoch 012: | Train Loss: 3.12300 | Test Loss: 3.29913\n",
      "Epoch 013: | Train Loss: 3.01805 | Test Loss: 3.19740\n",
      "Epoch 014: | Train Loss: 2.67638 | Test Loss: 2.80539\n",
      "Epoch 015: | Train Loss: 2.63909 | Test Loss: 2.60992\n",
      "Epoch 016: | Train Loss: 2.40212 | Test Loss: 2.44073\n",
      "Epoch 017: | Train Loss: 2.38397 | Test Loss: 2.29179\n",
      "Epoch 018: | Train Loss: 2.22061 | Test Loss: 2.33518\n",
      "Epoch 019: | Train Loss: 2.06544 | Test Loss: 2.02707\n",
      "Epoch 020: | Train Loss: 1.88831 | Test Loss: 1.91276\n",
      "Epoch 021: | Train Loss: 1.80398 | Test Loss: 1.89512\n",
      "Epoch 022: | Train Loss: 1.72815 | Test Loss: 1.74414\n",
      "Epoch 023: | Train Loss: 1.56634 | Test Loss: 1.57869\n",
      "Epoch 024: | Train Loss: 1.54322 | Test Loss: 1.49221\n",
      "Epoch 025: | Train Loss: 1.36216 | Test Loss: 1.47015\n",
      "Epoch 026: | Train Loss: 1.61910 | Test Loss: 1.36339\n",
      "Epoch 027: | Train Loss: 1.57824 | Test Loss: 1.40890\n",
      "Epoch 028: | Train Loss: 1.28766 | Test Loss: 1.26479\n",
      "Epoch 029: | Train Loss: 1.20459 | Test Loss: 1.28577\n",
      "Epoch 030: | Train Loss: 1.16169 | Test Loss: 1.18439\n",
      "Epoch 031: | Train Loss: 1.15669 | Test Loss: 1.14983\n",
      "Epoch 032: | Train Loss: 1.09859 | Test Loss: 1.13948\n",
      "Epoch 033: | Train Loss: 1.47618 | Test Loss: 1.10530\n",
      "Epoch 034: | Train Loss: 1.16932 | Test Loss: 1.09700\n",
      "Epoch 035: | Train Loss: 1.04479 | Test Loss: 1.15770\n",
      "Epoch 036: | Train Loss: 1.05125 | Test Loss: 1.03905\n",
      "Epoch 037: | Train Loss: 0.96166 | Test Loss: 1.01789\n",
      "Epoch 038: | Train Loss: 1.00741 | Test Loss: 1.00701\n",
      "Epoch 039: | Train Loss: 1.07186 | Test Loss: 0.99608\n",
      "Epoch 040: | Train Loss: 0.96257 | Test Loss: 1.02849\n",
      "Epoch 041: | Train Loss: 0.93836 | Test Loss: 0.95799\n",
      "Epoch 042: | Train Loss: 0.94847 | Test Loss: 0.94732\n",
      "Epoch 043: | Train Loss: 0.92076 | Test Loss: 0.99717\n",
      "Epoch 044: | Train Loss: 0.92324 | Test Loss: 0.93284\n",
      "Epoch 045: | Train Loss: 1.02263 | Test Loss: 0.91768\n",
      "Epoch 046: | Train Loss: 0.95578 | Test Loss: 0.92558\n",
      "Epoch 047: | Train Loss: 0.87659 | Test Loss: 0.89987\n",
      "Epoch 048: | Train Loss: 0.84007 | Test Loss: 0.91665\n",
      "Epoch 049: | Train Loss: 1.08360 | Test Loss: 0.89873\n",
      "Epoch 050: | Train Loss: 0.84290 | Test Loss: 0.90463\n",
      "Epoch 051: | Train Loss: 1.10182 | Test Loss: 0.87496\n",
      "Epoch 052: | Train Loss: 0.87292 | Test Loss: 0.88096\n",
      "Epoch 053: | Train Loss: 0.82889 | Test Loss: 0.91744\n",
      "Epoch 054: | Train Loss: 0.79080 | Test Loss: 0.86128\n",
      "Epoch 055: | Train Loss: 0.78592 | Test Loss: 0.86456\n",
      "Epoch 056: | Train Loss: 0.77849 | Test Loss: 0.86044\n",
      "Epoch 057: | Train Loss: 0.76582 | Test Loss: 0.85198\n",
      "Epoch 058: | Train Loss: 0.80564 | Test Loss: 0.84799\n",
      "Epoch 059: | Train Loss: 0.75324 | Test Loss: 0.84891\n",
      "Epoch 060: | Train Loss: 0.91176 | Test Loss: 0.84748\n",
      "Epoch 061: | Train Loss: 0.89346 | Test Loss: 0.85343\n",
      "Epoch 062: | Train Loss: 0.85178 | Test Loss: 0.83278\n",
      "Epoch 063: | Train Loss: 0.82481 | Test Loss: 0.87868\n",
      "Epoch 064: | Train Loss: 0.97574 | Test Loss: 0.84672\n",
      "Epoch 065: | Train Loss: 0.76786 | Test Loss: 0.83126\n",
      "Epoch 066: | Train Loss: 0.73264 | Test Loss: 0.85667\n",
      "Epoch 067: | Train Loss: 0.82951 | Test Loss: 0.82957\n",
      "Epoch 068: | Train Loss: 0.71902 | Test Loss: 0.88799\n",
      "Epoch 069: | Train Loss: 0.86772 | Test Loss: 0.82628\n",
      "Epoch 070: | Train Loss: 0.92431 | Test Loss: 0.84827\n",
      "Epoch 071: | Train Loss: 0.99830 | Test Loss: 0.89630\n",
      "Epoch 072: | Train Loss: 0.79818 | Test Loss: 0.89919\n",
      "Epoch 073: | Train Loss: 1.02528 | Test Loss: 0.97086\n",
      "Epoch 074: | Train Loss: 0.79890 | Test Loss: 0.84069\n",
      "Epoch 075: | Train Loss: 0.75535 | Test Loss: 0.84124\n",
      "Epoch 076: | Train Loss: 0.69295 | Test Loss: 0.86484\n",
      "Epoch 077: | Train Loss: 0.88210 | Test Loss: 0.86839\n",
      "Epoch 078: | Train Loss: 0.81201 | Test Loss: 0.83212\n",
      "Epoch 079: | Train Loss: 0.71946 | Test Loss: 0.82484\n",
      "Epoch 080: | Train Loss: 0.70080 | Test Loss: 0.93589\n",
      "Epoch 081: | Train Loss: 0.68927 | Test Loss: 0.81571\n",
      "Epoch 082: | Train Loss: 0.96433 | Test Loss: 0.84107\n",
      "Epoch 083: | Train Loss: 0.91853 | Test Loss: 0.83168\n",
      "Epoch 084: | Train Loss: 0.68680 | Test Loss: 0.83368\n",
      "Epoch 085: | Train Loss: 0.65729 | Test Loss: 0.87165\n",
      "Epoch 086: | Train Loss: 0.63922 | Test Loss: 0.80094\n",
      "Epoch 087: | Train Loss: 0.64005 | Test Loss: 0.79654\n",
      "Epoch 088: | Train Loss: 0.63690 | Test Loss: 0.84562\n",
      "Epoch 089: | Train Loss: 0.62682 | Test Loss: 0.79986\n",
      "Epoch 090: | Train Loss: 0.65850 | Test Loss: 0.80886\n",
      "Epoch 091: | Train Loss: 0.82075 | Test Loss: 0.81057\n",
      "Epoch 092: | Train Loss: 0.65445 | Test Loss: 0.82377\n",
      "Epoch 093: | Train Loss: 0.61061 | Test Loss: 0.86688\n",
      "Epoch 094: | Train Loss: 0.60378 | Test Loss: 0.81298\n",
      "Epoch 095: | Train Loss: 0.58869 | Test Loss: 0.82452\n",
      "Epoch 096: | Train Loss: 0.61354 | Test Loss: 0.80462\n",
      "Epoch 097: | Train Loss: 0.59374 | Test Loss: 0.81722\n",
      "Epoch 098: | Train Loss: 0.85104 | Test Loss: 0.80021\n",
      "Epoch 099: | Train Loss: 0.87757 | Test Loss: 0.83312\n",
      "Epoch 100: | Train Loss: 0.61455 | Test Loss: 0.87101\n",
      "Epoch 101: | Train Loss: 0.58466 | Test Loss: 0.84115\n",
      "Epoch 102: | Train Loss: 0.56744 | Test Loss: 0.81433\n",
      "Epoch 103: | Train Loss: 0.56348 | Test Loss: 0.83064\n",
      "Epoch 104: | Train Loss: 0.69413 | Test Loss: 0.80197\n",
      "Epoch 105: | Train Loss: 0.61482 | Test Loss: 0.82942\n",
      "Epoch 106: | Train Loss: 0.56455 | Test Loss: 0.88426\n",
      "Epoch 107: | Train Loss: 0.56508 | Test Loss: 0.82432\n",
      "Epoch 108: | Train Loss: 0.55047 | Test Loss: 0.84235\n",
      "Epoch 109: | Train Loss: 0.54406 | Test Loss: 0.85593\n",
      "Epoch 110: | Train Loss: 0.54102 | Test Loss: 0.82588\n",
      "Epoch 111: | Train Loss: 0.53868 | Test Loss: 0.86743\n",
      "Epoch 112: | Train Loss: 0.56970 | Test Loss: 0.85624\n",
      "Epoch 113: | Train Loss: 0.54058 | Test Loss: 0.86436\n",
      "Epoch 114: | Train Loss: 0.69837 | Test Loss: 0.86945\n",
      "Epoch 115: | Train Loss: 0.55784 | Test Loss: 0.84192\n",
      "Epoch 116: | Train Loss: 0.52896 | Test Loss: 0.83014\n",
      "Epoch 117: | Train Loss: 0.53337 | Test Loss: 0.86648\n",
      "Epoch 118: | Train Loss: 0.69515 | Test Loss: 0.85232\n",
      "Epoch 119: | Train Loss: 0.56504 | Test Loss: 0.87330\n",
      "Epoch 120: | Train Loss: 0.77578 | Test Loss: 0.84461\n",
      "Epoch 121: | Train Loss: 0.54790 | Test Loss: 0.86150\n",
      "Epoch 122: | Train Loss: 0.53671 | Test Loss: 0.89396\n",
      "Epoch 123: | Train Loss: 0.50527 | Test Loss: 0.85064\n",
      "Epoch 124: | Train Loss: 0.61772 | Test Loss: 0.82656\n",
      "Epoch 125: | Train Loss: 0.56069 | Test Loss: 0.81937\n",
      "Epoch 126: | Train Loss: 0.52555 | Test Loss: 0.83587\n",
      "Epoch 127: | Train Loss: 0.51448 | Test Loss: 0.85738\n",
      "Epoch 128: | Train Loss: 0.64009 | Test Loss: 0.88390\n",
      "Epoch 129: | Train Loss: 0.48478 | Test Loss: 0.84989\n",
      "Epoch 130: | Train Loss: 0.47451 | Test Loss: 0.83019\n",
      "Epoch 131: | Train Loss: 0.48430 | Test Loss: 0.84130\n",
      "Epoch 132: | Train Loss: 0.48489 | Test Loss: 0.84495\n",
      "Epoch 133: | Train Loss: 0.64381 | Test Loss: 0.81904\n",
      "Epoch 134: | Train Loss: 0.65109 | Test Loss: 0.81495\n",
      "Epoch 135: | Train Loss: 0.52297 | Test Loss: 0.89601\n",
      "Epoch 136: | Train Loss: 0.51142 | Test Loss: 0.80895\n",
      "Epoch 137: | Train Loss: 0.57440 | Test Loss: 0.86017\n",
      "Epoch 138: | Train Loss: 0.53803 | Test Loss: 0.81723\n",
      "Epoch 139: | Train Loss: 0.51011 | Test Loss: 0.81165\n",
      "Epoch 140: | Train Loss: 0.48975 | Test Loss: 0.86017\n",
      "Epoch 141: | Train Loss: 0.47892 | Test Loss: 0.81943\n",
      "Epoch 142: | Train Loss: 0.51032 | Test Loss: 0.83793\n",
      "Epoch 143: | Train Loss: 0.53824 | Test Loss: 0.85391\n",
      "Epoch 144: | Train Loss: 0.48756 | Test Loss: 0.83477\n",
      "Epoch 145: | Train Loss: 0.47558 | Test Loss: 0.80954\n",
      "Epoch 146: | Train Loss: 0.46867 | Test Loss: 0.84521\n",
      "Epoch 147: | Train Loss: 0.56984 | Test Loss: 0.87109\n",
      "Epoch 148: | Train Loss: 0.44258 | Test Loss: 0.85112\n",
      "Epoch 149: | Train Loss: 0.52119 | Test Loss: 0.87701\n",
      "Epoch 150: | Train Loss: 0.60032 | Test Loss: 0.88416\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01711297035217285,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7492ff999c0f44309aacc618bb22cbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 39.20369 | Test Loss: 39.23815\n",
      "Epoch 002: | Train Loss: 38.37359 | Test Loss: 38.30925\n",
      "Epoch 003: | Train Loss: 40.36951 | Test Loss: 36.98740\n",
      "Epoch 004: | Train Loss: 35.16481 | Test Loss: 34.69244\n",
      "Epoch 005: | Train Loss: 33.04051 | Test Loss: 29.80287\n",
      "Epoch 006: | Train Loss: 26.63849 | Test Loss: 20.73363\n",
      "Epoch 007: | Train Loss: 16.19300 | Test Loss: 8.91144\n",
      "Epoch 008: | Train Loss: 5.73908 | Test Loss: 4.35698\n",
      "Epoch 009: | Train Loss: 6.07821 | Test Loss: 5.97613\n",
      "Epoch 010: | Train Loss: 4.12219 | Test Loss: 3.86031\n",
      "Epoch 011: | Train Loss: 3.15258 | Test Loss: 3.52292\n",
      "Epoch 012: | Train Loss: 3.46817 | Test Loss: 3.38450\n",
      "Epoch 013: | Train Loss: 4.60237 | Test Loss: 3.20240\n",
      "Epoch 014: | Train Loss: 2.81288 | Test Loss: 3.28776\n",
      "Epoch 015: | Train Loss: 2.72597 | Test Loss: 2.96372\n",
      "Epoch 016: | Train Loss: 2.53012 | Test Loss: 2.78558\n",
      "Epoch 017: | Train Loss: 2.50743 | Test Loss: 2.69207\n",
      "Epoch 018: | Train Loss: 2.32157 | Test Loss: 2.59698\n",
      "Epoch 019: | Train Loss: 2.26269 | Test Loss: 2.53227\n",
      "Epoch 020: | Train Loss: 2.17787 | Test Loss: 2.45073\n",
      "Epoch 021: | Train Loss: 2.25993 | Test Loss: 2.30510\n",
      "Epoch 022: | Train Loss: 2.04956 | Test Loss: 2.21540\n",
      "Epoch 023: | Train Loss: 2.16532 | Test Loss: 2.13286\n",
      "Epoch 024: | Train Loss: 2.55890 | Test Loss: 2.06163\n",
      "Epoch 025: | Train Loss: 1.96577 | Test Loss: 2.03264\n",
      "Epoch 026: | Train Loss: 1.81342 | Test Loss: 1.93808\n",
      "Epoch 027: | Train Loss: 1.78878 | Test Loss: 1.90903\n",
      "Epoch 028: | Train Loss: 1.66943 | Test Loss: 1.81800\n",
      "Epoch 029: | Train Loss: 1.69822 | Test Loss: 1.78222\n",
      "Epoch 030: | Train Loss: 1.58337 | Test Loss: 1.74417\n",
      "Epoch 031: | Train Loss: 2.13226 | Test Loss: 1.63642\n",
      "Epoch 032: | Train Loss: 2.96990 | Test Loss: 1.66470\n",
      "Epoch 033: | Train Loss: 1.54644 | Test Loss: 1.62762\n",
      "Epoch 034: | Train Loss: 1.73564 | Test Loss: 1.47603\n",
      "Epoch 035: | Train Loss: 1.32014 | Test Loss: 1.42148\n",
      "Epoch 036: | Train Loss: 1.27111 | Test Loss: 1.35746\n",
      "Epoch 037: | Train Loss: 1.44127 | Test Loss: 1.31869\n",
      "Epoch 038: | Train Loss: 1.25537 | Test Loss: 1.31398\n",
      "Epoch 039: | Train Loss: 1.20534 | Test Loss: 1.26925\n",
      "Epoch 040: | Train Loss: 1.16696 | Test Loss: 1.26903\n",
      "Epoch 041: | Train Loss: 1.13720 | Test Loss: 1.20499\n",
      "Epoch 042: | Train Loss: 1.34517 | Test Loss: 1.17535\n",
      "Epoch 043: | Train Loss: 1.07257 | Test Loss: 1.21640\n",
      "Epoch 044: | Train Loss: 1.08476 | Test Loss: 1.09696\n",
      "Epoch 045: | Train Loss: 1.03506 | Test Loss: 1.05346\n",
      "Epoch 046: | Train Loss: 0.99239 | Test Loss: 1.04857\n",
      "Epoch 047: | Train Loss: 1.27926 | Test Loss: 0.99634\n",
      "Epoch 048: | Train Loss: 0.94831 | Test Loss: 1.01165\n",
      "Epoch 049: | Train Loss: 1.03716 | Test Loss: 0.92944\n",
      "Epoch 050: | Train Loss: 0.94210 | Test Loss: 0.94394\n",
      "Epoch 051: | Train Loss: 0.86930 | Test Loss: 0.89331\n",
      "Epoch 052: | Train Loss: 0.83626 | Test Loss: 0.86742\n",
      "Epoch 053: | Train Loss: 1.27478 | Test Loss: 0.87620\n",
      "Epoch 054: | Train Loss: 1.26881 | Test Loss: 0.84323\n",
      "Epoch 055: | Train Loss: 0.85260 | Test Loss: 0.89141\n",
      "Epoch 056: | Train Loss: 1.10595 | Test Loss: 0.83968\n",
      "Epoch 057: | Train Loss: 0.77445 | Test Loss: 0.86160\n",
      "Epoch 058: | Train Loss: 1.04715 | Test Loss: 0.89145\n",
      "Epoch 059: | Train Loss: 0.78343 | Test Loss: 0.85773\n",
      "Epoch 060: | Train Loss: 0.89370 | Test Loss: 0.81399\n",
      "Epoch 061: | Train Loss: 0.81296 | Test Loss: 0.81103\n",
      "Epoch 062: | Train Loss: 0.76489 | Test Loss: 0.82308\n",
      "Epoch 063: | Train Loss: 0.72289 | Test Loss: 0.83214\n",
      "Epoch 064: | Train Loss: 0.69923 | Test Loss: 0.78066\n",
      "Epoch 065: | Train Loss: 0.68505 | Test Loss: 0.78621\n",
      "Epoch 066: | Train Loss: 0.89717 | Test Loss: 0.80364\n",
      "Epoch 067: | Train Loss: 0.71667 | Test Loss: 0.80208\n",
      "Epoch 068: | Train Loss: 0.78997 | Test Loss: 0.78218\n",
      "Epoch 069: | Train Loss: 0.72986 | Test Loss: 0.83612\n",
      "Epoch 070: | Train Loss: 0.83637 | Test Loss: 0.76690\n",
      "Epoch 071: | Train Loss: 0.67815 | Test Loss: 0.79023\n",
      "Epoch 072: | Train Loss: 0.72628 | Test Loss: 0.75407\n",
      "Epoch 073: | Train Loss: 0.70074 | Test Loss: 0.75448\n",
      "Epoch 074: | Train Loss: 0.69280 | Test Loss: 0.79590\n",
      "Epoch 075: | Train Loss: 0.61259 | Test Loss: 0.74722\n",
      "Epoch 076: | Train Loss: 0.62215 | Test Loss: 0.74636\n",
      "Epoch 077: | Train Loss: 0.77921 | Test Loss: 0.73893\n",
      "Epoch 078: | Train Loss: 0.65488 | Test Loss: 0.74236\n",
      "Epoch 079: | Train Loss: 0.67031 | Test Loss: 0.71726\n",
      "Epoch 080: | Train Loss: 0.60322 | Test Loss: 0.71758\n",
      "Epoch 081: | Train Loss: 0.59093 | Test Loss: 0.71744\n",
      "Epoch 082: | Train Loss: 0.59642 | Test Loss: 0.70219\n",
      "Epoch 083: | Train Loss: 1.13926 | Test Loss: 0.70920\n",
      "Epoch 084: | Train Loss: 0.99018 | Test Loss: 0.71416\n",
      "Epoch 085: | Train Loss: 0.64174 | Test Loss: 0.89723\n",
      "Epoch 086: | Train Loss: 0.67885 | Test Loss: 0.70663\n",
      "Epoch 087: | Train Loss: 0.62002 | Test Loss: 0.70644\n",
      "Epoch 088: | Train Loss: 0.64956 | Test Loss: 0.71110\n",
      "Epoch 089: | Train Loss: 0.58203 | Test Loss: 0.74624\n",
      "Epoch 090: | Train Loss: 0.54463 | Test Loss: 0.70403\n",
      "Epoch 091: | Train Loss: 0.55277 | Test Loss: 0.70657\n",
      "Epoch 092: | Train Loss: 0.53113 | Test Loss: 0.73195\n",
      "Epoch 093: | Train Loss: 0.53248 | Test Loss: 0.69974\n",
      "Epoch 094: | Train Loss: 0.83923 | Test Loss: 0.69822\n",
      "Epoch 095: | Train Loss: 0.58360 | Test Loss: 0.70056\n",
      "Epoch 096: | Train Loss: 0.83123 | Test Loss: 0.72300\n",
      "Epoch 097: | Train Loss: 0.76517 | Test Loss: 0.71284\n",
      "Epoch 098: | Train Loss: 0.55648 | Test Loss: 0.83004\n",
      "Epoch 099: | Train Loss: 0.58982 | Test Loss: 0.72628\n",
      "Epoch 100: | Train Loss: 0.62200 | Test Loss: 0.71985\n",
      "Epoch 101: | Train Loss: 0.56464 | Test Loss: 0.82477\n",
      "Epoch 102: | Train Loss: 0.56426 | Test Loss: 0.72097\n",
      "Epoch 103: | Train Loss: 0.50661 | Test Loss: 0.71954\n",
      "Epoch 104: | Train Loss: 0.54526 | Test Loss: 0.72155\n",
      "Epoch 105: | Train Loss: 0.49601 | Test Loss: 0.71400\n",
      "Epoch 106: | Train Loss: 0.50864 | Test Loss: 0.73437\n",
      "Epoch 107: | Train Loss: 0.49773 | Test Loss: 0.72431\n",
      "Epoch 108: | Train Loss: 0.56842 | Test Loss: 0.70925\n",
      "Epoch 109: | Train Loss: 0.51461 | Test Loss: 0.70515\n",
      "Epoch 110: | Train Loss: 0.48069 | Test Loss: 0.72664\n",
      "Epoch 111: | Train Loss: 0.48088 | Test Loss: 0.72233\n",
      "Epoch 112: | Train Loss: 0.46904 | Test Loss: 0.70317\n",
      "Epoch 113: | Train Loss: 0.46895 | Test Loss: 0.71648\n",
      "Epoch 114: | Train Loss: 0.46162 | Test Loss: 0.71747\n",
      "Epoch 115: | Train Loss: 0.46977 | Test Loss: 0.70869\n",
      "Epoch 116: | Train Loss: 0.46830 | Test Loss: 0.70980\n",
      "Epoch 117: | Train Loss: 0.46872 | Test Loss: 0.70193\n",
      "Epoch 118: | Train Loss: 0.45350 | Test Loss: 0.71852\n",
      "Epoch 119: | Train Loss: 0.45297 | Test Loss: 0.71806\n",
      "Epoch 120: | Train Loss: 0.45066 | Test Loss: 0.70644\n",
      "Epoch 121: | Train Loss: 0.45372 | Test Loss: 0.71547\n",
      "Epoch 122: | Train Loss: 0.43665 | Test Loss: 0.70707\n",
      "Epoch 123: | Train Loss: 0.45024 | Test Loss: 0.72852\n",
      "Epoch 124: | Train Loss: 0.45906 | Test Loss: 0.73790\n",
      "Epoch 125: | Train Loss: 0.58944 | Test Loss: 0.72733\n",
      "Epoch 126: | Train Loss: 0.48398 | Test Loss: 0.77945\n",
      "Epoch 127: | Train Loss: 0.49497 | Test Loss: 0.74204\n",
      "Epoch 128: | Train Loss: 0.46305 | Test Loss: 0.74993\n",
      "Epoch 129: | Train Loss: 0.46335 | Test Loss: 0.73988\n",
      "Epoch 130: | Train Loss: 0.62272 | Test Loss: 0.74410\n",
      "Epoch 131: | Train Loss: 0.48660 | Test Loss: 0.75232\n",
      "Epoch 132: | Train Loss: 0.47065 | Test Loss: 0.77093\n",
      "Epoch 133: | Train Loss: 0.52926 | Test Loss: 0.75461\n",
      "Epoch 134: | Train Loss: 0.63046 | Test Loss: 0.74347\n",
      "Epoch 135: | Train Loss: 0.47987 | Test Loss: 0.75562\n",
      "Epoch 136: | Train Loss: 0.45336 | Test Loss: 0.81071\n",
      "Epoch 137: | Train Loss: 0.45278 | Test Loss: 0.73926\n",
      "Epoch 138: | Train Loss: 0.44754 | Test Loss: 0.74865\n",
      "Epoch 139: | Train Loss: 0.41216 | Test Loss: 0.75424\n",
      "Epoch 140: | Train Loss: 0.54709 | Test Loss: 0.74969\n",
      "Epoch 141: | Train Loss: 0.43484 | Test Loss: 0.76905\n",
      "Epoch 142: | Train Loss: 0.42080 | Test Loss: 0.72663\n",
      "Epoch 143: | Train Loss: 0.42235 | Test Loss: 0.77403\n",
      "Epoch 144: | Train Loss: 0.42253 | Test Loss: 0.74375\n",
      "Epoch 145: | Train Loss: 0.42693 | Test Loss: 0.73458\n",
      "Epoch 146: | Train Loss: 0.50017 | Test Loss: 0.74536\n",
      "Epoch 147: | Train Loss: 0.44418 | Test Loss: 0.79240\n",
      "Epoch 148: | Train Loss: 0.44860 | Test Loss: 0.73355\n",
      "Epoch 149: | Train Loss: 0.45028 | Test Loss: 0.73302\n",
      "Epoch 150: | Train Loss: 0.39936 | Test Loss: 0.73817\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017129898071289062,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa67e88e05b34bb0be9bd17cab81dbb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 40.25394 | Test Loss: 38.67225\n",
      "Epoch 002: | Train Loss: 36.91128 | Test Loss: 37.71722\n",
      "Epoch 003: | Train Loss: 37.06013 | Test Loss: 36.14649\n",
      "Epoch 004: | Train Loss: 36.00234 | Test Loss: 32.88130\n",
      "Epoch 005: | Train Loss: 30.88541 | Test Loss: 26.51960\n",
      "Epoch 006: | Train Loss: 22.14905 | Test Loss: 16.19399\n",
      "Epoch 007: | Train Loss: 11.20919 | Test Loss: 5.62452\n",
      "Epoch 008: | Train Loss: 4.20776 | Test Loss: 5.53622\n",
      "Epoch 009: | Train Loss: 4.72954 | Test Loss: 4.44103\n",
      "Epoch 010: | Train Loss: 3.41743 | Test Loss: 3.74424\n",
      "Epoch 011: | Train Loss: 3.32202 | Test Loss: 3.54154\n",
      "Epoch 012: | Train Loss: 3.03865 | Test Loss: 3.38414\n",
      "Epoch 013: | Train Loss: 4.86963 | Test Loss: 3.30025\n",
      "Epoch 014: | Train Loss: 3.99599 | Test Loss: 3.19315\n",
      "Epoch 015: | Train Loss: 3.31689 | Test Loss: 2.88688\n",
      "Epoch 016: | Train Loss: 2.53219 | Test Loss: 2.76876\n",
      "Epoch 017: | Train Loss: 2.46134 | Test Loss: 2.59384\n",
      "Epoch 018: | Train Loss: 2.62284 | Test Loss: 2.54520\n",
      "Epoch 019: | Train Loss: 2.19467 | Test Loss: 2.44580\n",
      "Epoch 020: | Train Loss: 2.10283 | Test Loss: 2.32179\n",
      "Epoch 021: | Train Loss: 2.05247 | Test Loss: 2.17490\n",
      "Epoch 022: | Train Loss: 3.37393 | Test Loss: 2.08330\n",
      "Epoch 023: | Train Loss: 1.88741 | Test Loss: 2.15191\n",
      "Epoch 024: | Train Loss: 1.94313 | Test Loss: 1.98672\n",
      "Epoch 025: | Train Loss: 1.67184 | Test Loss: 1.81570\n",
      "Epoch 026: | Train Loss: 1.65858 | Test Loss: 1.73984\n",
      "Epoch 027: | Train Loss: 1.58521 | Test Loss: 1.67157\n",
      "Epoch 028: | Train Loss: 1.52631 | Test Loss: 1.62868\n",
      "Epoch 029: | Train Loss: 1.52347 | Test Loss: 1.55621\n",
      "Epoch 030: | Train Loss: 1.71385 | Test Loss: 1.49231\n",
      "Epoch 031: | Train Loss: 1.91910 | Test Loss: 1.45941\n",
      "Epoch 032: | Train Loss: 1.32099 | Test Loss: 1.46598\n",
      "Epoch 033: | Train Loss: 1.47183 | Test Loss: 1.35049\n",
      "Epoch 034: | Train Loss: 1.21479 | Test Loss: 1.34011\n",
      "Epoch 035: | Train Loss: 1.24469 | Test Loss: 1.26694\n",
      "Epoch 036: | Train Loss: 1.13459 | Test Loss: 1.24227\n",
      "Epoch 037: | Train Loss: 1.28381 | Test Loss: 1.21737\n",
      "Epoch 038: | Train Loss: 1.20952 | Test Loss: 1.21058\n",
      "Epoch 039: | Train Loss: 1.40450 | Test Loss: 1.12684\n",
      "Epoch 040: | Train Loss: 3.99048 | Test Loss: 1.15008\n",
      "Epoch 041: | Train Loss: 1.04498 | Test Loss: 1.17264\n",
      "Epoch 042: | Train Loss: 1.06570 | Test Loss: 1.08492\n",
      "Epoch 043: | Train Loss: 1.00442 | Test Loss: 1.06319\n",
      "Epoch 044: | Train Loss: 0.97626 | Test Loss: 1.02695\n",
      "Epoch 045: | Train Loss: 0.95356 | Test Loss: 1.03364\n",
      "Epoch 046: | Train Loss: 0.99216 | Test Loss: 1.02041\n",
      "Epoch 047: | Train Loss: 1.09354 | Test Loss: 0.98294\n",
      "Epoch 048: | Train Loss: 1.23069 | Test Loss: 1.03151\n",
      "Epoch 049: | Train Loss: 0.97065 | Test Loss: 0.97843\n",
      "Epoch 050: | Train Loss: 0.98968 | Test Loss: 0.99178\n",
      "Epoch 051: | Train Loss: 0.95765 | Test Loss: 0.94293\n",
      "Epoch 052: | Train Loss: 0.85447 | Test Loss: 0.92373\n",
      "Epoch 053: | Train Loss: 0.96680 | Test Loss: 0.92806\n",
      "Epoch 054: | Train Loss: 0.99261 | Test Loss: 0.94269\n",
      "Epoch 055: | Train Loss: 1.11348 | Test Loss: 0.92423\n",
      "Epoch 056: | Train Loss: 0.96948 | Test Loss: 0.94408\n",
      "Epoch 057: | Train Loss: 0.86235 | Test Loss: 0.96325\n",
      "Epoch 058: | Train Loss: 1.01180 | Test Loss: 0.90585\n",
      "Epoch 059: | Train Loss: 0.79603 | Test Loss: 0.89142\n",
      "Epoch 060: | Train Loss: 0.76883 | Test Loss: 0.87806\n",
      "Epoch 061: | Train Loss: 0.83740 | Test Loss: 0.86344\n",
      "Epoch 062: | Train Loss: 0.73111 | Test Loss: 0.87144\n",
      "Epoch 063: | Train Loss: 0.91864 | Test Loss: 0.85792\n",
      "Epoch 064: | Train Loss: 1.03418 | Test Loss: 0.86109\n",
      "Epoch 065: | Train Loss: 0.79392 | Test Loss: 0.93388\n",
      "Epoch 066: | Train Loss: 0.78515 | Test Loss: 0.83987\n",
      "Epoch 067: | Train Loss: 0.84423 | Test Loss: 0.83122\n",
      "Epoch 068: | Train Loss: 0.67412 | Test Loss: 0.90854\n",
      "Epoch 069: | Train Loss: 1.33790 | Test Loss: 0.83362\n",
      "Epoch 070: | Train Loss: 0.85910 | Test Loss: 0.86780\n",
      "Epoch 071: | Train Loss: 0.72326 | Test Loss: 0.90563\n",
      "Epoch 072: | Train Loss: 0.69359 | Test Loss: 0.82965\n",
      "Epoch 073: | Train Loss: 0.98191 | Test Loss: 0.84673\n",
      "Epoch 074: | Train Loss: 0.63542 | Test Loss: 0.84946\n",
      "Epoch 075: | Train Loss: 0.61145 | Test Loss: 0.84426\n",
      "Epoch 076: | Train Loss: 0.58179 | Test Loss: 0.82423\n",
      "Epoch 077: | Train Loss: 0.69443 | Test Loss: 0.83559\n",
      "Epoch 078: | Train Loss: 0.59884 | Test Loss: 0.83825\n",
      "Epoch 079: | Train Loss: 0.60232 | Test Loss: 0.80818\n",
      "Epoch 080: | Train Loss: 0.75857 | Test Loss: 0.84552\n",
      "Epoch 081: | Train Loss: 0.67247 | Test Loss: 0.79226\n",
      "Epoch 082: | Train Loss: 0.60216 | Test Loss: 0.80016\n",
      "Epoch 083: | Train Loss: 0.62095 | Test Loss: 0.88960\n",
      "Epoch 084: | Train Loss: 0.71559 | Test Loss: 0.78100\n",
      "Epoch 085: | Train Loss: 0.65746 | Test Loss: 0.80392\n",
      "Epoch 086: | Train Loss: 0.72051 | Test Loss: 0.91735\n",
      "Epoch 087: | Train Loss: 0.63574 | Test Loss: 0.78439\n",
      "Epoch 088: | Train Loss: 0.54192 | Test Loss: 0.80652\n",
      "Epoch 089: | Train Loss: 0.62812 | Test Loss: 0.84842\n",
      "Epoch 090: | Train Loss: 0.53408 | Test Loss: 0.78679\n",
      "Epoch 091: | Train Loss: 0.49614 | Test Loss: 0.77899\n",
      "Epoch 092: | Train Loss: 0.50445 | Test Loss: 0.78675\n",
      "Epoch 093: | Train Loss: 0.46861 | Test Loss: 0.79141\n",
      "Epoch 094: | Train Loss: 0.47699 | Test Loss: 0.79535\n",
      "Epoch 095: | Train Loss: 0.53363 | Test Loss: 0.79382\n",
      "Epoch 096: | Train Loss: 0.67555 | Test Loss: 0.78045\n",
      "Epoch 097: | Train Loss: 0.50609 | Test Loss: 0.79489\n",
      "Epoch 098: | Train Loss: 0.50185 | Test Loss: 0.80940\n",
      "Epoch 099: | Train Loss: 0.50058 | Test Loss: 0.77620\n",
      "Epoch 100: | Train Loss: 0.45515 | Test Loss: 0.81518\n",
      "Epoch 101: | Train Loss: 0.44884 | Test Loss: 0.77087\n",
      "Epoch 102: | Train Loss: 0.48194 | Test Loss: 0.77235\n",
      "Epoch 103: | Train Loss: 0.44414 | Test Loss: 0.79884\n",
      "Epoch 104: | Train Loss: 0.45609 | Test Loss: 0.81606\n",
      "Epoch 105: | Train Loss: 0.43226 | Test Loss: 0.78128\n",
      "Epoch 106: | Train Loss: 0.42050 | Test Loss: 0.77406\n",
      "Epoch 107: | Train Loss: 0.43005 | Test Loss: 0.79084\n",
      "Epoch 108: | Train Loss: 0.48810 | Test Loss: 0.78322\n",
      "Epoch 109: | Train Loss: 0.46653 | Test Loss: 0.78741\n",
      "Epoch 110: | Train Loss: 0.42794 | Test Loss: 0.76900\n",
      "Epoch 111: | Train Loss: 0.40088 | Test Loss: 0.80948\n",
      "Epoch 112: | Train Loss: 0.44708 | Test Loss: 0.77820\n",
      "Epoch 113: | Train Loss: 0.50157 | Test Loss: 0.78991\n",
      "Epoch 114: | Train Loss: 0.40543 | Test Loss: 0.79425\n",
      "Epoch 115: | Train Loss: 0.43399 | Test Loss: 0.77848\n",
      "Epoch 116: | Train Loss: 0.43292 | Test Loss: 0.78291\n",
      "Epoch 117: | Train Loss: 0.37789 | Test Loss: 0.80193\n",
      "Epoch 118: | Train Loss: 0.39097 | Test Loss: 0.79235\n",
      "Epoch 119: | Train Loss: 0.44124 | Test Loss: 0.81117\n",
      "Epoch 120: | Train Loss: 0.53942 | Test Loss: 0.79918\n",
      "Epoch 121: | Train Loss: 0.38874 | Test Loss: 0.80722\n",
      "Epoch 122: | Train Loss: 0.38746 | Test Loss: 0.84363\n",
      "Epoch 123: | Train Loss: 0.37248 | Test Loss: 0.80464\n",
      "Epoch 124: | Train Loss: 0.35076 | Test Loss: 0.79595\n",
      "Epoch 125: | Train Loss: 0.34510 | Test Loss: 0.81885\n",
      "Epoch 126: | Train Loss: 0.35033 | Test Loss: 0.79271\n",
      "Epoch 127: | Train Loss: 0.33290 | Test Loss: 0.78872\n",
      "Epoch 128: | Train Loss: 0.39423 | Test Loss: 0.78785\n",
      "Epoch 129: | Train Loss: 0.71614 | Test Loss: 0.80015\n",
      "Epoch 130: | Train Loss: 0.37503 | Test Loss: 0.83097\n",
      "Epoch 131: | Train Loss: 0.39118 | Test Loss: 0.80212\n",
      "Epoch 132: | Train Loss: 0.32059 | Test Loss: 0.85105\n",
      "Epoch 133: | Train Loss: 0.32301 | Test Loss: 0.80924\n",
      "Epoch 134: | Train Loss: 0.32858 | Test Loss: 0.83156\n",
      "Epoch 135: | Train Loss: 0.31830 | Test Loss: 0.82298\n",
      "Epoch 136: | Train Loss: 0.30787 | Test Loss: 0.81405\n",
      "Epoch 137: | Train Loss: 0.35217 | Test Loss: 0.83555\n",
      "Epoch 138: | Train Loss: 0.37238 | Test Loss: 0.82636\n",
      "Epoch 139: | Train Loss: 0.33029 | Test Loss: 0.81845\n",
      "Epoch 140: | Train Loss: 0.30735 | Test Loss: 0.84617\n",
      "Epoch 141: | Train Loss: 0.36262 | Test Loss: 0.81442\n",
      "Epoch 142: | Train Loss: 0.34103 | Test Loss: 0.83451\n",
      "Epoch 143: | Train Loss: 0.31861 | Test Loss: 0.84616\n",
      "Epoch 144: | Train Loss: 0.29818 | Test Loss: 0.82677\n",
      "Epoch 145: | Train Loss: 0.31866 | Test Loss: 0.87081\n",
      "Epoch 146: | Train Loss: 0.32048 | Test Loss: 0.83264\n",
      "Epoch 147: | Train Loss: 0.30369 | Test Loss: 0.81861\n",
      "Epoch 148: | Train Loss: 0.30891 | Test Loss: 0.82943\n",
      "Epoch 149: | Train Loss: 0.29430 | Test Loss: 0.83210\n",
      "Epoch 150: | Train Loss: 0.45433 | Test Loss: 0.84452\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015459060668945312,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 150,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670a53185ad848afa53effd68cb46464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 40.17637 | Test Loss: 37.22912\n",
      "Epoch 002: | Train Loss: 36.95506 | Test Loss: 36.23726\n",
      "Epoch 003: | Train Loss: 38.67061 | Test Loss: 34.19677\n",
      "Epoch 004: | Train Loss: 34.92005 | Test Loss: 29.91157\n",
      "Epoch 005: | Train Loss: 26.26374 | Test Loss: 21.61025\n",
      "Epoch 006: | Train Loss: 18.66861 | Test Loss: 9.71496\n",
      "Epoch 007: | Train Loss: 5.90506 | Test Loss: 4.14818\n",
      "Epoch 008: | Train Loss: 5.12107 | Test Loss: 5.66477\n",
      "Epoch 009: | Train Loss: 3.81985 | Test Loss: 3.53258\n",
      "Epoch 010: | Train Loss: 3.21642 | Test Loss: 3.58632\n",
      "Epoch 011: | Train Loss: 3.20402 | Test Loss: 3.23600\n",
      "Epoch 012: | Train Loss: 4.99620 | Test Loss: 3.32335\n",
      "Epoch 013: | Train Loss: 2.77143 | Test Loss: 3.05965\n",
      "Epoch 014: | Train Loss: 2.95113 | Test Loss: 2.78394\n",
      "Epoch 015: | Train Loss: 2.38580 | Test Loss: 2.65480\n",
      "Epoch 016: | Train Loss: 2.40397 | Test Loss: 2.53119\n",
      "Epoch 017: | Train Loss: 3.97001 | Test Loss: 2.31813\n",
      "Epoch 018: | Train Loss: 2.14311 | Test Loss: 2.43668\n",
      "Epoch 019: | Train Loss: 2.74394 | Test Loss: 2.09000\n",
      "Epoch 020: | Train Loss: 1.95291 | Test Loss: 2.20455\n",
      "Epoch 021: | Train Loss: 1.78923 | Test Loss: 1.83059\n",
      "Epoch 022: | Train Loss: 1.75035 | Test Loss: 1.80995\n",
      "Epoch 023: | Train Loss: 2.08938 | Test Loss: 1.68737\n",
      "Epoch 024: | Train Loss: 1.51743 | Test Loss: 1.62014\n",
      "Epoch 025: | Train Loss: 2.27321 | Test Loss: 1.54496\n",
      "Epoch 026: | Train Loss: 1.72951 | Test Loss: 1.51813\n",
      "Epoch 027: | Train Loss: 1.84474 | Test Loss: 1.51566\n",
      "Epoch 028: | Train Loss: 1.38063 | Test Loss: 1.44065\n",
      "Epoch 029: | Train Loss: 1.56872 | Test Loss: 1.30226\n",
      "Epoch 030: | Train Loss: 1.30241 | Test Loss: 1.33024\n",
      "Epoch 031: | Train Loss: 1.72157 | Test Loss: 1.23866\n",
      "Epoch 032: | Train Loss: 1.19507 | Test Loss: 1.26236\n",
      "Epoch 033: | Train Loss: 1.26632 | Test Loss: 1.18769\n",
      "Epoch 034: | Train Loss: 1.32555 | Test Loss: 1.17354\n",
      "Epoch 035: | Train Loss: 1.07399 | Test Loss: 1.14834\n",
      "Epoch 036: | Train Loss: 1.06488 | Test Loss: 1.08790\n",
      "Epoch 037: | Train Loss: 1.05823 | Test Loss: 1.12897\n",
      "Epoch 038: | Train Loss: 1.03192 | Test Loss: 1.04738\n",
      "Epoch 039: | Train Loss: 0.98120 | Test Loss: 1.03726\n",
      "Epoch 040: | Train Loss: 1.00331 | Test Loss: 1.03008\n",
      "Epoch 041: | Train Loss: 1.00677 | Test Loss: 1.01054\n",
      "Epoch 042: | Train Loss: 1.05961 | Test Loss: 1.02579\n",
      "Epoch 043: | Train Loss: 0.97191 | Test Loss: 1.00391\n",
      "Epoch 044: | Train Loss: 0.99299 | Test Loss: 0.99083\n",
      "Epoch 045: | Train Loss: 0.90319 | Test Loss: 0.98924\n",
      "Epoch 046: | Train Loss: 1.64253 | Test Loss: 1.03517\n",
      "Epoch 047: | Train Loss: 0.94383 | Test Loss: 1.03232\n",
      "Epoch 048: | Train Loss: 0.94581 | Test Loss: 0.95272\n",
      "Epoch 049: | Train Loss: 0.93337 | Test Loss: 0.95756\n",
      "Epoch 050: | Train Loss: 0.83749 | Test Loss: 0.94247\n",
      "Epoch 051: | Train Loss: 0.83706 | Test Loss: 0.94783\n",
      "Epoch 052: | Train Loss: 0.88877 | Test Loss: 0.94285\n",
      "Epoch 053: | Train Loss: 0.87837 | Test Loss: 0.92807\n",
      "Epoch 054: | Train Loss: 0.79452 | Test Loss: 0.97896\n",
      "Epoch 055: | Train Loss: 1.05776 | Test Loss: 0.94384\n",
      "Epoch 056: | Train Loss: 0.81480 | Test Loss: 0.96070\n",
      "Epoch 057: | Train Loss: 0.83481 | Test Loss: 0.90947\n",
      "Epoch 058: | Train Loss: 0.80600 | Test Loss: 0.91896\n",
      "Epoch 059: | Train Loss: 0.79378 | Test Loss: 0.93497\n",
      "Epoch 060: | Train Loss: 0.84031 | Test Loss: 0.89676\n",
      "Epoch 061: | Train Loss: 0.75272 | Test Loss: 0.94398\n",
      "Epoch 062: | Train Loss: 0.81102 | Test Loss: 0.89467\n",
      "Epoch 063: | Train Loss: 0.75314 | Test Loss: 0.89084\n",
      "Epoch 064: | Train Loss: 0.79390 | Test Loss: 0.93874\n",
      "Epoch 065: | Train Loss: 0.74888 | Test Loss: 0.90361\n",
      "Epoch 066: | Train Loss: 0.81392 | Test Loss: 0.86779\n",
      "Epoch 067: | Train Loss: 1.11473 | Test Loss: 0.87098\n",
      "Epoch 068: | Train Loss: 0.79369 | Test Loss: 0.97266\n",
      "Epoch 069: | Train Loss: 0.70477 | Test Loss: 0.86035\n",
      "Epoch 070: | Train Loss: 0.76218 | Test Loss: 0.86227\n",
      "Epoch 071: | Train Loss: 0.75195 | Test Loss: 0.92268\n",
      "Epoch 072: | Train Loss: 0.77927 | Test Loss: 0.84121\n",
      "Epoch 073: | Train Loss: 0.76266 | Test Loss: 0.85327\n",
      "Epoch 074: | Train Loss: 0.78952 | Test Loss: 0.86270\n",
      "Epoch 075: | Train Loss: 0.73877 | Test Loss: 0.85175\n",
      "Epoch 076: | Train Loss: 0.83286 | Test Loss: 0.86412\n",
      "Epoch 077: | Train Loss: 0.81118 | Test Loss: 1.01623\n",
      "Epoch 078: | Train Loss: 0.71290 | Test Loss: 0.84149\n",
      "Epoch 079: | Train Loss: 0.79632 | Test Loss: 0.85359\n",
      "Epoch 080: | Train Loss: 0.70705 | Test Loss: 0.89297\n",
      "Epoch 081: | Train Loss: 0.69688 | Test Loss: 0.81924\n",
      "Epoch 082: | Train Loss: 0.63952 | Test Loss: 0.86885\n",
      "Epoch 083: | Train Loss: 0.71613 | Test Loss: 0.81751\n",
      "Epoch 084: | Train Loss: 0.68505 | Test Loss: 0.82431\n",
      "Epoch 085: | Train Loss: 0.63873 | Test Loss: 0.86745\n",
      "Epoch 086: | Train Loss: 0.80527 | Test Loss: 0.88580\n",
      "Epoch 087: | Train Loss: 0.64880 | Test Loss: 0.84224\n",
      "Epoch 088: | Train Loss: 0.63331 | Test Loss: 0.80670\n",
      "Epoch 089: | Train Loss: 0.79804 | Test Loss: 0.80964\n",
      "Epoch 090: | Train Loss: 0.64678 | Test Loss: 0.80866\n",
      "Epoch 091: | Train Loss: 0.66377 | Test Loss: 0.81395\n",
      "Epoch 092: | Train Loss: 0.60514 | Test Loss: 0.80413\n",
      "Epoch 093: | Train Loss: 0.59473 | Test Loss: 0.81567\n",
      "Epoch 094: | Train Loss: 0.64710 | Test Loss: 0.81991\n",
      "Epoch 095: | Train Loss: 0.65046 | Test Loss: 0.85200\n",
      "Epoch 096: | Train Loss: 0.60869 | Test Loss: 0.82030\n",
      "Epoch 097: | Train Loss: 0.80726 | Test Loss: 0.86229\n",
      "Epoch 098: | Train Loss: 0.79821 | Test Loss: 0.80874\n",
      "Epoch 099: | Train Loss: 0.65096 | Test Loss: 0.81978\n",
      "Epoch 100: | Train Loss: 0.80692 | Test Loss: 0.84788\n",
      "Epoch 101: | Train Loss: 0.67120 | Test Loss: 0.79645\n",
      "Epoch 102: | Train Loss: 0.59083 | Test Loss: 0.81233\n",
      "Epoch 103: | Train Loss: 1.44639 | Test Loss: 0.81239\n",
      "Epoch 104: | Train Loss: 0.73798 | Test Loss: 0.87762\n",
      "Epoch 105: | Train Loss: 0.61079 | Test Loss: 0.89523\n",
      "Epoch 106: | Train Loss: 0.58779 | Test Loss: 0.87870\n",
      "Epoch 107: | Train Loss: 0.55910 | Test Loss: 0.82940\n",
      "Epoch 108: | Train Loss: 0.54148 | Test Loss: 0.86600\n",
      "Epoch 109: | Train Loss: 0.53842 | Test Loss: 0.84586\n",
      "Epoch 110: | Train Loss: 0.52974 | Test Loss: 0.83293\n",
      "Epoch 111: | Train Loss: 0.70975 | Test Loss: 0.83261\n",
      "Epoch 112: | Train Loss: 0.59979 | Test Loss: 0.83156\n",
      "Epoch 113: | Train Loss: 0.55912 | Test Loss: 0.96071\n",
      "Epoch 114: | Train Loss: 0.59838 | Test Loss: 0.83141\n",
      "Epoch 115: | Train Loss: 0.58564 | Test Loss: 0.87004\n",
      "Epoch 116: | Train Loss: 0.54267 | Test Loss: 0.83799\n",
      "Epoch 117: | Train Loss: 0.52223 | Test Loss: 0.80623\n",
      "Epoch 118: | Train Loss: 0.51409 | Test Loss: 0.86839\n",
      "Epoch 119: | Train Loss: 0.68493 | Test Loss: 0.84940\n",
      "Epoch 120: | Train Loss: 0.50094 | Test Loss: 0.86649\n",
      "Epoch 121: | Train Loss: 0.57773 | Test Loss: 0.84364\n",
      "Epoch 122: | Train Loss: 0.49685 | Test Loss: 0.87216\n",
      "Epoch 123: | Train Loss: 0.49861 | Test Loss: 0.83223\n",
      "Epoch 124: | Train Loss: 0.53806 | Test Loss: 0.87879\n",
      "Epoch 125: | Train Loss: 0.50463 | Test Loss: 0.85559\n",
      "Epoch 126: | Train Loss: 0.54189 | Test Loss: 0.82472\n",
      "Epoch 127: | Train Loss: 0.53241 | Test Loss: 0.82904\n",
      "Epoch 128: | Train Loss: 0.51225 | Test Loss: 0.83972\n",
      "Epoch 129: | Train Loss: 0.47532 | Test Loss: 0.85635\n",
      "Epoch 130: | Train Loss: 0.47882 | Test Loss: 0.87818\n",
      "Epoch 131: | Train Loss: 0.46399 | Test Loss: 0.84813\n",
      "Epoch 132: | Train Loss: 0.58990 | Test Loss: 0.86977\n",
      "Epoch 133: | Train Loss: 0.47706 | Test Loss: 0.86360\n",
      "Epoch 134: | Train Loss: 0.46821 | Test Loss: 0.82191\n",
      "Epoch 135: | Train Loss: 0.53043 | Test Loss: 0.89295\n",
      "Epoch 136: | Train Loss: 0.66034 | Test Loss: 0.84350\n",
      "Epoch 137: | Train Loss: 0.62466 | Test Loss: 0.84174\n",
      "Epoch 138: | Train Loss: 0.48616 | Test Loss: 0.98583\n",
      "Epoch 139: | Train Loss: 0.47395 | Test Loss: 0.82054\n",
      "Epoch 140: | Train Loss: 0.47128 | Test Loss: 0.84125\n",
      "Epoch 141: | Train Loss: 0.45071 | Test Loss: 0.87127\n",
      "Epoch 142: | Train Loss: 0.43389 | Test Loss: 0.82852\n",
      "Epoch 143: | Train Loss: 0.44010 | Test Loss: 0.85905\n",
      "Epoch 144: | Train Loss: 0.45933 | Test Loss: 0.84029\n",
      "Epoch 145: | Train Loss: 0.48538 | Test Loss: 0.84594\n",
      "Epoch 146: | Train Loss: 0.46782 | Test Loss: 0.86997\n",
      "Epoch 147: | Train Loss: 0.50087 | Test Loss: 0.82645\n",
      "Epoch 148: | Train Loss: 0.45588 | Test Loss: 0.91477\n",
      "Epoch 149: | Train Loss: 0.57086 | Test Loss: 0.84822\n",
      "Epoch 150: | Train Loss: 0.44829 | Test Loss: 0.86179\n"
     ]
    }
   ],
   "source": [
    "result111 = {\"R2_train\": [], \"R2_test\": [], \"MSE_train\": [],\"MSE_test\": []}\n",
    "\n",
    "for i in range(20):\n",
    "    \n",
    "    \n",
    "    loss_status,model=fit_NN_model(train_loader,test_loader,EPOCHS,BATCH_SIZE,LEARNING_RATE,NUM_FEATURES)\n",
    "    \n",
    "    y_pred_train = model(torch.from_numpy(x_train).float()).tolist()\n",
    "    y_pred_test = model(torch.from_numpy(x_test).float()).tolist()\n",
    "    \n",
    "    result111[\"R2_train\"].append(r2_score(y_train, y_pred_train)) #输出训练集和测试集R Square的结果\n",
    "    result111[\"R2_test\"].append(r2_score(y_test, y_pred_test))\n",
    "    result111[\"MSE_train\"].append(mean_squared_error(y_train, y_pred_train))\n",
    "    result111[\"MSE_test\"].append(mean_squared_error(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存结果\n",
    "with open('DFNN十次重复实验.csv', 'w') as f:\n",
    "    [f.write('{0},{1}\\n'.format(key, value)) for key, value in result111.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing NN results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\"\n",
    "# 训练集/测试集预测\n",
    "y_pred_train = model(torch.from_numpy(x_train).float()).tolist()\n",
    "y_pred_test = model(torch.from_numpy(x_test).float()).tolist()\n",
    "\n",
    "R_value_train = r2_score(y_train, y_pred_train) #输出训练集和测试集R Square的结果\n",
    "R_value_test = r2_score(y_test, y_pred_test)\n",
    "MSE_value_train = mean_squared_error(y_train, y_pred_train) \n",
    "MSE_value_test = mean_squared_error(y_test, y_pred_test)\n",
    "    \n",
    "print(\"R2：\",R_value_train,R_value_test,\"MSE：\",MSE_value_train,MSE_value_test)\n",
    "print(len(y_pred_train),len(y_pred_test))\n",
    "    \n",
    "fig = plt.figure(figsize=(14,10))\n",
    "lw = 2\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.3, hspace=0.3)\n",
    "line_x = np.linspace(0,20,50)#从(-1,1)均匀取50个点\n",
    "# 预测结果散点图\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "plt.scatter(y_pred_train,y_train)\n",
    "ax2 = fig.add_subplot(2,2,2)\n",
    "plt.scatter(y_pred_test,y_test)\n",
    "# loss作图\n",
    "ax3 = fig.add_subplot(2,2,3)\n",
    "plt.plot(list(range(1,EPOCHS+1)),loss_status['train'])\n",
    "ax4 = fig.add_subplot(2,2,4)\n",
    "plt.plot(list(range(1,EPOCHS+1)),loss_status['test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the optimal predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./model/DFNN.pka\"\n",
    "\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./release/') \n",
    "from DFNN_predictor import NN_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"预测活性.csv\"\n",
    "\n",
    "dataset,canonical_smi,canonical_mols = load_data(path)\n",
    "vali_data = calcMCFP(mols = canonical_mols, dataset = dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vali_data  # validation set\n",
    "path = \"./model/DFNN.pka\"\n",
    "\n",
    "pre_act = NN_predict(data = vali_data, path =path, num_features = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
